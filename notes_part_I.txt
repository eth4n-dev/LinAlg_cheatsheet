Linear Algebra, First Part
Lecture Notes HS25
Bernd Gärtner
Department of Computer Science
ETH Zürich
September 16, 2025

Contents
0

Introduction
0.1 About these notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0.1.1 Major changes compared to HS24 . . . . . . . . . . . . . . . . . . . .
0.2 About computer science (and mathematics) . . . . . . . . . . . . . . . . . . .
0.3 About linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4
4
5
6
7

1

Vectors
1.1 Vectors and linear combinations . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.1 Vector addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.2 Scalar multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.3 Linear combinations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.4 Affine, conic, and convex combinations . . . . . . . . . . . . . . . . .
1.1.5 Defining the dots: sequences, sums, sets, and vectors . . . . . . . . .
1.2 Scalar products, lengths and angles . . . . . . . . . . . . . . . . . . . . . . . .
1.2.1 Scalar product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.2 Euclidean norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.3 Cauchy-Schwarz inequality . . . . . . . . . . . . . . . . . . . . . . . .
1.2.4 Angles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.5 Triangle inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.6 Covectors and v⊤ w . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Linear (in)dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.1 Definition and examples . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.2 Alternative definitions . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.3 Span of vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11
11
14
15
16
19
21
23
24
25
28
30
33
34
36
36
38
40

2

Matrices
2.1 Matrices and linear combinations . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.1 Matrix-vector multiplication . . . . . . . . . . . . . . . . . . . . . . .
2.1.2 Column space and rank . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.3 Row space and transpose . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.4 Rank-1 matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.5 Nullspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Matrices and linear transformations . . . . . . . . . . . . . . . . . . . . . . .

45
46
49
51
53
55
57
58

1

2.3

2.4

2.2.1 Matrix transformations . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Linear transformations and linear functionals . . . . . . . . . . . . .
2.2.3 The matrix of a linear transformation . . . . . . . . . . . . . . . . . .
2.2.4 Kernel and Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Matrix multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 Combining matrix transformations . . . . . . . . . . . . . . . . . . . .
2.3.2 Definition and basic properties . . . . . . . . . . . . . . . . . . . . . .
2.3.3 Distributivity and associativity . . . . . . . . . . . . . . . . . . . . . .
2.3.4 Everything “is” matrix multiplication . . . . . . . . . . . . . . . . . .
2.3.5 CR decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Invertible and Inverse matrices . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.1 Undoing matrix transformations . . . . . . . . . . . . . . . . . . . . .
2.4.2 Definitions and basic properties . . . . . . . . . . . . . . . . . . . . .

58
63
67
68
69
70
71
75
76
82
85
85
89

3

Solving Linear Equations Ax = b
93
3.1 Systems of linear equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
3.1.1 The PageRank algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 95
3.1.2 Matrix inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.2 Gauss elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
3.2.1 Back substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
3.2.2 Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
3.2.3 Row operations: the general picture . . . . . . . . . . . . . . . . . . . 104
3.2.4 Success and failure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
3.2.5 Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
3.2.6 Computing inverse matrices . . . . . . . . . . . . . . . . . . . . . . . 110
3.2.7 Solving Ax = b from A−1 , LU, and LUP . . . . . . . . . . . . . . . . . 113
3.3 Gauss-Jordan elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.3.1 Reduced row echelon form . . . . . . . . . . . . . . . . . . . . . . . . 114
3.3.2 Direct solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
3.3.3 Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
3.3.4 Standard form and CR decomposition . . . . . . . . . . . . . . . . . . 122
3.3.5 Computing inverse matrices, and solving Ax = b from R and M . . 124

4

The Three Fundamental Subspaces
126
4.1 Vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
4.1.1 Definition and examples . . . . . . . . . . . . . . . . . . . . . . . . . . 127
4.1.2 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
4.2 Bases and dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
4.2.1 Linear combinations and related concepts in vector spaces . . . . . . 134
4.2.2 Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
4.2.3 The Steinitz exchange lemma . . . . . . . . . . . . . . . . . . . . . . . 140
4.2.4 Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
4.2.5 Linear transformations between vector spaces . . . . . . . . . . . . . 143
2

4.3

4.4

4.2.6 Computing a vector space . . . . . . . . . . . . . . . . . . . . . . . . . 147
Computing the three fundamental subspaces . . . . . . . . . . . . . . . . . . 148
4.3.1 Column space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
4.3.2 Row space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
4.3.3 Nullspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
All solutions of Ax = b . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
4.4.1 Affine subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156
4.4.2 The number of solutions . . . . . . . . . . . . . . . . . . . . . . . . . . 156

Bibliography

159

Index

160

3

Chapter 0
Introduction
0.1

About these notes

These are the lecture notes for the first part of the course
Lineare Algebra (401-0131-00L)
held at the Department of Computer Science at ETH Zürich in HS25.
These notes can be considered as a full version of what will be written on the tablet
during the lectures. Lecture plans will be made available before each lecture. The tablet
notes (in German) reflect the reality and will be made available after each lecture. Together with the explanations (and answers to questions) given in the lectures, they will
contain the “essentials”, but in order to fully understand them, it can be helpful to look
up more details and additional explanations in the lecture notes.
In content, the lecture notes are loosely based on the first 8 chapters of the book
Introduction to Linear Algebra (Sixth Edition) by Gilbert Strang, Wellesley Cambridge Press, 2023 [Str23].
The ordering of material is somewhat different here. But the main difference is that
the approach taken here is more rigorous than Strang’s. Strang introduces the material on
an intuitive level, guided by many examples; this provides a great informal introduction
to Linear Algebra. What we add here (hopefully without losing the intuition) are formal
definitions of concepts, as well as mathematical statements with proofs for the key results.
Strang’s book is not part of the course’s official material, and there is no need for students to buy the book (it doesn’t have an official electronic version). With the lectures,
lecture plans, tablet notes, lecture notes, exercises, and exercises classes, the course is selfcontained. Strang’s book and others mentioned on the course web page serve as optional
literature.
There are a number of major changes (on top of many minor changes to improve
clarity), compared to the lecture notes of HS24. We outline them next, including brief
explanations why these changes were made.
4

0.1.1

Major changes compared to HS24

Column vector notation. This is the most visible change. For example, instead of
 
 
1
1
2
∈ R , we now write
∈ Rm .
2
2
The reason is that the square bracket notation was (and still is) used for matrices as well,
so it was previously not possible to visually distinguish between a vector in R2 and 2 × 1
matrix. In most cases, one could tell this from the context; however, always using square
brackets creates the (wrong) impression that vectors and matrices with one column are
the same objects. This has previously led to quite some confusion. Now, we clearly distinguish between vectors and matrices with one column, and this also shows in the notation.
We are now also clear about the difference between row and column vectors; previously,
we used both to write vectors in Rm , but now we consistently use column vectors for Rm ,
and row vectors for the dual space (Rm )∗ .
Matrix multiplication (AB), matrix inversion (A−1 ). These two important matrix operations are now motivated from “combining” and “undoing” linear transformations.
Previously, they came a little “out of the blue”. Both operations are now introduced in
Chapter 2 where they logically belong. Previously, matrix inversion was introduced after
Gauss elimination, because only at this point, we had the necessary theory for it. The
crucial (previously missing) piece of theory now already occurs in Chapter 1, in the form
of the rather innocent-looking but impactful Lemma 1.28.
LU and LUP decompositions. These have been removed from the notes. Previously,
the LU decomposition was presented only for a special class of matrices (the ones for
which Gauss elimination works without row exchanges), and the LUP decomposition
was only sketched in class. For theoretical (non-numerical) purposes, both LU and LUP
decompositions are of minor relevance, since Gauss-Jordan elimination (that we present
in full detail now) can easily replace them, without any restrictions on the matrix. As a
consequence, our longstanding first exam problem (compute the LU decomposition of a
3 × 3 matrix) will be replaced. The new first exam problem will ask you to solve a 3 × 3
system of linear equations.
Linear transformations between vector spaces. Previously, we have introduced linear
transformations as functions from some Rn to some Rm . However, the right level of
abstraction is to consider linear transformations between two vector spaces V and W .
Since we cover vector spaces anyway, it is easy to also include this more abstract view of
linear transformations. This now allows us to formally define when two vector spaces are
“essentially the same”, a concept that we have previously only covered informally. As a
concrete benefit, a basis of the nullspace of a matrix can now be derived in a much more
transparent and systematic way than previously.
5

0.2

About computer science (and mathematics)

As a first semester student of computer science, you may wonder why one of the first
things you see is mathematics, and in particular linear algebra. In order to answer this,
we first need to answer a different question: What is computer science?
Computer scientists are frequently approached for help with installing computers,
getting the internet to work, and similar technical tasks. To many people, a computer scientist is simply an information technology expert. But this point of view is fundamentally
wrong. The computer scientist Mike Fellows has explained this very well already in 1991,
using a simple analogy:
Computer science is not about machines in the same way that astronomy is
not about telescopes. There is an essential unity of mathematics and computer
science [Fel93].
The first sentence of this quote (often misattributed to Edward Dijkstra) is well-known
and gets the main point across: computer science is not mainly about computers. Everyone agrees that computers, just like telescopes, are great tools, but they merely help us in
achieving some goals, they are not the goals themselves. It is true that astronomers are
involved in building telescopes, and computer scientists are involved in building computers. Generally, if you need a tool that you cannot buy off the shelf, you will team up
with people that can build it for you, and this needs expertise from both sides. But in the
end, you want to use the tool for something, so you are not mainly interested in the tool
itself.
The second part of the quote is less known, but not less important. It indicates that
computer science and mathematics are very strongly connected. To understand this, we
need to say what computer science actually is. If you ask the internet for a definition of
computer science, you get many wrong answers that start with “the study of computers. . . ”, even from serious sources. The Wikipedia article about computer science starts
differently and does not mention computers in the first sentence:1
Computer science is the study of computation, information, and automation.
This is correct but a bit too short; the German version of the page,2 has a more detailed and clearer definition, adapted from the “Duden Informatik A-Z” [CSB06, Eintrag
Informatik, S. 305].
Informatik ist die Wissenschaft von der systematischen Darstellung, Speicherung, Verarbeitung und Übertragung von Informationen, wobei in der Regel
die automatische Verarbeitung mit Computern betrachtet wird.
1
2

https://en.wikipedia.org/wiki/Computer_science, accessed on September 2, 2025
https://de.wikipedia.org/wiki/Informatik, accessed on September 2, 2025

6

In English, this reads as follows:
Computer science is the science of the systematic representation, storage,
processing and transfer of information, where one usually considers the automatic processing using computers.
This means, computer science is about dealing with information. The German term
Informatik transports this message much better than the English term computer science.
As an example, let’s consider addition as you (and children throughout many centuries) have learned it in primary school, for example
123
+ 486
= 609
This is systematic processing of information (in this case numbers), represented in
decimal place-value system. Hence, this is not only mathematics, but according to the
above definition, it is also computer science! In modern terms, we would call school book
addition an algorithm. Computers can do such additions automatically and very fast (this
is what the “where one. . . ” part of the definition is about), but the algorithm itself was
invented much earlier than the computers.
While an algorithm is mostly associated with computer science, the theoretical foundations of many algorithms and other computer science inventions are inherently mathematical. School book addition is a prime example. Here, the most important theoretical
foundation is the place-value system. This is one of the great historical developments in
mathematics, driven by the need to efficiently compute with numbers.
Today, there are new needs, for example, efficiently training huge machine learning
models, or securing computer systems against cyberattacks. This needs established as
well as new mathematics. Mathematical research is often motivated by applications in
computer science, and mathematicians work with computer science tools on a daily basis. For these reasons, every computer science student needs mathematical foundations,
and every mathematics student needs computer science foundations. An essential unity,
indeed.

0.3

About linear algebra

The origins of (linear) algebra can be traced back to the 9th century when the Persian
polymath Al-Khwarizmi published The Compendious Book on Calculation by Completion and
Balancing. The word algebra also goes back to this book, see the highlighted part on the
title page shown in Figure 1. In modern Arabic letters and transcribed to Latin letters
(right to left), this reads as follows:
=

=

7

Al Khwarizmi’s book teaches how to systematically solve linear and quadratic equations in one variable. While this is basic highschool material today, the theory underlying
it had to be developed at some point, and it was Al-Khwarizmi who did this; for example, the word balancing in the title of his work refers to the technique of moving a term
to the other side of an equation, something you need to do when you want to solve for a
variable.

Figure 1: Title page of Al-Khwarizmi’s book; the highlighted text in the lines written
from bottom to top is the Arabic word Al-jabr. This is a higher-resolution image of the one
found on Wikipedia (https://en.wikipedia.org/wiki/Al-Jabr), provided to the
author by Digital Bodleian, https://digital.bodleian.ox.ac.uk, CC-BY-NC 4.0.
A translation of the title page can be found here.
The field of linear algebra has developed from two historical roots: analytic geometry
and linear equations. Analytic geometry deals with the description of and calculation with
geometric objects through coordinates and formulas. We could also call it “rigorous”
8

y
(3, 2)

2
3

x

Figure 2: A point in the two-dimensional plane, expressed in Cartesian coordinates
geometry. Everybody knows what a circle is, but if you want to describe a particular
circle, you need to say what the center and the radius are. The center is a point that you
typically describe with Cartesian coordinates, as in Figure 2.
Having such rigorous descriptions of geometric objects, you can start to answer questions about them analytically by using mathematics; for example, do two given lines in
three-dimensional space intersect or not?
In answering this and many other questions, systems of linear equations in more than
one variable come up, and this is the second root of linear algebra. Today, such systems
are considered as easy to solve in many cases, but this is the result of developments that
happened over centuries. And they still happen now: solving systems of linear equations
is an active research topic, in particular since the traditional algorithms cannot handle the
very large systems that we have in many applications today. Even small systems are not
necessarily easy for humans and form a basis of many puzzles. Consider this one:
Dominik is twice as old as Susanne and three years older than Claudia.
Together, the children are 17 years old. How old are the three children?
Even without having heard about linear equations, you will be able to figure this out,
employing some guesswork and mental arithmetic. But this quickly reaches its limits
when more children are involved in the puzzle. There is a reason why such puzzles that
are made for entertainment rarely have more than three variables (in this case, the ages of
the children). As a system of three linear equations, the puzzle reads as follows:
D = 2S
D = C +3
D + S + C = 17
Here, D, S, and C are variables for the unknown ages of the children, and the three equations encode the three pieces of information that the puzzle provides. The equations are
linear because every variable occurs only in the first power. In contrast, x2 − 2x + 3 = 0 is a
quadratic equation, because the variable x occurs in the second power as well. There are
also cubic, quartic, quintic, etc. equations. Solving systems of those falls into the domain
of (non-linear) algebra. Linear algebra deals with m linear equations in n variables, where
both numbers m and n can be very large in practice.
9

In the above linear puzzle equations, you can convince yourself that there are unique
numbers that you can plug in for D, S, and C such that all three equations are satisfied.
Conveniently, these numbers are natural numbers; you don’t want a puzzle where a child
is 2.7 years or −3 years old. But in principle, the solutions to a system of linear equations
can be arbitrary numbers, and it is up to the application to decide whether these make
sense or not.
Interestingly, Al-Khwarizmi only provided formulas for positive solutions of equations in his book, as he thought that only those make sense. Indeed, in a world where
numbers count physical quantities, the idea of a negative number seems downright crazy.
However, in Al-Khwarizmi’s computations (including “balancing”), negative numbers
implicitly occur in order to arrive at the positive solutions.
Starting from the roots of analytic geometry and systems of linear equations, linear
algebra has grown many important branches, some of which appear in Figure 3 and also
later in these notes.

numerical linear algebra

vector spaces

linear transformations

determinants
eigenvalues

complex number plane

matrices
vectors

systems of linear equations

analytic geometry

Figure 3: The tree of linear algebra: Roots and some important branches

10

Chapter 1
Vectors
Asking ChatGPT what a vector is will return something like this: “A vector is a mathematical object that has both magnitude (length) and direction.” This is in line with how
we visualize vectors in analytic geometry, namely as arrows that naturally have a length
and a direction:

These arrow pictures are good starting points, but in linear algebra, we will (have to)
do more. We can draw arrows in 2- and even 3-dimensional space, but we cannot do this
in 10-dimensional space. Higher-dimensional spaces where the geometric intuition fails
us are important in applications, therefore we need an actual definition of the mathematical object behind a vector. That a vector has (according to ChatGPT) both length and
direction is a property of the object, but not a definition. On top of this, it is not clear what
“length” and “direction” mean in higher dimensions.
The language of pictures is extremely useful for the intuitive understanding of mathematical objects, and we will keep using this language whenever we can. But if we want
to logically argue about the objects (in particular when we cannot “see” them anymore
in high dimensions), we need the language of mathematics. Mathematical objects are
imaginary and not physical, but once we can talk about them, they exist in some way (for
example, you can think about the way in which the number 5 exists).
In this chapter, we get to know the mathematical language around the (coordinate)
vectors whose 2- and 3-dimensional versions we know from analytic geometry. But in
Section 4.1, we will discover that there are also other kinds of vectors.

1.1

Vectors and linear combinations

Vectors in Rm and their linear combinations are fundamental in linear algebra and
at the same time easy to understand. In fact, you may know much of this material
from high school. This leaves room to also learn about some important elements of
mathematical thinking and writing. For example, you will see a first proof.
11

Vectors as you know them from high school “live” in 2-dimensional or 3-dimensional
space, see Figure 1.1.

y


−2
2

 
4
1
0


−2 y
  
2
−2
 2
0
3
3




x
 
0
0=
0

0
z



x
 
0

0 = 0
0


 
v1
w1
 v2 
 w2 
 
 
v =  ..  , w =  .. 
 . 
 . 
vm

wm

Figure 1.1: Vectors in the plane R2 (left), in space R3 (middle) and in Rm (right)
More abstractly, they live in some m-dimensional space Rm , where m ∈ N, the set
{0, 1, 2, . . .} of natural numbers; see Figure 1.1 (right).1 Starting from m = 4, these spaces
are hard to visualize, but linear algebra can handle them as easily as R2 and R3 .
Mathematically, these spaces are sets that are called R2 , R3 , and Rm , where R is the
set of real numbers. R2 , the xy-plane, contains (has as elements) all pairs (v1 , v2 ) of real
numbers, for example (4, 1). R3 , the xyz-space, contains all triples (v1 , v2 , v3 ) of real numbers, for example (−2, 2, 3). Rm contains all m-tuples or sequences (v1 , v2 , . . . , vm ) of m real
numbers. Here, each number from 1 to m serves as an index, indicating the position in the
sequence: for example, v5 denotes the 5-th element of the sequence.
Upfront, a sequence is just a mathematical object that contains some numbers that are
ordered; these numbers could mean anything. For example, the pair (94, 189) ∈ R2 could
stand for the weight (in kg) and height (in cm) of a person. We call an element of R2 a
vector when we think of the numbers in the pair as coordinates in 2-dimensional space;
we typically draw such vectors as arrows in a Cartesian coordinate system, with the tail of
the arrow at the origin (where the coordinate axes cross), and the head at the respective
coordinates. The zero vector would have both head and tail at the origin, with undefined
arrow direction, so we cannot nicely draw it as an arrow but think of it as a point at the
origin. For vectors, it is customary to use column vector notation to indicate that we now
think of a sequence as a vector; see Figure 1.1.
In referring to a vector in text or in a formula, we use bold lower case Latin letters
such as v and w. You may have learned to write vectors as ⃗v , w,
⃗ but v is as good (or bad)
as ⃗v . The important thing is to be consistent.
Definition 1.1 (Vector). Let m ≥ 0 be a natural number. An m-dimensional coordinate vector
(simply called vector in the following) is an element of Rm , written in column vector notation
1

For mathematicians, 1 is typically the first natural number; for computer scientists it is 0. None of the
two choices is better or worse than the other one. In these “linear algebra for computer scientists” notes,
we start with 0.

12

as




 
v1
0
 v2 
0
 
 
v =  ..  . The vector  ..  is the zero vector in Rm , denoted by 0.
 . 
.
vm

0

The numbers v1 , v2 , . . . , vm are the coordinates of v. In R0 , there is only one vector, namely the
empty sequence of real numbers, and this is simply written as ().
While examples are very useful to understand a concept, a definition is the official
reference. The goal of a definition is to introduce a concept in full generality, and to be
able to refer back to it later.
The notation 0 is actually imprecise and represents what mathematicians call an abuse
of notation. The abuse here is that the meaning of 0 depends on the context and may, as in
Figure 1.1, refer to the zero vector in R2 or the zero vector in R3 . Such abuse of notation is
common and also known from natural language. If you take “the bike”, it is clear that you
mean your bike, while your friends, saying the same thing, refer to their bikes. A fully
precise alternative would be to write 02 for the zero vector in R2 , 03 for the zero vector in
R3 , and 0m for the zero vector in Rm . This usually has not much of a benefit and instead
makes the text less readable— 02 for example looks more like oxygen than a vector.
The arrow drawing suggests an interpretation of a vector as a movement, for example
“go 4 steps right and 1 step up!” in case of the vector
 
4
.
1
Under this interpretation, the arrow can actually be placed anywhere and still visualizes
the same vector. It can also be useful to visualize not only the zero vector but any vector
as a point (at its coordinates); see Figure 1.2.
y  
4
 
1
4
1
0

y
 
4
1

x

0

x

Figure 1.2: Vector: visualization as arrow (left), or point (right)
As an example why this is useful, let us try to visualize the set of vectors in R2 whose
coordinates sum up to 5, highlighting a specific such vector. Then it should be clear that
Figure 1.3 (right), with the gray line showing the vectors in question as points, is more
suitable than Figure 1.3 (left) that is just cluttered with arrows.
13

y

y
 
4
1

 
4
1

x

0

0

x

Figure 1.3: Vectors v in R2 with v1 + v2 = 5: visualization as arrows (left), or points (right)

1.1.1

Vector addition

Adding two vectors combines their movements. Depending on which movement we do
first, we can take two different routes to the same result; together, these routes form a
parallelogram. Algebraically, vector addition works coordinate-wise, i.e. we add up corresponding coordinates of the two vectors; see Figure 1.4.

y  
2
3



 
5
2
3
−1

     
2
3
5
+
=
3
−1
2

x

Figure 1.4: The parallelogram of vector addition
Definition 1.2 (Vector addition). Let
 
 


v1
w1
v1 + w1
 v2 
 w2 
 v2 + w2 
 
 


v =  ..  , w =  ..  ∈ Rm . The vector v+w := 
 ∈ Rm is the sum of v and w.
..
 . 
 . 


.
vm
wm
vm + wm
Definition 1.2 tells you how to add any two vectors of any dimension. The symbol :=
is used to define what is left of it by what is right of it.
In comparison to Definition 1.1, we have omitted the first sentence: Let m ≥ 0 be a
natural number. This omission is not formally correct, but acceptable; the symbols m (and
also n later) will always stand for natural numbers, so there is no need to explicitly say
this each time.
In the same way, we can add more vectors. For example u + v + w is the sum of three
vectors, and in this case, we get six possible routes to the result; see Figure 1.5.
14

y

 
5
4

 
10
6

 
2
3

       
10
2
3
5
+
+
=
3
−1
4
6

x


3
−1



Figure 1.5: Adding three vectors
Strictly speaking, Definition 1.2 does not define u + v + w. It only talks about adding
two vectors at a time. The natural official definition of u + v + w would be (u + v) + w
(add vectors one by one, from left to right), but since addition in R is associative, it does
not matter where we put the brackets, so we can as well omit them and write u + v + w.
Similarly, in talking about the two possible routes to arrive at v + w, we implicitly used
that v + w = w + v which holds since addition in R is commutative. The six possible routes
for three vectors come from the fact that we can add them up in six different orders.
It is an element of mathematical thinking to clarify the meaning of u + v + w when this
was not explicitly defined. Only after that, you really understand the meaning and can
safely write u + v + w. As a Swiss luxury watch commercial puts it: to break the rules,
you must first master them.

1.1.2

Scalar multiplication

Scalar multiplication corresponds to moving λ times as far, for some real number λ (in
this context also called a scalar). For scalars, we typically use lower case Greek letters.
We say that the resulting vector is a scalar multiple of the original one. The scalar
can be negative, in which case we move into the opposite direction. Algebraically, scalar
multiplication multiplies each coordinate of the vector with the scalar; see Figure 1.6.
Definition 1.3 (Scalar multiplication). Let
 


v1
λv1
 v2 
 λv2 
 


m
v =  ..  ∈ R , λ ∈ R. The vector λv :=  ..  ∈ Rm is a scalar multiple of v.
 . 
 . 
vm
λvm
Note that the zero vector 0 is a scalar multiple of every vector, obtained by choosing
scalar λ = 0.
15

y
 
2
1

 
6
3
   
2
6
3
=
1
3

x


−4
−2

   
2
−4
(−2)
=
1
−2



Figure 1.6: Scalar multiplication

1.1.3

Linear combinations

This operation combines vector addition and scalar multiplication.
Definition 1.4 (Linear combination). Let v, w ∈ Rm , λ, µ ∈ R. The vector
λv + µw ∈ Rm
is a linear combination of v and w. In general, if v1 , v2 , . . . , vn ∈ Rm and λ1 , λ2 , . . . , λn ∈ R,
then
λ1 v1 + λ2 v2 + · · · + λn vn
is a linear combination of v1 , v2 , . . . , vn .
µ  λv  µw λv
 + µw

−6
6
0
−3
2
−9 −2 −11
2
−3
−1
1 −1
3
 1
 4
6
0
6
3
0
9
0
9
λ

 
 
2
3
v=
,w =
:
3
−1

Table 1.1: Three linear combinations of two vectors v, w
Table 1.1 gives three linear combinations of two specific vectors v and w. What are all
the linear combinations of v and w that can we get in this way? Here is the answer:
Fact 1.5. Every vector in R2 is a linear combination of the two vectors
 
 
2
3
v=
,w =
.
3
−1

16

Proof. Let
 
u1
u=
u2
be an arbitrary vector in R2 . We need to show that we can find scalars λ, µ ∈ R such that
 
   
2
3
u1
λ
+µ
=
.
3
−1
u2
Considering the rules of vector addition and scalar multiplication from Definitions 1.2
and 1.3, this vector equation is actually made up of two “normal” equations, one for each
coordinate:
2λ + 3µ = u1 ,
3λ − 1µ = u2 .
This is a system of two linear equations in two variables λ and µ, and the rest is therefore
a high school job.
Well, almost: What you may find unusual here is that the system also contains the
variables u1 , u2 . But while the variables λ and µ stand for the unknown numbers that
we want to compute, u1 and u2 stand for known numbers, the coordinates of our target
vector u. By solving this system of equations (the solution will depend on u1 and u2 ),
we therefore solve it for all possible values of u1 and u2 at the same time. And this was
exactly the point, since we want to make the argument for every possible vector u. This
is what differentiates a proof from a calculation: a proof makes one argument for many
(possibly infinitely many) situations, while a calculation just handles one situation.
After this digression, let us solve the system: Multiplying the second equation by 3
and adding it to the first one cancels the variable µ:
2λ + 3µ = u1
9λ − 3µ =
3u2
11λ
= u1 + 3u2
This gives
u1 + 3u2
.
11
To get µ, we isolate µ in one of the equations (let us take the first one) and substitute the
value of λ that we just got:
λ=

3µ = u1 − 2λ = u1 −

2u1 + 6u2
11u1 − (2u1 + 6u2 )
9u1 − 6u2
=
=
.
11
11
11

Dividing by 3 yields

3u1 − 2u2
.
11
So λ and µ have been found for all possible values of u1 and u2 which completes the
proof.
µ=

17

It is always good to double check this on examples. Let us look at the three linear combinations that we have previously computed in Table 1.1 and see whether the formulas
for λ and µ give us back the correct scalars. Table 1.2 shows that they do.
µ λv 
+ µw= u u1
u2
0
−3
2
0 −11
−11
 
−1
1 −1
−1
4
 4
6
3
0
6
9
9
λ

 
 
2
3
v=
,w =
:
3
−1

u1 +3u2
11

3u1 −2u2
11

−3

2

1

−1

3

0

Table 1.2: Checking the formulas from the proof of Fact 1.5 on three vectors
We can also understand this proof geometrically. The two equations 2λ + 3µ = u1 and
3λ − 1µ = u2 can be drawn as lines in the λµ-plane, and their point of intersection is the
desired solution to both equations. For u1 = −1, u2 = 4 (middle row of Table 1.2), the two
lines are drawn in Figure 1.7 (left). Unsurprisingly, their intersection is at (λ, µ) = (1, −1).

µ

µ

λ

λ
λ = 1, µ = −1
3λ − 1µ = 4

2λ + 3µ = u1
3λ − 1µ = u2

2λ + 3µ = −1

Figure 1.7: “Row picture” behind the proof of Fact 1.5, based on the rows of the equation
system. We find the target scalars by intersecting two lines in the λµ-plane.
The crucial observation is that for other values of u1 , u2 , the lines are different but
still parallel to the lines for u1 = −1, u2 = 4; see Figure 1.7 (right). So there is always
an intersection, meaning that we find values λ, µ for every vector u. This was the “row
picture” behind the proof.
There is also a “column picture”, see Figure 1.8. The two vectors v and w define axes
of a skewed coordinate system (solid lines). Given a target vector u, we make shifted
copies of both axes such that they cross in u (dashed lines). Between the two pairs of

18

 
u1
u=
u2

 
u1
u=
u2

y

 
2
=v
3
0

3
−1

y
λv
 
2
=v
3

µw
x

0


=w

3
−1

x


=w

Figure 1.8: “Column picture” behind the proof of Fact 1.5, based on the column vectors
of the equation system: We find the target scalars as coordinates in a skewed coordinate
system.
axes, a parallelogram emerges, and this is exactly the one that expresses u as a sum of
scaled versions of v and w, the gray arrows in Figure 1.8 (right).
Fact 1.5 may be wrong for other vectors. As an example, consider
 
 
2
4
v=
,w =
.
3
6
Here, w is a scalar multiple of v, so every linear combination of the two is just another
scalar multiple of v. Therefore, the linear combinations form a line through v, and a
vector not on that line cannot be obtained as a linear combination.
Challenge 1.6. Try to prove a version of Fact 1.5 where
 
 
v1
w1
v=
,w =
v2
w2
are arbitrary vectors. Where does the proof fail if the two vectors are scalar multiples of each other?
What goes wrong in the row and column pictures in this case?

1.1.4

Affine, conic, and convex combinations

In many applications, we are not interested in all linear combinations of the given vectors.
The following three types of special linear combinations are of particular importance.
Definition 1.7 (Affine, conic, convex combination). A linear combination λ1 v1 +λ2 v2 +· · ·+
λn vn of vectors v1 , v2 , . . . , vn is called
(i) an affine combination if λ1 + λ2 + · · · + λn = 1,
(ii) a conic combination if λj ≥ 0 for j = 1, 2, . . . , n, and
(iii) a convex combination if it is both an affine and a conic combination.
19

We will illustrate these notions with linear combinations λv + µw of two vectors
v, w ∈ R2 that are not scalar multiples of each other. By Challenge 1.6, the set of linear combinations of such vectors is the whole plane R2 ; see Figure 1.9 (i). What is the set
of affine combinations? If λ + µ = 1, we can write
λv + µw = λv + (1 − λ) w = w + λ(v − w).
| {z }
µ

Hence, in set builder notation (see also Section 1.1.5 below) the set of all affine combinations
is the set
{w + λ(v − w) : λ ∈ R},
the set of all vectors of the form w + λ(v − w) where λ is a real number.
This is the line through v and w (interpreted as points; see Figure 1.3). Plugging in
λ = 0 gives w, and for λ = 1, we obtain v. Values of λ between 0 and 1 lead to points in
between. For λ < 0 we end up left of w, and for λ > 1, we will be right of v. Generally,
any point on the line is obtained by the rule “go to w, then add a scalar multiple of v−w!”.
See Figure 1.9 (ii).

λv + µw
λv

v−w
v
w

v

v

w

w

v
w

µw
(i) linear

(ii) affine

(iii) conic

(iv) convex

combinations of two vectors
Figure 1.9: Two vectors in R2 and their linear, affine, conic, and convex combinations
In a conic combination, the parallelogram for adding λv and µw is always in the angle
between v and w , and by varying λ and µ, we can get every vector in this angle, officially
called the cone spanned by v and w; see Figure 1.9 (iii). Finally, the convex combinations
are by definition all points on the line through v and w that are also in the cone spanned
by v and w. This set is the line segment spanned by v and w, see Figure 1.9 (iv).
Challenge 1.8. Understand the affine, conic and convex combinations of three vectors in R2 !

v
w

u

20

1.1.5

Defining the dots: sequences, sums, sets, and vectors

The dot notations as used in

v1 , v2 , . . . , v n

and λ1 v1 + λ2 v2 + · · · + λn vn

and


v1
 v2 
 
 .. 
 . 
vm

are fine, but it is never too early to get to know the precise mathematical notations.
This will also help in really understanding the dot notations. Formally, when we write
v1 , v2 , . . . , vn , we mean the sequence (v1 , v2 , . . . , vn ) of n vectors; we typically omit the
surrounding brackets if we do not need the sequence itself as a mathematical object. There
is a more concise way of writing the sequence:
(v1 , v2 , . . . , vn ) = (vj )nj=1 .
Similarly, sums have a mathematical notation:
λ1 v1 + λ2 v2 + · · · + λn vn =

n
X

λj vj .

j=1

Here, j is called the summation index.
The benefit of this notation becomes apparent when we discuss some special cases.
What if n = 2? It is not entirely clear how to interpret v1 , v2 , . . . , vn in this case. Does
v2 now appear twice? And if n = 1? Then there is not even a vector v2 , so why is it
mentioned?
In starting with v1 , v2 , . . ., we want to indicate a pattern that continues until vn , and
we therefore mean (v1 , v2 ) if n = 2 and (v1 ) if n = 1. Still, it is potentially confusing to
always mention three vectors v1 , v2 and vn , even if there are less. In contrast, the notation
(vj )nj=1
has a clear definition: it is the sequence of all vectors vj where j goes through the range
from 1 to n in increasing order. This range contains the integers j that satisfy 1 ≤ j ≤ n.
This naturally covers the cases n = 1 and n = 2. For the sum, it is the same:
n
X

λj vj

j=1

is obtained by summing up all λj vj for which j goes through the range from 1 to n.
This brings us to the last special case: what if n = 0, so there are no vectors? The
mathematical notation handles this elegantly: as the range of integers from 1 to 0 is empty,
we have
(vj )0j=1 = (),
21

the empty sequence of vectors. And
0
X

λj vj = 0,

j=1

the zero vector. Indeed, if you add up vectors, you start from 0 and then add one vector
at a time. But if there are no vectors to add, you remain stuck at 0. This is like a scale
that shows a weight of 0 before you put anything on it. As a consequence, 0 is a linear
combination of every sequence of vectors, even the empty one.
Having made it clear that for n = 0, v1 , v2 , . . . , vn is the empty sequence and λ1 v1 +
λ2 v2 + · · · + λn vn = 0, there is also no harm if we keep
P using the dot notations. For
novices, they may be easier to read than (vj )nj=1 and nj=1 λj vj . But as in particular the
sum notation is standard throughout many sources, it is also good to get used to it early.
When we reorder a sequence of vectors, we get the same linear combinations, because
vector addition is commutative. Also, if a vector appears more than once in the sequence,
we can omit its duplicates and still get the same linear combinations. This means, all that
matters is the set of vectors involved in the sequence (vj )nj=1 . We will write this set as
{vj : j ∈ [n]}.
Here, [n] is an unambiguous notation for the the range from 1 to n, considered as a set.
What we see in action here is the set builder notation. The notation {vj : j ∈ [n]} means:
the set of all vj for which the condition j ∈ [n] behind the colon is true. So we can read
the colon as for which. In some sources, this is written as {vj | j ∈ [n]}, but the meaning is
the same. Here is another example. In set builder notation, we could define [n] as
[n] := {i : i ∈ N, 1 ≤ i ≤ n},
where a comma is to be read as and.
Within a set, the order does not matter, so we prefer the notation [n] over {1, 2, . . . , n}.
But the latter notation and also {v1 , v2 , . . . , vn } for the set of vectors are perfectly fine as
well. The important thing is that there is no order of vectors in the set, and every vector
only appears once. We can write {v1 , v1 , v2 }, but this is the same as {v1 , v2 } or {v2 , v1 }.
Finally, the vertical dots: a vector
 
v1
 v2 
 
 .. 
 . 
vm
m
can more concisely be written as (vi )m
i=1 . Indeed, a vector as an element of R is formally
just a sequence of m real numbers. This notation for example allows us to write down
the vector (i2 )5i=1 . What does this mean? The expression in brackets tells us what the i-th

22

coordinate of the vector should be, and the range after the bracket tells us which range of
i’s we are considering. Hence,
 
1
4
 

(i2 )5i=1 = 
 9 .
16
25
Similarly, (0)6i=1 is the 6-dimensional zero vector. Using this notation, we could also define
the sum of two vectors more “efficiently” than Definition 1.2 does it, namely as follows:
m
m
(vi )m
i=1 + (wi )i=1 := (vi + wi )i=1 .

We can still use the vertical dots notation as it is more readable, despite needing more
space. But now that we know what it precisely means, we for example understand that
for m = 1,
 
v1
 v2 
 
1
 ..  = (vi )m
i=1 = (v1 ) ∈ R .
 . 
vm
What about the case m = 0? As pointed out in Definition 1.1, R0 contains exactly one
element, namely the empty sequence () of real numbers. So (vi )0i=1 = () ∈ R0 .

1.2

Scalar products, lengths and angles

The scalar product of two vectors is a number that helps us in defining the length
of a vector and the angle between two vectors. Scalar products naturally appear
all over linear algebra and have many applications. Scalar products are also at the
heart of two important inequalities, the Cauchy-Schwarz inequality, and the triangle
inequality. We also introduce and explain the standard notation v⊤ w for the scalar
product.
Given how vector addition works (Definition 1.2), you might expect vector multiplication to work like this:
    
  
1
3
1·3
3
·
=
=
.
2
4
2·4
8
This is known as the Hadamard product, but in the range of linear algebra products, it only
occupies a niche. Among the topsellers, we find the scalar product whose result is not a
vector, but a number (or a scalar, in linear algebra jargon; this is where the name comes
from).

23

1.2.1

Scalar product

The scalar product of two vectors is obtained by multiplying corresponding coordinates,
and adding up the products:
   
1
3
·
= 1 · 3 + 2 · 4 = 11.
2
4
Definition 1.9 (Scalar product). Let



 
v1
w1
 v2 
 w2 
 
 
v =  ..  , w =  ..  ∈ Rm .
 . 
 . 
vm

wm

The scalar product of v and w is the number
v · w := v1 w1 + v2 w2 + · · · + vm wm =

m
X

vi wi .

i=1

Note that for v, w ∈ R0 , we get v · w = 0. For m = 1, scalar multiplication behaves like
the normal multiplication: (v1 ) · (w1 ) = v1 w1 .
From this definition, we can directly derive some rules that are frequently needed in
computing with scalar products. We summarize these in an observation. This is a statement whose proof is simple and straightforward enough to be omitted.
Observation 1.10. Let u, v, w ∈ Rm be vectors and λ ∈ R a scalar. Then
(i) v · w = w · v;

(symmetry)

(ii) (λv) · w = λ(v · w) = v · (λw);

(taking out scalars)

(iii) u · (v + w) = u · v + u · w and (u + v) · w = u · w + v · w;
(iv) v · v ≥ 0, with equality exactly if v = 0.

(distributivity)
(positive-definiteness)

While (i) and (iv) are quite obvious from the definition of the scalar product, (ii)
and (iii) need some straightforward calculations as a proof. To show what we mean by
“straightforward”, we provide the calculations for the first part of (iii). These use vector
addition (Definition 1.2) and distributivity in R:
u · (v + w) =

m
X
i=1

ui (vi + wi ) =

m
X

(ui vi + ui wi ) =

i=1

m
X
i=1

ui vi +

m
X

ui wi = u · v + u · w.

i=1

Properties (ii) and (iii) together are known as linearity in both arguments.
24

1.2.2

Euclidean norm

The Euclidean norm of a vector v is obtained by taking the square root of the scalar product with itself. We can do this, since this scalar product is nonnegative by Observation 1.10 (iv).
Definition 1.11 (Euclidean norm). Let v ∈ Rm . The Euclidean norm of v is the number
√
∥v∥ := v · v.
Some sources use |v| for the norm, but we reserve this notation for the absolute value
of a number and the size of a set; for example, |3| = | − 3| = 3 and |{2, 3, 5, 7}| = 4.
You can think of the Euclidean norm as defining the length of a vector. You may argue
that we do not need to define this, since a vector already has a length (just measure how
long the arrow is). But this is true only in R2 and R3 where we can draw vectors as arrows.
In higher dimensions, it is not a priori clear how to measure the length of a vector v. But
now we know: compute its Euclidean norm ∥v∥!
In R2 and R3 , the Euclidean norm indeed measures the length of the arrow; so we are
not really inventing a new concept of length here, we simply extend a familiar concept to
higher dimensions. To see this, we first expand the scalar product to obtain the following
formula for the Euclidean norm:
 
v
v1
u m
q
 v2 
uX
 
2
2
2
vi2 .
 ..  = v1 + v2 + · · · + vm = t
 . 
i=1
vm
For example,



p
√
−4
= (−4)2 + 22 = 20,
2

 
−4
p
√
 2 = (−4)2 + 22 + 32 = 29.
3

Let us first look at the situation in R2 where Figure 1.10 is the key.
The vector
 
v
v= 1
v2
is the hypotenuse of a right-angled triangle whose legs have lengths |v1 | and |v2 |, see
Figure 1.10. Hence, using the Pythagorean theorem, the squared length of v is
|v1 |2 + |v2 |2 = v12 + v22 = ∥v∥2 ,
and “length of v equals ∥v∥” is obtained by taking square roots. Building on this, Figure 1.11 deals with the situation in R3 .
25

 
v
v= 1
v2

|v1|

p
v12 + v22

900

|v2|

0
Figure 1.10: The Euclidean norm measures the length of a vector in R2 .
 
v1
v2
 
0
v1
|v1|
900
v = v2
p
v3
|v3|
v12 + v22
|v2|
p
v12 + v22 + v32

0

Figure 1.11: The Euclidean norm measures the length of a vector in R3 .
In R3 , the vector

 
v1
v = v2 
v3

p
is the hypotenuse of a right-angled triangle whose legs have lengths v12 + v22 (as just
computed) and |v3 |; see Figure 1.11. Thus, Pythagoras tells us that the squared length of
v is
q
2

v12 + v22 + |v3 |2 = v12 + v22 + v32 = ∥v∥2 .

Unit vectors. A unit vector is a vector u such that ∥u∥ = 1. In R2 , the unit vectors lie on
the unit circle with center 0 and radius 1; see Figure 1.12.
Definition 1.11 of the Euclidean norm and taking out scalars (Observation 1.10 (ii))
easily give that for every vector v ̸= 0, the vector
v
∥v∥
26

u

v
v
∥v∥

1
0

Figure 1.12: A unit vector u on the unit circle. For every nonzero vector v, the scaled
vector v/∥v∥ is a unit vector.
is a unit vector, where scalar division is just scalar multiplication with the reciprocal:
1
v
:=
v.
∥v∥
∥v∥
In Rm , there are m standard unit vectors. These are the ones that have one coordinate
equal to 1 and all others equal to 0. We use the notation ei for the standard unit vector that
has the 1 at the i-th coordinate (in abuse of notation, we call this ei in every dimension):
 
0
 
 
 
 .. 
1
0
0
.
 
3
m






R : e1 = 0 , e2 = 1 , e3 = 0
R : ei = 1 ← coordinate i
.
0
0
1
 .. 
0
In R2 , the two standard unit vectors are the ones in the directions of x- and y-axis. In
R , e3 goes along the z-axis; see Figure 1.13.
3

y

 
0
e2 =
1
x 
1
e1 =
0

y
 
0

e 3 = 0
1
z

 
0

e 2 = 1
0
 
x 1
e 1 =  0
0

Figure 1.13: The standard unit vectors in R2 and R3

Other norms. To stress the point that the length of a vector is nothing God-given, we
remark that there are many other norms for vectors that make sense and are being used.
27

Here are two examples, the 1-norm and the ∞-norm:
m
X

∥v∥1 :=

|vi | (1-norm)

i=1

and

m

∥v∥∞ := max |vi | (∞-norm).
i=1

In these norms, the “unit circles” look different, see Figure 1.14. The vectors in R2 that
have length 1 according to the 1-norm form a “diamond”; under the ∞-norm, we get a
square. The standard unit vectors are also unit vectors under the 1-norm and the ∞-norm.

u

u

u

0

0

0

kuk = 1

kuk1 = 1

kuk∞ = 1

Figure 1.14: Unit vectors in the Euclidean norm (left), 1-norm (middle), ∞-norm (right)
All three norms are special cases of p-norms, where p ≥ 1 can be any real number:
v
u m
uX
p
∥v∥p = t
|vi |p .
i=1

We see that the Euclidean norm is actually the 2-norm, but we still write it as ∥v∥, not
∥v∥2 , since for us, it is the “standard” norm. But some sources use the notation ∥v∥2 . The
∞-norm is an abuse of notation, since ∞ is not a real number. But as “p goes to infinity”,
the largest coordinate of v in absolute value is all that matters. In formulas,
lim ∥v∥p = ∥v∥∞ .

p→∞

1.2.3

Cauchy-Schwarz inequality

As innocent as it looks, the importance of this inequality cannot be overestimated.
Lemma 1.12 (Cauchy-Schwarz inequality). For any two vectors v, w ∈ Rm ,
|v · w| ≤ ∥v∥∥w∥.
Moreover, equality holds exactly if one vector is a scalar multiple of the other.
28

Generally, a lemma is a helper statement that may not be very interesting on its own;
but the more it actually helps, the more important it becomes. In this sense, a lemma is
like a Swiss army knife, where the Cauchy-Schwarz inequality is a highly multifunctional
one.
Proof. We first consider the case where v and w are unit vectors, so ∥v∥ = ∥w∥ = 1. Using
Observation 1.10 and Definition 1.11 of the Euclidean norm, we compute
· w} − 2v · w = 2 − 2v · w
0 ≤ (v − w) · (v − w) = v
·v + w
| {z
|{z}
∥v∥2
∥w∥2
z}|{
z }| {
0 ≤ (v + w) · (v + w) = v · v + w · w + 2v · w = 2 + 2v · w

⇒

v·w ≤

⇒

v · w ≥ −1.

1,

Summarized as |v · w| ≤ 1, this proves the Cauchy-Schwarz inequality for unit vectors.
Now suppose v and w are arbitrary vectors. If one of them is 0, the inequality holds
(both sides are 0). If v ̸= 0 and w ̸= 0, we can apply the previous calculations after scaling
v and w to unit length. This gives
−1 ≤

w
v
·
≤ 1,
∥v∥ ∥w∥

Taking out scalars according to Observation 1.10 (ii) and multiplying all three terms
with ∥v∥∥w∥ results in
−1 ≤

v·w
≤ 1 and
∥v∥∥w∥

− ∥v∥∥w∥ ≤ v · w ≤ ∥v∥∥w∥.

This can be summarized as |v·w| ≤ ∥v∥∥w∥ which proves the Cauchy-Schwarz inequality
in general.
For the “Moreover” part, we need to understand under which conditions equality
holds. Going back to the calculations with the unit vectors, we see that the conditions are
precisely 0 = (v − w)(v − w) or 0 = (v + w)(v + w). By positive-definiteness of the scalar
product (Observation 1.10(iv)), this translates to v − w = 0 or v + w = 0.
In other words, the two unit vectors are either the same or opposite vectors. For the
unit vectors v/∥v∥ and w/∥w∥, it is easy to see that this is the case exactly if v and w are
scalar multiples of each other. If equality is due to one of v and w being 0, it is still true
that one vector (namely 0) is a scalar multiple of the other one.
Here is an application of the Cauchy-Schwarz inequality. Imagine that you have m
squares with total area A, and you put them next to each other as in Figure 1.15. How
much horizontal space do you need at most?
To solve this, let v1 , v2 , . . . , vm be the side lengths of the squares. Then the horizontal
space needed is
m
X
vi .
i=1

29

?
Figure 1.15: Horizontal alignment of m squares with total area A
Let v ∈ Rm be the vector with coordinates v1 , v2 , . . . , vm . We know that
2

∥v∥ =

m
X

vi2 = A.

i=1

Furthermore, let 1∈ Rm be the vector with all coordinates equal to 1. We have ∥1∥ =
By the Cauchy-Schwarz inequality,
m
X

√

vi = 1 · v ≤ ∥1∥∥v∥ =

√

m.

√ √
m A,

i=1

so we need at most mA horizontal space. If √
v is a scalar multiple of 1 (meaning that all
squares have the same size), we need exactly mA. Otherwise, we need less.
Exercise 1.13. For the 1-norm and ∞-norm as defined on page 28, prove that the following inequalities hold for every vector v ∈ Rm .
√
∥v∥ ≤ ∥v∥1 ≤ m∥v∥
and
∥v∥∞ ≤ ∥v∥ ≤

1.2.4

√
m∥v∥∞ .

Angles

In R2 and R3 , the angle between two vectors is simply the angle between their arrows;
see Figure 1.16.

v
w

α

Figure 1.16: The angle α between two vectors v and w
As with the length, we are looking for a way to define the angle between two vectors
also in higher dimensions, in such a way that nothing changes in R2 and R3 . The CauchySchwarz inequality is the key here.
30

Definition 1.14 (Angle). Let v, w ∈ Rm be two nonzero vectors. The angle between them is the
unique α between 0 and π (180 degrees) such that
cos(α) =

v·w
∈ [−1, 1].
∥v∥∥w∥

(Here, the interval [−1, 1] = {x ∈ R : −1 ≤ x ≤ 1} comes from the Cauchy Schwarz inequality,
Lemma 1.12). In other words,


v·w
α = arccos
.
∥v∥∥w∥
Let us check that this coincides with the usual concept of angles in R2 . As the angle
does not depend on how long the arrows are, and whether we simultaneously rotate
them, we can look at the case where v = e1 and w is another unit vector, with an angle of
α between the arrows; see Figure 1.17.

w
w
1
sin(α)
α

cos(α)

0

∥v − w∥
∥v − w∥

v = e1
1 − cos(α)

1
sin(α)
− cos(α)

α
0

v = e1
1 − cos(α)

Figure 1.17: The angle α between two unit vectors in R2 . Left: α acute; right: α obtuse
From high school, we know that the legs of the gray triangle (whose hypotenuse is
∥w∥ = 1) are sin(α) and cos(α) (if α is an acute angle) or − cos(α) (if α is an obtuse angle).
In both cases, the red triangle with hypotenuse ∥v − w∥ therefore has legs sin(α) and
1 − cos(α). By the Pythagorean theorem,
∥v − w∥2 = sin2 (α) + (1 − cos(α))2 .
Using sin2 (α) + cos2 (α) = 1, the right-hand side is 2 − 2 cos(α). By definition of the Euclidean norm, the left-hand side is (v − w) · (v − w), and we have already argued in the
proof of Lemma 1.12 (Cauchy-Schwarz inequality) that this equals 2 − 2v · w. Hence, we
have shown
2 − 2v · w = 2 − 2 cos(α)
which simplifies to
cos(α) = v · w.
This indeed agrees with what Definition 1.14 says for unit vectors.
31



−1
2

 y
 
4
2
900

x

   
4
−1
·
= −4 · 1 + 2 · 2 = 0
2
2

Figure 1.18: Orthogonal vectors: the scalar product equals 0.
Definition 1.15 (Orthogonal vectors). Two vectors v, w ∈ Rm are orthogonal if v · w = 0; in
other words, if the cosine of the angle between them is 0 and the angle itself is 90 degrees.
Figure 1.18 gives an example.
Given a nonzero vector d ∈ Rm (for “direction”), we can collect all vectors v ∈ Rm
that are orthogonal to d. The result is a hyperplane through the origin.
Definition 1.16 (Hyperplane through the origin). Let d ∈ Rm , d ̸= 0. The set
Hd = {v ∈ Rm : v · d = 0}
is called a hyperplane through the origin.
Figure 1.19 illustrates this. Indeed, Hd is going through the origin by definition: because 0 is orthogonal to every vector, we have 0 ∈ Hd .

v

0
900

Hd

d

Figure 1.19: A hyperplane through the origin in R3 : if we imagine d as the leg of a table,
then Hd is the plane containing the table top.
A general hyperplane can be obtained from a hyperplane through the origin by a
parallel translation. We will get to general hyperplanes at a later point.

32

1.2.5

Triangle inequality

Lemma 1.17. Let v, w ∈ Rm . Then
∥v + w∥ ≤ ∥v∥ + ∥w∥.
In R2 , Figure 1.20 shows that this is quite obvious from the parallelogram of vector
addition (Figure 1.4).

v
kvk
0

kwk
v+w

kv + wk

kwk

kvk
w

Figure 1.20: The triangle inequality: going from 0 directly to v + w is shorter than making
a detour via v or w.
In higher dimensions, the following proof shows that the triangle inequality is nothing
but Cauchy-Schwarz in a different disguise.
Proof. Since both sides of the inequality are nonnegative, we can instead prove the squared
triangle inequality
∥v + w∥2 ≤ (∥v∥ + ∥w∥)2
and then take square roots on both sides to obtain the triangle inequality. To prove the
squared version, we compute
∥v + w∥2 =
=
=
≤
≤
=

(v + w) · (v + w) (Definition 1.11) of the Euclidean norm)
v · v + w · w + 2v · w (Observation 1.10 (iii) on scalar products)
∥v∥2 + ∥w∥2 + 2v · w (Definition 1.11 of the Euclidean norm)
∥v∥2 + ∥w∥2 + 2|v · w|
∥v∥2 + ∥w∥2 + 2∥v∥∥w∥ (Cauchy Schwarz inequality, Lemma 1.12)
(∥v∥ + ∥w∥)2 . (Binomial theorem)

Exercise 1.18. Turn this proof around and derive the Cauchy-Schwarz inequality from the (squared)
triangle inequality!
As a consequence, some sources say that the Cauchy-Schwarz inequality is equivalent
to the triangle inequality; this is another abuse of notation, since any two statements that
are both true are logically equivalent, even if they have otherwise nothing to do with each
other. What is meant here is that the triangle inequality can easily be proved using the
Cauchy-Schwarz inequality (as we did in the proof above), and vice versa, as we ask you
to do in Exercise 1.18.
33

1.2.6

Covectors and v⊤ w

So far, we have used the notation v · w for the scalar product of two vectors, but “in the
wild”, you also find other notations. A notable one is ⟨v, w⟩, but the most popular one
is v⊤ w. This notation is very useful in connection with matrices (see the next Chapter 2).
Here, we want to introduce and explain it.
Long story short. A vector v ∈ Rm is a mathematical object (sequence of m real numbers) that we write down as a column vector. Its transpose v⊤ is another mathematical
object called a covector that we write down as a row vector. For example, if
 

1
v=
, then v⊤ = 1 2 .
2
Generally, if



v1
 v2 

 
v =  ..  , then v⊤ = v1 v2 · · · vm .
 . 
vm
This notation explains why we use the term transpose and the symbol ⊤ . It exchanges
(“transposes”) a column and a row, and if we do this twice, we expect to arrive back at
the original, so (v⊤ )⊤ = v.
A covector v⊤ and a vector w of the same size can be multiplied, with the result being
exactly the scalar product.
Definition 1.19 (Scalar product as covector-vector multiplication). Let v, w ∈ Rm . Then
 
w1
m
 w2 
X

 
⊤
v w = v1 v2 · · · vm  ..  :=
vi wi = v · w.
 . 
i=1
wm
This kind of multiplication may seem unnecessary, as it does not do anything more
than Definition 1.9, after turning v into a covector v⊤ . For the long story short, it is sufficient to know that the above “transpose” and “covector times vector” operations also
appear in the context of matrices (Chapter 2), with essentially the same definitions for
matrices that look like row or column vectors. As a consequence, v⊤ and v⊤ w are the
natural language in (the many) situations involving both vectors and matrices. We will
discuss this in detail in Section 2.3.4.
Short story long. Now, what exactly is covector? It is formally a function, and we provide a brief introduction to functions as mathematical objects immediately after the following formal definition of a covector.
34

Definition 1.20 (Covector). Let v ∈ Rm be a vector. The covector v⊤ (also called the transpose
of v) is the function v⊤ : Rm → R,
⊤

v : x 7→

m
X

vi xi .

i=1

We also define (v⊤ )⊤ := v and call the vector v the transpose of the covector v⊤ .
In row vector notation, v⊤ is written as

v⊤ = v1 v2 · · · vm .
The set of m-dimensional covectors is denoted by (Rm )∗ and called the dual space of Rm . A
covector is in some sources also called linear form or linear functional.
As functions and their notation appear here for the first time, here is a very small
“crash course” on functions.
Generally, a function f : X → Y does the following: whenever it gets an “input” x
from its domain X (a set), it produces an “output” f (x) ∈ Y from its codomain Y (another
set). We also say that f maps x to f (x), or that we apply f to the input x to get the output
f (x). The same input always leads to the same output. The element f (x) ∈ Y is also
called the function value.
To describe how the mapping from input to output works, we use a definition of the
form f : x 7→ ”expression in x”. For example, f : R → R, f : x 7→ x2 is the function that
maps a given real number x ∈ R to its square x2 ∈ R. If f is clear from the context, we do
not have to repeat it in the definition. For example, the previous square function could
also be defined as f : R → R, x 7→ x2 .
Pm
⊤
In case of a covector v⊤ : Rm → R, the definition of the mapping
is
v
:
x
→
7
i=1 vi xi ,
Pm
m
⊤
i.e. given input x ∈ R , the output is thereal number v (x) = i=1 vi xi .
For example, the covector v⊤ = 3 4 ∈ (R2 )∗ is the function v⊤ : x 7→ 3x1 + 4x2 . Just
as vectors, covectors can be added and multiplied with scalars, by simply performing
the


corresponding operations on their underlying vectors. For example, 2 3 4 = 6 8 ,
the function x 7→ 6x1 + 8x2 . The covector 0⊤ is the zero covector, the function x 7→ 0.
With covectors, the scalar product of two vectors v, w ∈ Rn can be written as v⊤ (w)
(meaning that we apply the function v⊤ to the vector w). Indeed, by Definition 1.20, the
result of this is
m
X
⊤
v (w) =
vi wi = v · w,
i=1

the scalar product! If we omit the brackets around the function argument w (a very common abuse of notation), we arrive at the notation v⊤ w for the scalar product. So although
we think of v⊤ w as multiplication (of a covector and a vector), it formally is the application of the function v⊤ to the vector w.

35

1.3

Linear (in)dependence

Linear independence of vectors is probably the single most important concept of linear algebra. A sequence of vectors is called linearly independent if none of the vectors
is a linear combination of the others, and linearly dependent otherwise. We provide
a number of alternative definitions of linear (in)dependence that illuminate the concept from different angles. We also introduce the span of a sequence of vectors, the
set of all their linear combinations, and prove some important properties of the span.

1.3.1

Definition and examples

Definition 1.21 (Linear (in)dependence). Vectors v1 , v2 , . . . , vn ∈ Rm are linearly dependent if at least one of them is a linear combination of the others, i.e. there is an index k ∈ [n] and
scalars λj such that
n
X
vk =
λj vj .
j=1
j̸=k

Otherwise, v1 , v2 , . . . , vn are linearly independent.
Here, the additional “j ̸= k” below the sum adds a condition to the j’s considered in
the sum: take only the ones in the given range that satisfy the condition. Hence, the sum
is over all j except k, so the equation indeed says that vk is a linear combination of the
other vectors.
Let us do some examples. The two vectors
   
2
4
,
3
6
are linearly dependent, because
 
 
 
 
1 4
2
4
2
=
or
=2
.
6
3
3
2 6
We also call these two vectors collinear because they are are on the same line; see Figure 1.21 (left). In contrast, the two vectors
   
2
3
,
3
−1
that we have considered in Fact 1.5 are linearly independent, as none of them is a linear
combination (scalar multiple) of the other one; see Figure 1.21 (right).
Let us now consider three vectors in R2 . These are always linearly dependent: If two
of them are collinear, one of them is a linear combination of the other one and therefore
36

y

y
 
4
6
 
2
3

0

 
2
3

x

x

0


3
−1



Figure 1.21: Two linearly dependent vectors (left); two linearly independent vectors
(right)
of both other ones (pick scalar 0 for the second other one). Otherwise, each of the vectors
is a linear combination of the other two by Challenge 1.6.
What if we have just one vector v ∈ Rm ? Can v even be a linear combination of
the other vectors when there are no other vectors? Yes, if v = 0, because 0 is a linear
combination of the empty sequence of vectors (Section 1.1.5). But if v ̸= 0, the sequence
consisting only of the vector v is linearly independent.
Generally, when 0 is one of the vectors, they are automatically linearly dependent,
because 0 is always a linear combination of the other ones, even if there are no other ones.
Similarly, when some vector appears twice, the vectors are linearly dependent, because
one of the copies is already a linear combination of the other copy.
Finally, what about the empty sequence of vectors? This in turn is linearly independent by Definition 1.21: because [n] = ∅ in this case (∅ is the symbol for the empty set),
there is no index k ∈ [n], whatever we may require of it. Table 1.3 summarizes these
examples.
linearly
 independent
 
2
3
,
−1
3

linearly dependent

   
2
4
,
3
6
v1 , v2 , v3 ∈ R2
v ̸= 0
v=0
. . . , 0, . . .
. . . , v, . . . , v, . . .
empty sequence ()
Table 1.3: Linear (in)dependence of some sequences of vectors
37

1.3.2

Alternative definitions

There are two more important alternative definitions of linear dependence. The following
lemma provides them. In some sources, (ii) is the “standard” definition of linear dependence.
Lemma 1.22 (Alternative definitions of linear dependence). Let v1 , v2 . . . , vn ∈ Rm . The
following statements are equivalent (meaning that they are either all true, or all false).
(i) At least one of the vectors is a linear combination of the other ones. (This means, the vectors
are linearly dependent according to Definition 1.21.)
P
(ii) There are scalars λ1 , λ2 , . . . , λn besides 0, 0, . . . , 0 such that nj=1 λj vj = 0. We also say
that 0 is a nontrivial linear combination of the vectors.
(iii) At least one of the vectors is a linear combination of the previous ones.
For the proof, we apply the basic principles of logic. We first argue that (i) implies (ii),
meaning that if (i) is true, then also (ii) is true. Logically, this is written as (i)⇒(ii). Next
we prove (ii)⇒(iii) and (iii)⇒(i).
Having done this, we know that (i), (ii) and (ii) are equivalent: either all true or all
false, written as (i)⇔(ii)⇔(iii). Indeed, because of the three (circular) implications, it cannot be that one of the statements is true and another one is false.
In math prose, an equivalence such as (i)⇔(ii) is also written as “(i) if and only if (ii)”.
This summarizes the two implications : “(i) if (ii)” writes out (ii)⇒(i), while “(i) only if
(ii)” excludes the possibility that (i) is true and (ii) is false. In other words, it writes out
the implication (i)⇒(ii).
Proof.
(i)⇒(ii): If at least one of the vectors, vk say, is a linear combination of the other vectors,
then
n
X
vk =
λj vj .
j=1
j̸=k

This can be written as
0=

n
X

λj vj − vk ,

j=1
j̸=k

so if we define λk = −1, we get
0=

n
X

λj vj .

j=1

Hence, 0 is a nontrivial linear combination of the vectors (it is nontrivial because λk ̸= 0).

38

(ii)⇒(iii): If 0 is a nontrivial linear combination of the vectors, then we can write 0 in
the form
n
X
0=
λj vj ,
j=1

where not all λj are zero. Let k be the largest index such that λk ̸= 0. Then we actually
have
k
k−1
X
X
0=
λj vj , which we can also write as λk vk =
−λj vj .
j=1

j=1

Dividing by λk ̸= 0, we get

k−1 
X
λj
vj .
vk =
−
λk
j=1
Hence, at least one vector, namely vk , is a linear combination of the previous ones.
(iii)⇒(i): If at least one vector is a linear combination of the previous ones, the same
vector is also a linear combination of the other ones (use scalar 0 for vectors after it).
We have in particular proved that (i) implies (iii). So if some vector is a linear combination of the other ones, then some vector is a linear combination of the previous ones.
However, this is not not necessarily the same vector. As an example, consider the sequence
     
1
1
0
,
,
.
1
0
1
Here, the first vector is a linear combination of the other two, but the first vector is not
a linear combination of the previous ones (there are no previous ones, and the vector is
nonzero). Here, only the third vector is a linear combination of the previous ones.
Here are the corresponding alternative definitions of linear independence. These are
simply obtained by taking the opposites (logical negations) of (i)–(iii) in Lemma 1.22: If
some statements are either all true or all false, the same holds for their opposites.
We formulate the resulting definitions as a corollary which is a result that directly follows from (is implied by) a previous one (in this case Lemma 1.22), without the need for a
proof (or only a very simple proof such as “take the opposites of all statements!”).
Corollary 1.23 (Alternative definitions of linear independence). Let v1 , v2 . . . , vn ∈ Rm .
The following statements are equivalent (meaning that they are either all true, or all false).
(i) None of the vectors is a linear combination of the other ones. (This means, the vectors are
linearly independent according to Definition 1.21.)
P
(ii) There are no scalars λ1 , λ2 , . . . , λn besides 0, 0, . . . , 0 such that nj=1 λj vj = 0. We also say
that 0 can only be written as a trivial linear combination of the vectors.
(iii) None of the vectors is a linear combination of the previous ones.
39

This has another important consequence: a linear combination of linearly independent
vectors can be written as a linear combination in only one way.
Lemma 1.24. Let v1 , v2 . . . , vn ∈ Rm be linearly independent, v ∈ Rm . Let
v=

n
X

λj vj =

j=1

n
X

µj vj

j=1

be two ways of writing v as a linear combination of v1 , v2 . . . , vn . Then λj = µj for all j ∈ [n].
P
Proof. Subtracting the two linear combinations gives 0 = nj=1 (λj − µj )vj . Since 0 can
only be written as a trivial linear combination of v1 , v2 . . . , vn by Corollary 1.23 (ii), we
get λj − µj = 0 for all j, so λj = µj for all j.

1.3.3

Span of vectors

The set of all linear combinations of some vectors is important enough to deserve a name.
Definition 1.25 (Span). Let v1 , v2 , . . . , vn ∈ Rm . Their span is the set of all linear combinations.
In formulas,
( n
)
X
Span(v1 , v2 , . . . , vn ) :=
λj vj : λj ∈ R for all j ∈ [n] .
j=1

As an example, let us consider the span of three vectors v1 , v2 , v3 in R3 . There are three
cases, see Figure 1.22.



−3  
 3 −2 y−1
 2
 1
3
2
1




−2
−3   y
2
 3
0
3


x
z




−2
 2 y
0






2
 3
−1

x 


−1
 1
z
3


−1
 1 z
3

x

Figure 1.22: The span of three vectors in R3 : a line, a plane, or the whole space
We do not yet have the tools to prove this here, but simply appeal to the geometric
intuition: the span can be a line through the origin (vectors are collinear), a plane through
40

the origin (vectors are coplanar), or the whole space (vectors are linearly independent). In
Section 4.3.1, we will have the tools available that allow us to formalize the intuition.
The attentive reader might have noticed that there is a fourth (but pretty boring) case:
if v1 = v2 = v3 = 0, then 0 is the only linear combination, so the span consists of a single
point at the origin. Here are some more observations to illustrate the concept.
We always have 0 ∈ Span(v1 , v2 , . . . , vn ) (obtained by setting all λj to 0). This even
holds if n = 0 and there are no vectors. As we have argued in Section 1.1.5, 0 is a linear
combination of the empty sequence of vectors (and the only one), so Span() = {0}.
In “span language,” Fact 1.5 can be rewritten as follows:
   
2
3
Span
,
= R2 .
3
−1
The span of two nonzero vectors that are scalar multiples of each other is always a
line. For example,
      

2
4
2
Span
,
= λ
:λ∈R .
3
6
3
Figure 1.23 illustrates these two cases.

y

y
 
4
6
 
2
3

0

 
2
3

x

0

3
−1

x



Figure 1.23: The span of two vectors in R2 : a line, or the whole space
Next we prove a very useful statement that may seem obvious from Figure 1.22 but
needs a proof: the span of vectors does not change when we add a linear combination of
them as a new vector.
Lemma 1.26. Let v1 , v2 . . . , vn ∈ Rm , and let v ∈ Rm be a linear combination of v1 , v2 . . . , vn .
Then
Span(v1 , v2 , . . . , vn ) = Span(v1 , v2 , . . . , vn , v) .
|
{z
} |
{z
}
S

T

For the proof, we need to show that two sets S and T are equal. For this, we typically
argue that each element of S is contained in T (then S is a subset of T , in formulas S ⊆ T )
41

and—vice versa—that each element of T is contained in S (then T ⊆ S). Having done
this, there cannot be an element which is in one of the sets and not in the other one (same
logic as with the implications in Lemma 1.22), so the two sets must be equal.
Proof of Lemma 1.26.
S ⊆ T : Each element w ∈ S is a linear combination of v1 , v2 , . . . , vn and therefore also
a linear combination of v1 , v2 , . . . , vn , v (add the scalar multiple 0v). So w ∈ T .
T ⊆ S: each element w ∈ T is a linear combination of v1 , v2 , . . . , vn , v,
w=

n
X

λj vj + λv.

j=1

But since v is a linear combination of v1 , v2 . . . , vn , we can also write v as
v=

n
X

µj vj .

j=1

Plugging the second equation into the first one, we get
w=

n
X
j=1

λj vj + λv =

n
X

λj vj + λ

j=1

n
X
j=1

!
µj v j

n
X
=
(λj + λµj )vj .
j=1

This means that w is a linear combination of v1 , v2 , . . . , vn and hence in S.
The fact that the new vector is the last one in the right sequence of Lemma 1.26 has
no significance. Because vector addition is commutative, the span does not depend on
how we order the vectors. So another useful “backward” way of reading this lemma is
the following: if some vector in a sequence is a linear combination of other vectors in the
sequence, this vector can be removed from the sequence without changing the span. We
formalize this in a corollary.
Corollary 1.27. Let v1 , v2 . . . , vn ∈ Rm and suppose that for some k ∈ [n], vk is a linear combination of the other vectors. Then
Span(v1 , v2 , . . . , vn ) = Span(v1 , v2 , . . . , vk−1 , vk+1 , vk+2 , . . . , vn ).
We conclude this chapter with another important result that seems obvious from our
geometric intuition but needs a proof.
Lemma 1.28 (The span of m linearly independent vectors is Rm ). Let v1 , v2 , . . . , vm ∈ Rm
be linearly independent. Then Span(v1 , v2 , . . . , vm ) = Rm .
Recall Challenge 1.6 that asks you to prove that the span of two linearly independent
vectors v, w ∈ R2 is the whole plane R2 . For two specific such vectors, we have proved
this in Fact 1.5, but the same proof can be extended such that it works for any two linearly
independent vectors. Geometrically, we can understand this with a column picture as in
Figure 1.8: if v and w are not pointing into the same direction, any target vector u can be
expressed as the sum of scaled versions of v and w:
42

y

u

λv
v
µw
x

0
w

In the proof of Fact 1.5, we have found the scaling factors λ and µ by solving a system of two equations in two variables. We could follow the same approach to prove
Lemma 1.28: every target vector u ∈ Rm is a sum of scaled versions of the vi , i ∈ [m],
and the corresponding scaling factors λi , i ∈ [m] can be found by solving a system of m
equations in m variables. Unfortunately, we will develop the necessary theory for this
only in Chapter 3. But maybe this is actually fortunate, because there is a simpler proof
that we can do already now.
Proof of Lemma 1.28. The strategy is the following: we start with a sequence of m vectors
whose span is obviously the whole space Rm . One by one, we replace them by the vi′ s
and argue that the span never changes. In the end, the sequence is v1 , v2 , . . . , vm ∈ Rm ,
and the span is still Rm .
The “obvious” sequence is e1 , e2 , . . . , em , consisting of the m standard unit vectors; see
page 27. Every target vector u ∈ Rm can be written as the linear combination
u=

m
X

ui ei ,

i=1

so the span is indeed the whole space Rm ; in R3 , this looks like
 
 
 
 
1
0
0
u1
u2  = u1 0 + u2 1 + u3 0 .
0
1
u3
0
Now we consider the sequence
v1 , e1 , e2 , . . . , em ,
obtained by adding v1 before the standard unit vectors. Since Span(e1 , e2 , . . . , em ) = Rm ,
we know that v1 ∈ Rm is a linear combination of e1 , e2 , . . . , em , and hence the m+1 vectors
are linearly dependent by Definition 1.21. Then we also know that one of the vectors
is a linear combination of the previous ones in the sequence, see Lemma 1.22 (iii). This
vector cannot be v1 because v1 also starts the linearly independent sequence v1 , v2 , . . . , vm
in which no vector is a linear combination of the previous ones by Corollary 1.23 (iii).
Therefore, one of the ei ’s is a linear combination of the previous vectors. Removing that
43

vector does not change the span (see the previous Corollary 1.27), so we get a sequence
v1 , u2 , u3 , . . . , um (the ui ’s name the m − 1 remaining standard unit vectors) with
Span(v1 , u2 , u3 , . . . , um ) = Rm .
So we have successfully replaced one of the standard unit vectors with v1 , without changing the span. Now we do the same with v2 : we add v2 directly after v1 and argue as before
that the m + 1 vectors
v1 , v2 , u2 , u3 , . . . , um
must be linearly dependent; therefore one of them is a linear combination of the previous ones. This can neither be v1 nor v2 , because v1 , v2 also start a sequence of linearly
independent vectors. Hence, some ui is a linear combination of the previous vectors and
can be removed without changing the span. We call the m − 2 remaining standard unit
vectors u3 , u4 , . . . , um (a slight abuse of notation, as this redefines the ui ’s) and get
Span(v1 , v2 , u3 , u4 , . . . , um ) = Rm .
By now, the pattern should be clear. After k replacement steps, we have m − k remaining standard unit vectors uk+1 , uk+2 , . . . , um and
Span(v1 , v2 , . . . , vk , uk+1 , uk+2 , . . . , um ) = Rm .
After k = m steps, we get our desired result:
Span(v1 , v2 , . . . , vm ) = Rm .
What we have done in this proof (exchange vectors without changing the span) is an
important technique. It will reappear later in the proof of a more general and fundamental
result of linear algebra, the Steinitz exchange lemma, see Section 4.2.3. This particular proof
is due to Oleksandr Kulkov, TA for this course in HS24.

44

Chapter 2
Matrices
Upfront, a matrix is just a table of real numbers, for example


3
5
2 −1
 0 −6
5
4 .
−3
2 −6
8
But unlike column or row vectors that are (for us) simply notations for vectors and covectors, we treat matrices as mathematical objects on their own. In linear algebra, matrices are used to compactly represent sequences of vectors (the columns of the matrix)
or covectors (the rows of the matrix). But most importantly, matrices can represent linear
transformations that act on vectors and shapes according to specific rules:

−→ linear transformation −→

Linear transformations are everywhere. Computer graphics is a classical application.
Whenever 3D objects move or rotate in a computer game, linear transformations happen
in the background. Today, neural network computations in machine learning and artificial intelligence involve massive amounts of linear transformations. Consequently, AI
chips are optimized for fast matrix computations. In this chapter, we get to know matrices
in detail and understand how they represent linear transformations.
Important operations such as matrix-vector multiplication, matrix multiplication, or
matrix inversion are motivated from the point of view of linear transformations.
Advanced matrix concepts such as determinants, eigenvalues and eigenvectors are
not covered in this chapter; they will be introduced in the second part of the course.
It can be quite interesting to ask “the Internet” (try Google, or ChatGPT, for example)
what the difference between vectors and matrices is. Here is an answer that you might
not easily get but that conveys the spirit of the above introduction: vectors are the “raw
material” of linear algebra, while matrices are an important part of the “toolkit” that has
been developed to work with vectors.
45

2.1

Matrices and linear combinations

A matrix is a table of numbers. This can be seen as an efficient notation for a sequence
of vectors (the columns of the matrix), or a sequence of covectors (the rows of the
matrix). Therefore, matrices turn out to be very useful in arguing about or computing
with sequences of (co)vectors and their linear combinations. Central matrix concepts
that we introduce here are matrix-vector multiplication, the column space, the row
space, the transpose, the rank, and the nullspace.
We often work with sequences of vectors. A matrix can be considered as a more compact notation for such a sequence. For example,


   
1 2
1
2
3 4 is a 3 × 2 matrix (3 rows, 2 columns) that represents the sequence 3 , 4
5 6
5
6
of two column vectors in R3 . Using the matrix, we save brackets and commas, but more
importantly, it also represents a sequence of three covectors in (R2 )∗ (see Section 1.2.6):



1 2 , 3 4 , 5 6 .
Definition 2.1 (Matrix). An m×n matrix is a table of real numbers with m rows and n columns.
We use upper-case letters (A, B, . . .) to denote matrices, and write their entries with the corresponding lower case letters and two indices, as in


a11 a12 · · · a1n
 a21 a22 · · · a2n 


A =  ..
..
..  .
.
.
 .
.
.
. 
am1 am2 · · · amn
Hence, aij is the entry in row i and column j of matrix A. The “dot-free” notation (see also
Section 1.1.5) is
n
A = [aij ]m
i=1,j=1 .

| |

| |

|

|

The set of m × n matrices is denoted by Rm×n .
If we want to talk about the columns of A as vectors or the rows of A as covectors, we use
column notation or row notation,


u⊤


1
|
|
|


u⊤
2


A= 
A = v1 v2 · · · vn ,
 .
..


.
|
|
|
⊤
{z
}
|
um
|
{z
}
column notation with
row notation with

v1 , v2 , . . . , vn ∈ Rm

⊤
⊤
n ∗
u⊤
1 , u2 , . . . , um ∈ (R )

46

Here,




a1j
 a2j 



vj =  ..  (j-th column), and u⊤
i = ai1 ai2 · · · ain (i-th row).
 . 
amj

While a vector needs one dot symbol in “dot notation”, a matrix needs seven. This is
significant enough to use the other notations (dot-free, column, row) more frequently for
matrices.
Some sources use the notation ai,j instead of aij , and indeed, this would avoid some
ambiguities, in particular for concrete values of i and j. For example, we use a12 for the
second entry in the first row of matrix A, but this could be misunderstood as the twelfth
entry of a vector a. The notation a1,2 would avoid this misunderstanding.
However, if we are in the context of matrices, and there are only two index symbols
(such as ij or 12), then things are clear: the first symbol stands for the row, and the second
one for the column. If there are more symbols, it is different: we better write a37,25 to refer
to the entry in row 37 and column 25 of A, since the ambiguities in a3725 cannot easily be
resolved. But as more than two symbols are rare, it is customary and economical to use
separating commas only in this case and not always.
What is a matrix as a mathematical object? Definition 2.1 of a matrix was rather informal (“table of real numbers”). For all practical purposes, this is sufficient and intuitive.
But we can easily turn this into a formal definition: let us say that a matrix is officially a
function A : [m] × [n] → R: for each i ∈ [m] (row index) and j ∈ [n] (column index), the
function value A(i, j) is some real number, providing the entry in row i and column j of
the matrix. Thus, the “table of real numbers” is simply the value table of the function A,
and aij is a shortcut for A(i, j). The symbol Rm×n that we have introduced for the set of
all matrices corresponds to this definition.
After this digression, let us turn to what we can do with matrices: Just like vectors, two
matrices of the same shape can be added and multiplied with a scalar, as in the following
examples:

 
 


 

1 2
5 6
6 8
1 2
2 4
+
=
, 2
=
.
3 4
7 8
10 12
3 4
6 8
Definition 2.2 (Matrix addition, scalar multiplication, zero matrix, square matrix). Let
n
m n
A = [aij ]m
i=1,j=1 and B = [bij ]i=1,j=1 be m × n matrices, λ ∈ R a scalar.
n
(i) The matrix A + B := [aij + bij ]m
i=1,j=1 is the sum of A and B.
n
(ii) The matrix λA := [λaij ]m
i=1,j=1 is a scalar multiple of A.
n
(iii) The matrix [0]m
i=1,j=1 is the m × n zero matrix, written as 0.

(iv) If m = n (number of rows equals number of columns), then A is a square matrix.
47

The non-square matrices come in two kinds of shapes with intuitive names. We have
the tall matrices with more rows than columns, and the wide matrices with more columns
than rows; see Figure 2.1.

m
m
n
tall
m>n

m

n
square
m=n

n
wide
m<n

Figure 2.1: Matrix shapes
Square matrices are particularly important, and we often consider additional properties of them. Before we provide the general definitions, we give some 3 × 3 examples.

 







1 0 0
2 0 0
2 1 0
2 0 0
2 1 0
0 1 0 0 4 0
0 4 7
1 4 0
1 4 7
0 0 1
0 0 5
0 0 5
0 7 5
0 7 5
identity
matrix

diagonal
matrix

upper triangular
matrix

lower triangular symmetric
matrix
matrix

m
Definition 2.3 (Square matrix classes). Let A = [aij ]m
i=1,j=1 be an m × m square matrix. If
i < j / i = j / i > j, then aij is said to be above / on / below the diagonal.

i=

i<j

j

i>j
(i) If aii = 1 for all i (entries on the diagonal are 1) and aij = 0 for all i ̸= j (entries not on the
diagonal are 0), then A is the identity matrix, denoted (in abuse of notation) by I for every
m. A different way of defining I is as
m
I := [δij ]m
i=1,j=1 .

Here, δij is the Kronecker delta, defined as 1 if i = j and 0 otherwise.
(ii) If aij = 0 for all i ̸= j (entries not on the diagonal are 0), then A is a diagonal matrix.
(iii) If aij = 0 for all i > j (entries below the diagonal are 0), then A is an upper triangular
matrix.
48

(iv) If aij = 0 for all i < j (entries above the diagonal are 0), then A is a lower triangular
matrix.
(v) If aij = aji for all i, j, then A is a symmetric matrix.
Note that (ii)-(iv) each require that some entries are zero, but not that the other entries
are nonzero. For example, the m × m zero matrix is at the same time diagonal, upper
triangular, and lower triangular, even though we do not see a “diagonal”, or a “triangle”
in it. Also whenever we say things like “for all i”, we mean “for all applicable i”, in this
case i ∈ [m].

2.1.1

Matrix-vector multiplication

Here is the efficient “matrix way” of writing down a linear combination as a matrix-vector
product:


 
 
 
1 2  
23
1
2
7







3 4
53 .
=
=
7 3 +8 4
8
5 6
83
5
6
{z
}
|
{z
}
| {z }
|
linear combination

matrix-vector product

result

Definition 2.4 (Matrix-vector multiplication with A in column notation). Let
 
x1


|
|
|
 x2 
 
A = v1 v2 · · · vn  ∈ Rm×n , x =  ..  ∈ Rn .
.
|
|
|
xn
The vector
Ax :=

n
X

xj vj ∈ Rm

j=1

is the product of A and x.
This allows us to express linear combinations (Definition 1.4) and linear (in)dependence
(according to Corollary 1.23 (ii)) in “matrix language”.
Observation 2.5. Let A be an m × n matrix.
(i) A vector b ∈ Rm is a linear combination of the columns of A if and only if there is a vector
x ∈ Rn (of suitable scalars) such that Ax = b.
(ii) The columns of A are linearly independent if and only if x = 0 is the only vector such that
Ax = 0.

49

We can also think of matrix-vector multiplication as transforming an “input vector”
x ∈ Rn into an “output vector” Ax ∈ Rm , where the matrix A tells us how to do this
transformation. We will explore this interpretation of the matrix-vector product in detail
in Section 2.2, but it is useful to have it in mind already here. Pictorially, matrix-vector
multiplication looks like this:

A

x

m

Ax

n = m
n

It is also important to understand the product in the other matrix notations.
Observation 2.6 (Matrix-vector multiplication with A in table notation). Let

 

a11 a12 · · · a1n
x1
 a21 a22 · · · a2n 
 x2 

 

m n
m×n
A =  ..
, x =  ..  = (xj )nj=1 ∈ Rn .
..
..  = [aij ]i=1,j=1 ∈ R
.
.
 .
.
.
.
. 
am1 am2 · · · amn
xn
Then




a11 x1 + a12 x2 + · · · + a1n xn
 a21 x1 + a22 x2 + · · · + a2n xn 


Ax = 
=
..


.
am1 x1 + am2 x2 + · · · + amn xn

n
X
j=1

!m
∈ Rm .

aij xj
i=1

This is in many sources the official definition of matrix-vector multiplication. It does
not explicitly refer to the columns or rows of A but just looks at A as a table. But we easily
see that both definitions say the same, by annotating the columns in Observation 2.6:




a11 a12 · · · a1n
a11 x1 + a12 x2 + · · · + a1n xn
 a21 a22 · · · a2n 
 a21 x1 + a22 x2 + · · · + a2n xn 




 ..



.
.
.
.
..
..
..  , Ax = 
..
A=  .
 .
am1 am2 · · · amn
am1 x1 + am2 x2 + · · · + amn xn
v1 v 2 · · · vn
x1 v 1 + x2 v 2 + · · · + xn v n
An immediate consequence is the following.
Corollary 2.7 (Identity-vector multiplication). Let I be the m × m identity matrix (Definition 2.3). Then Ix = x for all x ∈ Rm .
Finally, we ce can also use row notation of A to define Ax in terms of scalar products.
50

| |

| |

|

|

Observation 2.8 (Matrix-vector multiplication with A in row notation). Let




⊤
u
x
u⊤
1
1


 u⊤ x 
u⊤
2
2 



A=
 ∈ Rm×n , x ∈ Rn . Then Ax =  ..  .
..


 . 
.
⊤
um
u⊤
mx
| {z }
m scalar products

This is seen to be correct by annotating the rows in Observation 2.6:




a11 a12 · · · a1n u⊤
a11 x1 + a12 x2 + · · · + a1n xn
u⊤
1
1x
 a21 a22 · · · a2n  u⊤
 a21 x1 + a22 x2 + · · · + a2n xn  u⊤ x

 2

 2
A =  ..
 .. .
..
..  .. , Ax = 
..
.
.
 .

 .
.
.
.  .
.
⊤
am1 am2 · · · amn um
am1 x1 + am2 x2 + · · · + amn xn u⊤
mx
Observation 2.8 formalizes the way in which many people do matrix-vector multiplication in practice: to get the i-th entry of the output vector Ax, multiply (scalar product)
the i-th row of A with the input vector x. Here is how this can be visualized for the initial
example of this section:


 


 


 
1 2  
23
1 2  
23
1 2  
23
7
7
7
 3 4
3 4
 3 4
=  53 
= 53
=  53 
8
8
8
5 6
83
5 6
83
5 6
83

1 · 7 + 2 · 8 = 23

2.1.2

3 · 7 + 4 · 8 = 53

5 · 7 + 6 · 8 = 83

Column space and rank

Definition 2.9 (Column space). Let A be an m × n matrix. The column space C(A) of A is
the span (set of all linear combinations) of the columns,
C(A) := {Ax : x ∈ Rn } ⊆ Rm .
Here we use Definition 2.4 of the matrix-vector product Ax as the linear combination
of the columns with scalars from x. When we consider all possible vectors x ∈ Rn , we
obtain all possible linear combinations (the span) of the columns. Note that we always
have 0 ∈ C(A) because A0 = 0.
Using the column space, the statement of Fact 1.5 can be written as


   
2
3
2
3
C
= Span
,
= R2 .
3 −1
3
−1
See Figure 1.23 (right) for an illustration. In general, whenever A is a 2 × 2 matrix with
linearly independent columns, C(A) = R2 holds; see Challenge 1.6.
A crucial parameter of a matrix is its rank. The rank is defined as the number of
independent columns. A column is called independent if it is not a linear combination of
previous columns.
51

Definition 2.10 ((In)dependent column and (column) rank of a matrix). Let


|
|
|
A = v1 v2 · · · vn 
|
|
|
be an m × n matrix with columns v1 , v2 , . . . , vn ∈ Rm . Column vj is called independent if vj
is not a linear combination of v1 , v2 , . . . , vj−1 . Otherwise, vj is called dependent. The rank of
A, written as rank(A), is the number of independent columns of A. The rank is sometimes also
called the column rank to distinguish it more explicitly from the row rank, see Definition 2.14
below.
This means, rank(A) is a number between 0 and n. We have rank(A) = n exactly if no
column is a linear combination of the previous ones. According to Corollary 1.23 (iii), this
is the same as saying that the columns are linearly independent. The case rank(A) = 0
happens exactly if A = 0, the zero matrix. For A = 0, every (even the first) column is a
linear combination of the previous ones, because 0 is a linear combination of the empty
sequence of vectors; see Section 1.1.5.
Figure 2.2 provides two more examples.



2 4
rank
=2
3 1

y

y

 
2
3

 
2
3



2 4
rank
=1
3 6

 
4
1
x

 
4
6

x

Figure 2.2: Ranks of two 2 × 2 matrices: 2 when both columns are independent (left), or 1
when only the first column is independent (right)
If we reorder the columns of a matrix, we may get other independent columns. For
example, the two matrices




2 4
4 2
and
3 6
6 3
have the same columns, but in different order. Each of the two matrices has one independent column, namely its first one, but these are different. Still, both matrices have the
same rank 1. This is not a coincidence. We will take this up again in the example immediately preceding Section 4.2.3; a consequence of the results in Section 4.2.3 is that the rank
of a matrix does not change when we reorder the columns.
The independent columns of a matrix A are also of interest, because they already span
the column space of A.
52

Lemma 2.11 (The independent columns span the column space). Let A be an m × n matrix with r independent columns, and let C be the m × r submatrix containing the independent
columns. Then C(A) = C(C).
By a submatrix of a matrix A, we mean a matrix obtained from A by selecting some
rows and/or columns. Here, C is obtained from A by selecting the independent columns.
Proof. Let u1 , u2 , . . . , ur be the independent columns of A, and w1 , w2 , . . . , wn−r the dependent columns (in the same order as they appear in A). We will prove that
Span(u1 , u2 , . . . , ur , w1 , w2 , . . . , wn−r ) = Span(u1 , u2 , . . . , ur ) .
{z
} |
{z
}
|
C(A)

C(C)

We first observe that wj is a linear combination of u1 , u2 , . . . , ur , w1 , w2 , . . . wj−1 , for all j.
Indeed, by Definition 2.10, a dependent column is a linear combination of the previous
columns in A, and the sequence u1 , u2 , . . . , ur , w1 , w2 , . . . wj−1 contains all those (and possibly a few extra independent ones). Hence, if we start from the sequence u1 , u2 , . . . , ur
and then add w1 , w2 , . . . , wn−r one by one, Lemma 1.26 guarantees that the span of the
sequence never changes.

2.1.3

Row space and transpose

Recall that a matrix also represents a sequence of covectors, namely its rows. So we can
also define the row space R(A) of a matrix: the span of its rows. We have not officially
defined the span of a sequence of covectors, and we are not planning to do so. It would
be possible to make “covector copies” of all our vector definitions (or silently apply the
vector versions to covectors, via abuse of notation). But this is not necessary. Instead, for
the definition of the row space, we treat the rows as columns (of another matrix) and not
as covectors. This is more practical, and Definition 2.14 of row space below does it in a
clean way. Figure 2.3 illustrates the row spaces of the matrices from Figure 2.2. But we
still think about the row space of a matrix as being spanned by the rows.

y

y


2 4
R
= R2
3 1

 
2
4
 
3
1



2 4
R
3 6
  =

2
λ
:λ∈R
4

x

 
3
6
 
2
4
x

Figure 2.3: Row spaces of two 2 × 2 matrices: the whole plane (left) when the rows are
linearly independent, or a line (right) when the rows are linearly dependent
53





3 2 1

6 5 4

For both matrices, the number of independent columns (rank) equals the number of
independent rows (row rank). Is this a coincidence? No! It turns out that this is true for
every matrix. This is quite surprising, and we will prove it in Section 4.3.2.
What we will do here is prepare the ground. We are still missing formal definitions
of row space, independent row, and row rank of a matrix A. For this, we express the
rows (covectors) of A as the columns (vectors) of another matrix A⊤ , and then define the
row space of A as the column space of A⊤ , an independent row of A as an independent
column of A⊤ , and the row rank of A as the (column) rank of A⊤ .
This needs the concept of matrix transposition. The transpose A⊤ of a matrix A is another matrix, obtained by “mirroring” A along the diagonal “ ⧹”, the line going through
the diagonal entries a11 , a22 , . . . of A. Figure 2.4 shows a physical such mirror image and
its mathematical abstraction where we do not screw up the number and bracket symbols.
The effect of this mirroring is that the rows of A become the columns of A⊤ .

1 2 3

↔
4 5 6











1 2 3
A=
4 5 6

↔


1 4
A ⊤ = 2 5 
3 6






Figure 2.4: Mirroring a matrix along the diagonal, physically and mathematically
n
Definition 2.12 (Transpose). Let A = [aij ]m
i=1,j=1 be an m × n matrix. The transpose of A is
the n × m matrix
m
A⊤ := B = [bij ]ni=1,j=1
,

where bij = aji for all i, j.
This means, the entry of A⊤ in row i and column j is aji , the entry of A in row j and
column i. Transposing a matrix thus interchanges columns with rows. Previously, we
have transposed vectors to turn them into covectors and vice versa, see Section 1.2.6. For
example,
 ⊤
1

3 = 1 3 5 .
| {z }
5
covector
| {z }
vector

With matrices of the corresponding dimensions, the same happens:
 ⊤
1


3 = 1 3 5 .
| {z }
5
1 × 3 matrix
|{z}

3 × 1 matrix

54

| |

|



A=


u⊤
1
u⊤
2
..
.
u⊤
m

|



| |

| |

| |

|

|

This motivates the use of the same transposition symbol ⊤ in both cases.
In column and row notation, we have


v1⊤


|
|
|


v2⊤


⊤


A = v 1 v2 · · · v n
⇔ A =
,
..


.
|
|
|
⊤
vn








⇔


|
|
|
A⊤ = u1 u2 · · · um  .
|
|
|

It is easy to see that mirroring twice gives back the original. Also, the symmetric
matrices are exactly the ones that are mirror images of themselves:
Observation 2.13 (Transposing twice, and transposing symmetric matrices). Let A be an
m × n matrix. Then
(A⊤ )⊤ = A.
Moreover, a square matrix A is symmetric (Definition 2.3) if and only if A = A⊤ .
Now we can define the row space of A simply as the column space of A⊤ . Definitions
of independent row and row rank are done in the same way.
Definition 2.14 (Row space, independent row, row rank). Let A be an m × n matrix. The
row space R(A) of A is the column space of the transpose,
R(A) := C(A⊤ ).
For every i, the i-th row of A is called independent if the i-th column of A⊤ is independent. The
row rank of A is the (column) rank of A⊤ .
Note that by this definition, the row space is a set of vectors, not of covectors. While
C(A) ⊆ Rm , we have R(A) ⊆ Rn .

2.1.4

Rank-1 matrices

The statement that the number of independent columns of a matrix equals the number of
independent rows can via Definition 2.14 be expressed as rank(A) = rank(A⊤ ). Indeed,
rank(A) counts the independent columns of A, while rank(A⊤ ) counts the independent
columns of A⊤ and therefore the independent rows of A.
Here, we will prove rank(A) = rank(A⊤ ) for the case where A has rank 1 (the general
case is handled in Theorem 4.33). The rank-1 result will be an easy consequence of the

55

following lemma that tells us how rank-1 matrices look like. Before that, let us consider
an example of a rank-1-matrix:


1 2 3
A=
.
2 4 6
This matrix has rank 1, since there is only one independent column (the first one), and
the second and third are scalar multiples of it. In this example, there is also only one
independent row (the first one), and the second one is a scalar multiple of it.
Lemma 2.15 (Rank-1 matrices). Let A be an m × n matrix. The following two statements are
equivalent.
(i) rank(A) = 1.
(ii) There are nonzero vectors v ∈ Rm , w ∈ Rn such that
n
A = [vi wj ]m
i=1,j=1 .

In dot notation, the rank-1 matrices are therefore exactly the nonzero matrices of the
form


v1 w1 v1 w2 · · · v1 wn
 v2 w1 v2 w2 · · · v2 wn 


 ..
..
..  .
.
.
 .
.
.
. 
vm w1 vm w2 · · · vm wn
Proof. (i)⇒(ii): If rank(A) = 1, there is exactly one independent column v ̸= 0 (Definition 2.10). This means that all columns before v are 0, and all columns after v are scalar
multiples of v. Thus, all columns are scalar multiples of v where at least one scalar (the
one for column v itself) is nonzero. Let w ̸= 0 be the vector of scalars, meaning that
the j-th column of A is wj v. Hence, aij (the entry in row i and column j of A) equals
wj vi = vi wj . In other words,
n
A = [vi wj ]m
i=1,j=1 .
n
(ii)⇒(i): If A = [vi wj ]m
i=1,j=1 for nonzero vectors v, w, then column j of A is wj v. The
first column for which wj ̸= 0 is independent, since v ̸= 0 and all columns before it are 0;
all columns after it are scalar multiples of this independent column (the scalar for column
k > j is wk /wj ). Hence, there is exactly one independent column, so rank(A) = 1.

Corollary 2.16 (Transpose of a rank-1 matrix). Let A be an m × n matrix with rank(A) = 1.
Then also rank(A⊤ ) = 1.
Proof. By Lemma 2.15 (i)⇒(ii) , there are nonzero vectors v ∈ Rm , w ∈ Rn such that
n
A = [vi wj ]m
.
|{z} i=1,j=1
aij

56

By Definition 2.12 of the transpose,
m
m
A⊤ = [vj wi ]ni=1,j=1
= [wi vj ]ni=1,j=1
,
|{z}
aji

with nonzero w ∈ Rn , v ∈ Rm . Using Lemma 2.15 (ii)⇒(i) for A⊤ gives rank(A⊤ ) = 1.

2.1.5

Nullspace

After column and row space, here is the third fundamental space associated with a matrix.
Definition 2.17 (Nullspace). Let A be an m × n matrix. The nullspace of A is the set
N(A) = {x ∈ Rn : Ax = 0} ⊆ Rn .
Coming back to the interpretation of A as transforming “input vector” x to “output
vector” Ax, we can say that the nullspace contains all input vectors that lead to output
vector 0. We always have 0 ∈ N(A), and sometimes this is the only vector in N(A).
We in fact understand the situation precisely: by Observation 2.5 (ii), the matrices with
a “trivial” nullspace (only containing 0) are exactly the ones with linearly independent
columns. At the other extreme, if A = 0 (the m × n zero matrix), then N(A) = Rn , because
every input leads to output 0.
Let us also look at the nullspace of the rank-1 matrix whose column and row spaces
we have previously examined in Figures 1.23 (left) and Figure 2.3 (right). This is the set

  

 

2 4
x
2 4 x
2
N
=
∈R :
=0 .
3 6
y
3 6 y
By Definition 2.4 of matrix-vector multiplication, the vector equation defining the
nullspace can be written as the two “normal” equations 2x + 4y = 0 and 3x + 6y = 0.
These two equations actually say the same (you get one equation from the other by multiplication with a factor of 3/2 or 2/3). So we can ignore the second one, and simplify the
first one to get y = −x/2. This is the equation of a line through the origin; see Figure 2.5.

y


  
x
2 4
x
2
N
=
∈R :y=−
3 6
y
2

x
y = − x2

Figure 2.5: Nullspace of a 2 × 2 matrix of rank 1: a line

57

2.2

Matrices and linear transformations

When we multiply an m × n matrix A with a vector x in Rn , we get a “transformed”
vector Ax ∈ Rm . Here, we look at the properties of and the theory behind such
matrix transformations. They are for example used to draw 3-dimensional objects
in 2-dimensional space, and they have many other applications. The main insight is
that matrix transformations are the same objects as linear transformations. The latter
are not defined via matrices, but via a condition that we call linearity.

2.2.1

Matrix transformations

An m × n matrix A defines a function that “transforms” an input vector x ∈ Rn into an
output vector Ax ∈ Rm . In the context of functions, we would normally say that x is
mapped to Ax; the term “transformation” comes from a geometric viewpoint where we
think of the output as a transformed (for example rotated) version of the input. We will
explore the geometric viewpoint in some more detail below.
Definition 2.18 (Matrix transformation). Let A be an m × n matrix. The function TA : Rn →
Rm defined by
TA : x 7→ Ax
is the matrix transformation with matrix A.
For example, if


0 1
A=
,
1 0
then

 

   
x1
0 1 x1
x2
TA :
7→
=
,
x2
1 0 x2
x1

the function that swaps the two coordinates of its
input vector. A does not
 2-dimensional

have to be a square matrix. For example, if A = 1 1 (a 1 × 2 matrix), then
 
 

 x1

x1
7→ 1 1
= x1 + x2 ∈ R 1 .
TA :
x2
x2
The following lemma looks a bit technical at first sight but has a natural interpretation.
Lemma 2.19 (Linearity of matrix transformations). Let A be an m × n matrix, x1 , x2 ∈ Rn
and λ1 , λ2 ∈ R. Then
A(λ1 x1 + λ2 x2 ) = λ1 Ax1 + λ2 Ax2 .
This says the following: taking the linear combination λ1 x1 + λ2 x2 and then applying
TA to get the output A(λ1 x1 + λ2 x2 ) gives the same result as applying TA to both vectors individually and then taking the linear combination λ1 Ax1 + λ2 Ax2 of the individual
58

outputs. We can visualize this with a commutative diagram:

x 1 , x2

linear combination
−→

λ1 x1 + λ2 x2


TA y


y TA

Ax1 , Ax2

−→
λ1 Ax1 + λ2 Ax2 = A(λ1 x1 + λ2 x2 )
linear combination

We also say that the diagram commutes (although it does not do anything by itself).
The arrows in a commutative diagram correspond to certain operations, and we can follow any path through the diagram (apply the operations in any order), and the result is
always the same.
Proof of Lemma 2.19. A natural approach is to first prove the following two simpler statements that deal with vector addition and scalar multiplication separately: for all x, x′ ∈
Rn and all λ ∈ R,
(i) A(x + x′ ) = A(x) + A(x′ ), and
(ii) A(λx) = λA(x).
These equalities follow quite directly from the rules of vector addition (Definition 1.2),
scalar multiplication (Definition 1.3) and matrix-vector multiplication (Definition 2.4); we
omit the easy calculations. Combining (i) and (ii), we then get linearity:
(i)

(ii)

A(λ1 x1 + λ2 x2 ) = A(λ1 x1 ) + A(λ2 x2 ) = λ1 A(x1 ) + λ2 A(x2 ).

Vice versa, from linearity we also get (i) and (ii) (think about how, or skip ahead to the
proof of Lemma 2.23), so linearity is in fact equivalent to the combination of (i) and (ii).
Some sources refer to this combination as linearity.
The geometry of matrix transformations. To understand what a matrix transformation
TA “does”, it is useful to not only look at individual inputs, but also at a whole set of
inputs. For a set of input vectors X ⊆ Rn , we define A(X) := {Ax : x ∈ X} ⊆ Rm as the
set of transformed output vectors. Note that A(Rn ) = C(A), the column space of A; see
Definition 2.9.
In the following examples, we see how different A’s transform the standard unit vectors e1 , e2 , and a set X ⊆ R2 (gray L-shaped polygon); see Figure 2.6 (middle).
Figure 2.6 (right) corresponds to our first example above where A swaps the two coordinates of the input vector. The geometric effect is that the input is mirrored along the
diagonal ⧸ of the coordinate system.
59



Ae2

1 0
A=
0 43
A(X)
←−

Ae1





X
e2

0 1
A=
1 0
−→



A(X)
Ae1
Ae2

e1

Figure 2.6: Middle: The input; Right: Mirroring the input along the diagonal. Left:
Stretching the input by a factor of 34 along the second coordinate.
Figure 2.6 (left) gives an example of stretching, which means to make the input longer
or shorter along some or all of the coordinates. If the stretching factors are the same for all
coordinates, we have a scaling, resulting in a larger or smaller copy of the input.
Let us investigate this in some more detail. We first observe that in both examples of
Figure 2.6, the output vector Ae1 is the first column of A, while output vector Ae2 is the
second column of A. This is not a coincidence. Generally, multiplying a matrix with the
j-th standard unit vector (that has a 1 at index j and 0’s everywhere else) gives us the j-th
column of the matrix; this is an immediate consequence of Definition 2.4 (matrix-vector
multiplication in column notation).
The polygon X to which we apply A in Figure 2.6 has 6 corners and 6 line segments
connecting the corners. This is an infinite set, but to compute A(X), we only need to apply
A to the corners; each line segment will “follow” its two corners. This is a consequence of
linearity: consider a line segment s connecting two corners x1 and x2 . In Section 1.1.4, we
have seen that s is the set of convex combinations of x1 and x2 ,
s = {λ1 x1 + λ2 x2 : λ1 + λ2 = 1, λ1 ≥ 0, λ2 ≥ 0}.
Hence,
A(s) = {A(λ1 x1 + λ2 x2 ) : λ1 + λ2 = 1, λ1 ≥ 0, λ2 ≥ 0}
= {λ1 Ax1 + λ2 Ax2 : λ1 + λ2 = 1, λ1 ≥ 0, λ2 ≥ 0} (linearity, Lemma 2.19).
This means that the transformed line segment A(s) is simply the line segment connecting
the transformed corners Ax1 and Ax2 :

s

x2

TA
−→

Ax1

A(s)
Ax2

x1

If A = I, the identity matrix, then the matrix transformation TA simply outputs the
input, without transforming it (due to Ix = x for all x; see Corollary 2.7). Figure 2.7
shows two more examples of matrix transformations, a shear, and a rotation.
60

A(X)
Ae2

Ae1

"
A=

√1
2
√1
2

←−

− √12

#


1 − 12
A=
0
1
−→
Ae2


√1
2

X
e2
e1

A(X)

Ae1

Figure 2.7: Middle: The input; Right: Shearing the input parallel to the first coordinate.
Left: Rotating the input by 45 degrees.
If A is a 2 × 3 matrix, then TA is a projection from R3 to R2 . Such projections can be
used to draw 3-dimensional objects in 2-dimensional space, for example the cube in the
margin.
You can recognize the cube, although what you really see is just a 2-dimensional
image, a parallel projection. Figure 2.8 shows how such a parallel projection is obtained
through a matrix transformation TA .


2
1
0
Applying TA : R3 → R2 with A =
:
0 1 2
 
 




1
0
1
3
 1 
 1 
y
3
3
1
1
 
 
1
0





 0 
0 
0
2
1
1
2
2
−→




 
 
1
3
0
1
1
1
 1 
 1 
0
0
 
x
 




1
0
0
2
 0 
 0 
0
0
0
0
Figure 2.8: Parallel projection of a 3-dimensional cube. The left figure shows the 8 corners
of the 3-dimensional unit cube as vectors in R3 . Two corners are connected in the cube
if they differ in exactly one coordinate. The right figure is a 2-dimensional drawing, resulting from applying TA to the cube corners (the connections follow the corners). With
different matrices A, we get different drawings; see Figure 2.9 for another drawing.
If you take a photograph of a real 3-dimensional cube, you will also get a 2-dimensional
image of it; this one will not be a parallel projection, though, but a perspective projection.
61


2 −1 −1
Applying TA : R → R with A =
:
0
2 −1




−1
1
2
2
3



−2
1










2

−1
−1

0
0

0
1










1
−1

2
0





Figure 2.9: Projecting the cube in Figure 2.8 (left) using a different matrix
Figure 2.10 shows such an image. There is no matrix transformation that can realize this
projection. The reason is that a matrix transformation TA has the following property:
if two line segments s, s′ (for example two vertical edges of the original 3-dimensional
cube) are parallel, then the transformed line segments A(s) and A(s′ ) are also parallel.
Exercise 2.20 asks you to prove this. But in Figure 2.10, parallel cube edges get transformed to edges that are not parallel. For example, look at the three “vertical” edges in
Figure 2.10. Your brain may tell you that they are parallel, because it “sees” the original
cube, but in the actual image, they are definitely not parallel.

Figure 2.10: Perspective projection of a cube as obtained through a photograph
Exercise 2.20. Let s ⊆ Rn be a line segment with endpoints x1 , x2 ∈ Rn and s′ ⊆ Rn a line
segment with endpoints x′1 , x′2 ∈ Rn . Let A be an m × n matrix.
62

On page 60, we have argued (and you can use this here) that the transformed line segment
A(s) ⊆ Rm has endpoints Ax1 , Ax2 ∈ Rm , and the transformed line segment A(s′ ) ⊆ Rm has
endpoints Ax′1 , Ax′2 ∈ Rm .
We call two line segments s (with endpoints x1 , x2 ) and s′ (with endpoints x′1 , x′2 ) parallel if
both x1 −x2 and x′1 −x′2 are scalar multiples of some vector v, i.e. x1 −x2 = λv and x′1 −x′2 = λ′ v
for some real numbers λ, λ′ .
Prove the following statement: if the line segments s, s′ are parallel, then the transformed line
segments A(s) and A(s′ ) are also parallel.

2.2.2

Linear transformations and linear functionals

Recall that a matrix transformation satisfies linearity (transformation commutes with linear combination, see Lemma 2.19). When we look at all the functions T : Rn → Rm that
satisfy linearity, we arrive at the class of linear transformations. A function T : Rn → R
satisfying linearity is called a linear functional.
Definition 2.21 (Linear transformation, linear functional). A function T : Rn → Rm /
T : Rn → R is called a linear transformation / linear functional if the following linearity
axiom holds for all x1 , x2 ∈ Rn and all λ1 , λ2 ∈ R.
T (λ1 x1 + λ2 x2 ) = λ1 T (x1 ) + λ2 T (x2 ).
An axiom is a defining property of a class of mathematical objects. Exactly the objects
satisfying the property belong to the class.
Since TA (x) = Ax for a matrix transformation, the following is an immediate consequence of Lemma 2.19.
Observation 2.22. Every matrix transformation is a linear transformation.
The following provides an alternative definition of linearity in terms of two linearity
axioms. For matrix transformations, we have already touched upon this in the proof of
Lemma 2.19. This alternative definition is in some sources the main definition.
Lemma 2.23. A function T : Rn → Rm / T : Rn → R is a linear transformation / linear
functional if and only if the following two linearity axioms hold for all x, x′ ∈ Rn and all λ ∈ R.
(i) T (x + x′ ) = T (x) + T (x′ ), and
(ii) T (λx) = λT (x).
It is quite common that a class of objects is being defined via axioms in different ways,
where each way serves its own purpose. For example, if we want to prove that some function is a linear transformation or a linear functional, the two simpler linearity axioms (i)
and (ii) in Lemma 2.23 are usually easier to check than the more “complicated” single linearity axiom in Definition 2.21. On the other hand, if we want to apply linearity to prove
something else, the single axiom that combines (i) and (ii) may lead to shorter proofs.
63

Proof of Lemma 2.23. If we have linearity according to Definition 2.21, we can apply it to
suitable vectors and scalars in order to derive the two simpler linearity axioms (i) and (ii):
With x1 = x, x2 = x′ , λ1 = λ2 = 1, we get (i). Using x1 = x, x2 = 0, λ1 = λ, λ2 = 0, we obtain (ii). Vice versa, if (i) and (ii) hold, we can derive linearity according to Definition 2.21:
(i)

(ii)

T (λ1 x1 + λ2 x2 ) = T (λ1 x1 ) + T (λ2 x2 ) = λ1 T (x1 ) + λ2 T (x2 ).

Let us look at a few examples. Let v ∈ Rm be a vector. The function T : Rm → R,
T : x 7→

m
X

v i xi

i=1

is a linear functional (also known as the covector v⊤ , see Definition 1.20). It is often a bit
clearer to verify the two simple linearity axioms (i) and (ii) in Lemma 2.23. Here, this is
done as follows:
′

T (x + x ) =

m
X

vi (xi + x′i ) =

i=1

T (λx) =

m
X
i=1

m
X

v i xi +

i=1

vi λxi = λ

m
X

m
X

vi x′i = T (x) + T (x′ ), and

i=1

vi xi = λT (x).

i=1

In these derivations, we use well-known properties (commutativity, distributivity) of
addition and multiplication of real numbers, as well as Definition 1.2 of vector addition
and Definition 1.3 of scalar multiplication (in the first equality of each line).
The function T : R3 → R2 ,


x1 + 2x2
T :x→
3x2 + 4x3
is a linear transformation. We could again check this directly, but a more elegant proof is
to observe that T is actually a matrix transformation and therefore a linear transformation
by Theorem 2.26. Indeed, it is easy to see that T = TA for


1 2 0
A=
.
0 3 4
Another (somewhat trivial but still instructive) example of a matrix transformation
is T : Rn → Rm , x 7→ 0. Here, 0 is the m-dimensional zero vector; linearity obviously
holds, and we can also express T as the matrix transformation TA with A = 0 ∈ Rm×n .
As our last example, we consider the identity T : Rm → Rm , x 7→ x. This is also a linear
transformation and equal to the matrix transformation TI where I is the m × m identity
matrix; see Corollary 2.7.
64

Now for some counterexamples. Consider T : Rm → R,
T : x 7→

m
X

|xi | = ∥x∥1 .

i=1

This outputs the 1-norm of x (see Section 1.2.2) but is not a linear functional. To show this,
we need to find a violation of linearity. For example, if x ̸= 0 and λ < 0, then T (λx) > 0
but λT (x) < 0; so linearity (ii), T (λx) = λT (x) in Lemma 2.23, does not always hold.
Slightly changing the third example above, we can produce another counterexample.
Consider T : x 7→ v, where v ∈ Rm is some fixed nonzero vector. This is not a linear
transformation, because linearity (i), T (x + x′ ) = T (x) + T (x′ ) in Lemma 2.23, always
fails: we have T (x + x′ ) = v and T (x) + T (x′ ) = 2v ̸= v.
In this counterexample, we have T (0) ̸= 0, and this is already enough to conclude that
T is not a linear transformation. Indeed, we have
Lemma 2.24. Let T : Rn → Rm / T : Rn → R be a linear transformation / linear functional.
Then T (0) = 0 / T (0) = 0.
Proof. We use linearity (ii) in Lemma 2.23 with any x: T (0) = T (0x) = 0T (x) = 0 / 0.
Linearity generalizes to linear combinations of more (or less) than two vectors, and
this turns out to be the key for fully understanding linear transformations.
Lemma 2.25. Let T : Rn → Rm / T : Rn → R be a linear transformation / linear functional, let
x1 , x2 , . . . , xℓ ∈ Rn and λ1 , λ2 , . . . , λℓ ∈ R. Then
!
ℓ
ℓ
X
X
T
λj xj =
λj T (xj ).
j=1

j=1

Proof (with dots). We use linearity (i) and (ii) from Lemma 2.23 to obtain
!
!
!
!
ℓ
ℓ−1
ℓ−1
ℓ−1
X
X
X
X
(i)
(ii)
T
λj xj = T
λj xj + λℓ x ℓ = T
λj xj +T (λℓ xℓ ) = T
λj xj +λℓ T (xℓ ) .
j=1

j=1

j=1

j=1

Doing the same for ℓ − 1, we further get
!
!
ℓ
ℓ−2
X
X
T
λj xj = T
λj xj + λℓ−1 T (xℓ−1 ) +λℓ T (xℓ ) .
j=1

j=1

|

{z

Pℓ−1
T ( j=1
λj x j )

}

Repeating this for ℓ − 2, . . . , 1, we finally obtain
!
!
ℓ
0
ℓ
X
X
X
T
λj xj = T
λj xj +λ1 T (x1 ) + . . . + λℓ−1 T (xℓ−1 ) + λℓ T (xℓ ) =
λj T (xj ),
j=1

j=1

|

{z

T (0)

j=1

}

because T (0) = 0 by Lemma 2.24.
65

This proof has the usual dots in λ1 T (x1 )+· · ·+λℓ−1 T (xℓ−1 )+λℓ T (xℓ ), but also dots of a
different quality, namely the ones in ℓ − 2, . . . , 1. These dots indicate a repeating pattern in
the proof itself. In Section 1.1.5, we have seen dot-free notations for sequences and sums
and argued that they are more precise than the ones with the dots; is there also a dot-free
notation for proofs such as the one above? Yes, and this notation is known as proof by
induction, an important proof technique in mathematics. The concept of induction is the
following: We want to prove that some statement holds for all natural numbers n. For
example, that
n
X
n(n + 1)
j=
.
2
j=1
This is a famous formula often attributed to the German mathematician Carl Friedrich
Gauss; legend has it that he discovered the formula in school, trying to avoid a very boring
task assigned to him and his classmates by the teacher: sum up all numbers from 1 to 100!
Indeed, using the formula, one immediately sees that the sum is 5, 050.
Back to induction: We first check the base case, meaning that the statement holds for
the first natural number n = 0. Indeed, for n = 0, both sides of Gauss’ formula give 0.
For n > 0, we perform the induction step. This proves an implication: if the statement
is true for the number n − 1 (this is the induction hypothesis), then it is also true for n (this
concludes the induction step). In our example, if Gauss’ formula is true for n − 1, we can
compute
!
n
n−1
X
X
n(n + 1)
(n − 1)n
+n =
.
j=
j +n =
2
2
j=1
j=1
↑
induction hypothesis

↑
easy calculation

So if Gauss’ formula is true for n − 1, it is indeed also true for n.
Once we have completed base case and induction step, we have proved the statement
for all natural numbers. Why is that? We have proved it for the first natural number (base
case), and the induction step lets us conclude that it is also true for the second natural
number (we know that if it is true for the first, then it is also true for the second). So we
have proved it for the second natural number, and the induction step lets us conclude that
it is also true for the third natural number. And so on (these are the dots, but now they
do not appear in the proof, but in its justification). Every natural number is eventually
reached by this sequence of steps, so we have proved it for all natural numbers. Even
though there are infinitely many natural numbers, every natural number itself is finite, so
we eventually get to it.
This is often illustrated with the domino effect. Suppose you have an infinite sequence
of dominoes, numbered 0, 1, 2 . . ., and arranged in line. For every n, you know that if
domino n − 1 falls, then it knocks over domino n (induction step). Hence, if you knock
over domino 0 (base case), then every domino will eventually fall.
Let us exercise induction for the statement of Lemma 2.25. Here, the natural number
symbol is not n but ℓ.
66

Proof of Lemma 2.25 (by induction). Base case: For ℓ = 0, the statement reads as T (0) = 0
which is true by Lemma 2.24. For ℓ > 0, we perform the induction step: if the statement
is true for ℓ − 1, we compute

T

ℓ
X

!
λj xj

= T

j=1

ℓ−1
X

!
λj xj + λℓ xℓ

(i)

=T

j=1
(ii)

= T

ℓ−1
X

ℓ−1
X

!
λj xj

+ T (λℓ xℓ )

j=1

!
λj xj

+ λℓ T (xℓ )

j=1

=

ℓ−1
X

λj T (xj ) + λℓ T (xℓ )

(induction hypothesis)

j=1

=

ℓ
X

λj T (xj ).

j=1

So if the statement is true for ℓ − 1, it is indeed true for ℓ.
The proof is conceptually the same as the previous one, but replaces “repeating this
for ℓ − 2, . . . , 1” by a single step. Whenever you see a proof using such repetitions, or
saying “and so on. . . ”, you can be pretty sure that this an informal proof by induction.
There is nothing wrong with a proof using “and so on”, as long as you can turn it into a
formal proof by induction, if needed.
Looking back at the proofs of Lemma 1.28 and Lemma 2.11, these were also inductions
in disguise, where we have used “one by one” to indicate a repeating step in a proof. In
these cases, we would actually apply limited induction, meaning that we do not prove a
result for all natural numbers, but only the ones that are at most as large as some given
number. But the principle is the same.

2.2.3

The matrix of a linear transformation

We already know that every matrix transformation is a linear transformation (Observation 2.22). Now we will prove that every linear transformation (Definition 2.21) is also a
matrix transformation (Definition 2.18). This means that there are no “fancy” linear transformations beyond the ones that we already know, namely the matrix transformations.
Theorem 2.26. Let T : Rn → Rm be a linear transformation. There is a unique m × n matrix A
such that T = TA (meaning that T (x) = TA (x) for all x ∈ Rn ). This matrix is


|
|
|
A = T (e1 ) T (e2 ) · · · T (en ) .
|
|
|

67

Proof. In order for T = TA to hold, we must in particular have T (ej ) = TA (ej ) = Aej
(which is the j-th column of A) for all j ∈ [n]. Hence, the only candidate for A is the
one mentioned in the statement of the theorem, the matrix whose columns are the mdimensional output vectors that we get when we apply T to the n-dimensional standard
unit vectors as inputs. With this matrix A, we indeed get T = TA , because for all x ∈ Rn ,
!
n
n
X
X
TA (x) = Ax =
xj T (ej ) = T
xj ej = T (x).
j=1

j=1

In the second equality, we use Definition 2.4 of matrix-vector multiplication with our
matrix A as defined above. In the third equality, we apply Lemma 2.25, and the last
equality uses that every vector x is a linear combination of the standard unit vectors,
where the scalars are simply the entries of x, as in
 
 
 
 
0
0
x1
1
x2  = x1 0 +x2 1 +x3 0 .
0
1
x3
0
| {z }
| {z }
| {z }
e1

e2

e3

A consequence of this lemma is the following: every linear transformation is completely determined by its behavior on the standard unit vectors. This also explains why
we have paid special attention to this behavior in Figures 2.6 and 2.7.

2.2.4

Kernel and Image

For every linear transformation, there are two important sets of vectors.
Definition 2.27 (Kernel and image). Let T : Rn → Rm be a linear transformation. The set
Ker(T ) := {x ∈ Rn : T (x) = 0} ⊆ Rn
is the kernel of T . The set
Im(T ) := {T (x) : x ∈ Rn } ⊆ Rm
is the image of T .
The image of T is the set of all outputs that T can produce. This is actually a familiar
concept.
Observation 2.28. Let T : Rn → Rm be a linear transformation and A the unique m × n matrix
(that exists by Theorem 2.26) such that T = TA . Then
Im(T ) = C(A),
the column space of A.
68

This immediately follows from Definition 2.9 of C(A) and T (x) = Ax. In light of this,
some sources call the column space of A the image of A (although it is officially the image
of TA ).
For the kernel, we have a similar statement.
Observation 2.29. Let T : Rn → Rm be a linear transformation and A the unique m × n matrix
(that exists by Theorem 2.26) such that T = TA . Then
Ker(T ) = N(A),
the nullspace of A; see Definition 2.17.
Again, some sources call the nullspace of A the kernel of A (although it is officially the
kernel of TA ).
Table 2.1 provides kernel and image of three linear transformations that we have considered as examples on page 64.
Im(T )
T
Ker(T
Pn
Pn )
n
R → R , x 7→ i=1 xi {x ∈ R : i=1 xi = 0}
R1
n
m
n
R → R , x 7→ 0
R
{0}
m
m
R → R , x 7→ x
{0}
Rm
n

1

Table 2.1: Kernel and image of some linear transformations
Exercise 2.30. Let A be an m × m (square) matrix, λ ∈ R a scalar. Prove that the function
T : Rm → Rm , x → Ax + λx is a linear transformation!
Exercise 2.31. Prove that the function T : R2 → R, x 7→ x1 x2 is not a linear functional!
Exercise 2.32. Let v ∈ Rm , w ∈ Rn be nonzero vectors and consider the m × n matrix A =
n
[vi wj ]m
i=1,j=1 (this matrix has rank 1 by Lemma 2.15). Give formulas for Ker(TA ) and Im(TA ),
depending on the vectors v and w!

2.3

Matrix multiplication

If we combine two matrix transformations TA and TB into one transformation (“first
do TB , then TA ”), it turns out that we get another matrix transformation. Its matrix
can be obtained by multiplying A and B in a suitable way. Here, we formally introduce this kind of matrix multiplication and derive its essential properties. Matrix
multiplication naturally comes up in matrix decompositions where we write a matrix
as a product of other matrices with the goal of revealing some structural properties.
We present the CR decomposition in this section; other decompositions will appear
in subsequent chapters.
69

2.3.1

Combining matrix transformations

We want to show that combining two matrix transformations gives another matrix transformation. Before we do this, let us look at two examples. In Figure 2.6, we have seen
two matrix transformations, mirroring and stretching. Figure 2.11 shows what happens if
we combine them by first stretching the input to obtain an intermediate result, and then
mirroring the intermediate result to get the output. We can also check that there is a single
matrix transformation TC that gets us from the input to the output in one step. We can
read off the matrix C from what happens to the unit vectors e1 (red vector in the input)
and e2 (blue vector in the input) when we apply TC : We know from Theorem 2.26 that the
first column of C is TC (e1 ) (the red vector in the output), and the second column of C is
TC (e2 ) (the blue vector in the output).




1 0
0 1
B=
A=
0 34
1 0
−→
−→
TB
TA

 3
0 4
C=
1 0
TC
Figure 2.11: Left: The input. Middle: Stretching the input by a factor of 34 along the second
coordinate. Right: The output, obtained from mirroring the intermediate result along the
diagonal. Bottom: Combining the two transformations into one.
We can also combine the two matrix transformations in reverse order: first mirror the
input, then stretch the intermediate result:




0 1
1 0
A=
B=
1 0
0 43
−→
−→
TA
TB




0 1
C= 3
4 0
TC
The output is different from the one in Figure 2.11, so the combination order matters;
but also here, we get a single matrix transformation TC that combines the two steps into
one. Again, we can read off C from how the two standard unit vectors get transformed.
70

Next we formalize this kind of combination that generally works in the context of
functions.
Definition 2.33 (Composition of functions). Let g : X → Y and f : Y → Z be two functions
where X, Y, Z are arbitrary sets. The function h : X → Z,
h : x 7→ f (g(x))
is the composition of f and g, written as f ◦ g (“first apply g, then f ”).
This works because the set Y is both the codomain of g and the domain of f . In other
words, every possible output of g is a valid input for f . Here is what happens when we
compose two matrix transformations.
Lemma 2.34 (Composition of matrix transformations). Let TB : Rb → Rn and TA : Rn →
Ra be two matrix transformations. The composition TA ◦ TB : Rb → Ra (“first do TB , then TA ”)
is another matrix transformation.
Proof. We only need to show that TA ◦ TB is a linear transformation according to Definition 2.21. Then it follows from Theorem 2.26 that TA ◦ TB is also a matrix transformation.
To verify the linearity axiom in Definition 2.21, we use Definition 2.33 of composition and
linearity of both TB and TA :
(TA ◦ TB )(λ1 x1 + λ2 x2 ) =
=
=
=

2.3.2

TA (TB (λ1 x1 + λ2 x2 )) (composition)
TA (λ1 TB (x1 ) + λ2 TB (x2 )) (linearity of TB )
λ1 TA (TB (x1 )) + λ2 TA (TB (x2 )) (linearity of TA )
λ1 (TA ◦ TB )(x1 ) + λ2 (TA ◦ TB )(x2 ). (composition)

Definition and basic properties

Suppose we are given an a × n matrix A and an n × b matrix B. From Lemma 2.34, we
know that TA ◦ TB = TC for some a × b matrix C. We can think of TC as “first do TB , then
TA ”. Equality between two functions (TA ◦ TB and TC in our case) means that for every
possible input, both functions produce the same output. Here is how we get the matrix C
from this.
Lemma 2.35 (Matrix of the composition). Let A be an a × n matrix and


|
|
|
B = x1 x2 · · · xb 
|
|
|

71

an n × b matrix. The a × b matrix



|
|
|
C = Ax1 Ax2 · · · Axb 
|
|
|
is the unique matrix that satisfies TC = TA ◦ TB .
Proof. We already know from Lemma 2.34 that there is a matrix C with TC = TA ◦ TB .
Both TC and TA ◦ TB must produce the same output on every standard unit vector:
TC (ej ) = (TA ◦ TB )(ej )

Def. 2.33

=

TA (TB (ej ))

Def. 2.18

=

ATB (ej ),

j = 1, 2, . . . , b.

(2.1)

We know that TC (ej ) = Cej is the j-th column of C, and TB (ej ) = Bej is the j-th
column of B that we called xj . Hence, (2.1) says that the j-th column of C is Axj for all j,
and this gives


|
|
|
C = Ax1 Ax2 · · · Axb  .
|
|
|

We will now call this matrix C the product of A and B, and write it as AB.
Definition 2.36 (Matrix multiplication in column notation). Let A be an a × n matrix and


|
|
|
B = x1 x2 · · · xb 
|
|
|
an n × b matrix. The a × b matrix

|
|
|
AB := Ax1 Ax2 · · · Axb 
|
|
|


is the product of A and B.
Note that for the product AB to be defined, the number of columns of A needs to
match the number of rows of B; this is the number n in Definition 2.36. This naturally
connects to the fact that the composition TA ◦ TB is only defined if the output of TB can be
used as input for TA ; in other words, we must have TB : · · · → Rn and TA : Rn → · · · for
some n. We can nicely summarize Lemma 2.35 and Definition 2.36 as
Corollary 2.37 (Matrix of the composition). Let A be an a×n matrix and B be an n×b matrix.
Then TA ◦ TB = TAB .
72

One can define the product AB directly, without referring to matrix-vector multiplication: in order to compute the entry of AB in row i and column j, we take the scalar
product of row i of A and column j of B.

| |

u⊤
1
u⊤
2
..
.

| |

|

u⊤
a

|

Observation 2.38. Let



A=

Then






 ∈ Ra×n ,



|
|
|
B = x1 x2 · · · xb  ∈ Rn×b .
|
|
|


⊤
⊤
u⊤
x
u
x
·
·
·
u
x
1 1
1 2
1 b
u⊤ x1 u⊤ x2 · · · u⊤ xb  
a b
2
2
2


⊤
x
=
u
∈ Ra×b .
AB =  ..

..
.
j
.
i
i=1,j=1
.
.

 .
.
.
.
⊤
⊤
x
·
·
·
u
x
u
u⊤
a xb
a 2
a 1
|
{z
}


ab scalar products

a
This works because Axj , the j-th column of AB, can be written as Axj = [u⊤
i xj ]i=1 ; see
Observation 2.8 (matrix-vector multiplication with A in row notation).
The previous definition of AB corresponds to how matrix multiplication is usually
done, mentally; see Figure 2.12.



 

2 3 −3 1 3
0 −1 6
=
3 −1
2 −1 0
−11 4 9



2 3
3 −1

2 · 1 + 3 · (−1) = −1


 

−3 1 3
0 −1 6
=
2 −1 0
−11 4 9

3 · 3 + (−1) · 0 = 9

Figure 2.12: Matrix multiplication: multiply (scalar product) each row of the first matrix
with each column of the second matrix! In this example, this needs six scalar products,
two of which are shown.
Finally, here is the dot-free definition of AB.
n
b
Observation 2.39. Let A = [aij ]ai=1,j=1
, B = [bij ]ni=1,j=1
. Then
" n
#a b
X
AB =
aiℓ bℓj
.
ℓ=1

i=1,j=1

This is just a different way of writing Observation 2.38, since
(“row i of A times column j of B”).
As two more examples, let us come back to the matrices




0 1
1 0
A=
,B =
.
1 0
0 34
73

Pn

⊤
ℓ=1 aiℓ bℓj equals ui xj

of Figure 2.11. We have
 
 1
 0 1 0
 
AB = 

 1
1 0
0

 
 0
0 1 3   3
0 4
 4 
 0 = 1 0
1 0 3

 
 0
 1 0 1
 
BA = 

 0
3
0 4
1

 
 1


1 0

0
0
1
 
 1 = 3 0 .
3
4
0 4
0



and



4

We see that matrix multiplication is not commutative, since we have BA ̸= AB here.
On page 70 , we already saw that it makes a difference whether we apply TA or TB first.
Also in life, the order of combining two activities typically matters. You first put your
socks on, and then the shoes. The reverse order would have a quite different effect. On the
other hand, there are activities that do commute, for example, it does not matter whether
you put your left or right shoe on first. Similarly, we do have AB = BA in some cases.
If A and B are not square matrices, it may happen that AB is defined and BA is not.
Or that both products are defined, but are of different shape.
It is not hard to understand the situation exactly. For AB to be defined, A must be
a × n and B must be n × b, for some number n. For BA to be defined as well, we also need
a = b. This means, A must be m × n and B must be n × m, for some number m. Then
AB is m × m and BA is n × n, so both products are square matrices. If m = n, they have
the same shape, otherwise, one is larger than the other. Here is a pictorial view of matrix
multiplication:

A
a

B

AB
=a

n

b

n
b

An easy but important fact is how matrix multiplication interacts with transposition
(Definition 2.12).
Lemma 2.40. Let A be an a × n matrix and B an n × b matrix. Then
(AB)⊤ = B ⊤ A⊤ .
Proof. Since B ⊤ is a b × n matrix and A⊤ an n × a matrix, the product B ⊤ A⊤ is a b × a
matrix and therefore has the same shape as (AB)⊤ .
Next, we compare the two matrices
a
a
C = (AB)⊤ = [cij ]bi=1,j=1
and D = B ⊤ A⊤ = [dij ]bi=1,j=1

74

entry by entry. By Definition 2.12 of the transpose, cij is the entry in row j and column i
of AB and therefore the scalar product of the j-th row of A and the i-th column of B; see
Observation 2.38:
cij = (j-th row of A) · (i-th column of B).
By the same observation,
dij = (i-th row of B ⊤ ) · (j-th column of A⊤ ).
To conclude cij = dij , it remains to note that
(j-th row of A) = (j-th column of A⊤ ) and (i-th column of B) = (i-th row of B ⊤ ).

A proof by induction can be used to show that this generalizes to more matrices, for
example
(ABC)⊤ = C ⊤ B ⊤ A⊤ .
As seen before in Corollary 2.7 for matrix-vector multiplication, identity matrices also
have no effect in matrix multiplications. We leave the proof of this as an (easy) exercise.
Corollary 2.41. Let I be the m × m identity matrix. Then IA = A for all m × n matrices, and
AI = A for all n × m matrices.

2.3.3

Distributivity and associativity

While matrix multiplication is not commutative (we may have AB ̸= BA), two other
important laws hold.
Lemma 2.42. Let A, B, C be three matrices. Whenever the respective sums and products in the
following are defined, we have
(i) A(B + C) = AB + AC and (A + B)C = AC + BC;

(distributivity)

(ii) (AB)C = A(BC).

(associativity)

Proof. We omit the mechanical proof of distributivity, based on Definition 2.2 (i) of matrix
addition, Definition 2.36 of matrix multiplication, and distributivity of the real numbers:
a(b + c) = ab + ac and (a + b)c = ac + bc.
For associativity, the proof is more interesting. We use the fact that matrix multiplication according to Definition 2.36 has been designed in such a way that AB is the matrix of
the linear transformation TA ◦ TB , the composition of TA and TB ; see Corollary 2.37. Using
this once more, we see that (AB)C is the matrix of (TA ◦ TB ) ◦ TC . Similarly, A(BC) is the
matrix of TA ◦ (TB ◦ TC ).
It remains to observe that (TA ◦ TB ) ◦ TC and TA ◦ (TB ◦ TC ) are the same linear transformations and therefore also have the same matrices (AB)C = A(BC) by Theorem 2.26.
75

Indeed, applying Definition 2.33 of composition twice for each transformation, we get
that for every input x, both functions produce the same output:
((TA ◦ TB ) ◦ TC )(x) = (TA ◦ TB )(TC (x)) = TA (TB (TC (x))),
(TA ◦ (TB ◦ TC ))(x) = TA ((TB ◦ TC )(x)) = TA (TB (TC (x))).

Knowing that matrix multiplication is associative allows us to write ABC for a product of three matrices. As it does not matter whether we compute this as (AB)C or A(BC),
we can as well omit the brackets.
This also works for more matrices and is then called generalized associativity. For example, (AB)(CD) = A((BC)D) = · · · = ABCD. This can be proved along the same
lines. (AB)(CD) is the matrix of (TA ◦ TB ) ◦ (TC ◦ TD ), while A((BC)D) is the matrix of
TA ◦ ((TB ◦ TC ) ◦ TD ). Again, both linear transformations have the same matrix, since
they are the same transformations: on every input x, both produce the same output
TA (TB (TC (TD (x)))) as we can see by repeatedly applying Definition 2.33 of composition.

2.3.4

Everything “is” matrix multiplication

Let us look at a matrix multiplication AB in the special case where the matrix B has just
one column. For example,
 


1
0 1  
2 3 1 = 5 .
1
9
4 5 |{z}
|{z}
| {z } B,2×1
A,3×2

AB,3×1

Except for the shapes of the brackets, this looks exactly like the matrix-vector multiplication


 
0 1  
1
2 3 1 = 5 .
1
4 5 |{z}
9
| {z } x∈R2 | {z }
Ax∈R3

A,3×2

This easily follows from Definition 2.36 of matrix multiplication: when B has just one
column x, then AB also has just one column, the matrix-vector product Ax.
In fact, for every matrix multiplication where at least one of the matrices is “flat” (has
only one row or one column), we have a corresponding version involving vectors, covectors, or scalars, see Table 2.2 on the next page. covector-matrix multiplication as well as the
outer product are new, and we will formally define them next. We will also explore why
this “zoo” of rather diverse multiplications contains exactly the “animals” in the table,
and why they all work like matrix multiplications. For just using them in the following,
only the latter point (they work like matrix multiplications) is important and Table 2.2 is
all that you need.
76

matrix multiplication
Example
 


1
0 1  
2 3 1 = 5
1
9
4 5 |{z}
|{z}
| {z } 2×1
3×2

3×1

covector / vector / scalar version
Example
operation

 

matrix-vector
0 1  
1
1
multiplication
2 3
= 5
Ax
|{z}
1
A ∈ Rm×n , x ∈ Rn
4 5 |{z}
9
∈Rm
(Definition 2.4)
| {z } ∈R2
| {z }
3×2

∈R3

 
 
 3
1 2
= 11
|{z}
| {z } 4
1×2 |{z}
1×1

 
 3
1 2
= |{z}
11
| {z } 4
∈R
∈(R2 )∗ |{z}



0
1




1 1 1 2 3 = 6 9
| {z } 4 5
| {z }
1×3
1×2
| {z }
3×2 
 

3 
3 6

4 1 2 = 4 8
5 | {z }
5 10
|{z} 1×2
| {z }



0
1


1 1 1 2 3 = 6 9
| {z } 4 5
| {z }
∈(R3 )∗
∈(R2 )∗
| {z }
3×2 
 

3
3 6

4 1 2 = 4 8
5 | {z2 ∗}
5 10
| {z } ∈(R )
| {z }

 
 
6
3  
2 =
8
4 |{z}
|{z}
|{z} 1×1

 
 
6
3
2 =
8
4 |{z}
|{z}
|{z} ∈R


  
 
2 3 4 = 6 8
|{z} | {z } | {z }



2 3 4 = 6 8
|{z}
| {z } | {z }

λv⊤

   
 
2 3 = 6
|{z} |{z} |{z}

2 |{z}
3 = |{z}
6
|{z}

λµ



2×1

3×1

3×2

2×1

2×1

1×2

1×1

1×1

1×1

1×2

1×1

⊤
v
| {zw}
∈R

∈R2

y⊤ A
|{z}

∈(Rn )∗

⊤

vw
| {z }
m×n

scalar product
v, w ∈ Rm
(Definition 1.9,
Definition 1.19)
covector-matrix
multiplication
y⊤ ∈ (Rm )∗ , A ∈ Rm×n
(Definition 2.43)
outer product
v ∈ Rm , w ∈ Rn
(Definition 2.44)

3×2

∈R3

∈R2

∈R

∈Rm

∈R2

∈(R2 )∗

∈R

vλ
|{z}

∈(R2 )∗

∈R

∈R

scalar multiplication
(the other way around)
v ∈ Rm , λ ∈ R
(Definition 1.3).
scalar multiplication
(covector version)
v⊤ ∈ Rm , λ ∈ R
standard multiplication
λ, µ ∈ R

Table 2.2: “Mixed” multiplications that work like matrix multiplications
Definition 2.43 (Covector-matrix multiplication). Let


|
|
|

y⊤ = y1 y2 · · · ym ∈ (Rm )∗ , A = v1 v2 · · · vn  ∈ Rm×n .
|
|
|
The covector

y⊤ A = y⊤ v1 y⊤ v2 · · · y⊤ vn ∈ (Rn )∗
{z
}
|
n scalar products

77

is the product of y⊤ and A.

|

|

By the definition of matrix multiplication via scalar products (Observation 2.38), this
is indeed the covector version of the matrix multiplication




y⊤
A = y ⊤ v 1 y ⊤ v 2 · · · y ⊤ vn .
|
|
{z
}
{z
}
1×m

1×n

Now for the outer product.
Definition 2.44. Let v ∈ Rm , w ∈ Rn . The outer product vw⊤ of v and w is the m × n matrix


v1 w1 v1 w2 · · · v1 wn
 v2 w1 v2 w2 · · · v2 wn 


m n
⊤
vw :=  ..
..
..  = [vi wj ]i=1,j=1 .
.
.
 .
.
.
. 
vm w1 vm w2 · · · vm wn
The outer product is sometimes also written as v ⊗ w but we will not use this notation.

|

|

This is the vector / covector version of the matrix multiplication


v1 w1 v1 w2 · · · v1 wn
 
| 

 
 v2 w1 v2 w2 · · · v2 wn 
⊤
v
w
=  ..
..
..  ,
..
 .
.
.
. 
|
vm w1 vm w2 · · · vm wm
as we can see from the definition of matrix multiplication in terms of scalar products
(Observation 2.38). Here, vi wj is the scalar product of the i-th row of v (which has just
one entry vi ) and the j-th column of w⊤ (which has also just one entry wj ).
The outer product provides a different way of thinking about rank-1 matrices: according to Lemma 2.15, a rank-1 matrix can now equivalently be characterized as the outer
product of two vectors.
Using the arsenal of “mixed” multiplications from Table 2.2, we can form “mixed”
expressions, involving matrices, vectors, covectors, and scalars (assuming the dimensions
are such that all products are defined). A frequent example of such a mixed expression is
x⊤ A⊤ Ax,
where x ∈ Rn is a vector, x⊤ ∈ (Rn )∗ its transpose (a covector), A an m × n matrix, and
A⊤ its transpose (an n × m matrix). The result is a scalar.
We can evaluate such mixed expressions—using matrix multiplication—as if
 
 
x1
x1
 x2 
 x2 
 
 
• vectors in Rm were m × 1 matrices:  ..  ”=”  .. ;
 . 
 . 
xm
xm
78




• covectors in (Rn )∗ were 1 × n matrices: x1 x2 · · · xn ”=” x1 x2 · · · xn ; and
 
• scalars were 1 × 1 matrices: λ”=” λ .
Since it is only “as if”, the result may differ from the matrix multiplication result in the
surrounding brackets. But associativity still holds, so evaluation order does not matter
(Lemma 2.42), and when we transpose a product, we can instead transpose the individual
factors and multiply them in reverse order (Lemma 2.40). Transposing a scalar has no
effect, so λ⊤ = λ.
For example, we can evaluate x⊤ A⊤ Ax as
⊤
x
A |{z}
Ax ,
|{z}
covector vector
| {z }
scalar

and according to Table 2.2, the result is the scalar product of the vectors (x⊤ A)⊤ = Ax
(the transpose of the covector x⊤ A) and Ax; so we have the scalar product of a vector
with itself. This means that
x⊤ A⊤ Ax = ∥Ax∥2 ,
the squared Euclidean norm (length) of Ax; see Definition 1.11.
But we could also evaluate x⊤ A⊤ Ax as
⊤
x
or |{z}
x⊤ |A{z
A} |{z}

⊤
x
x⊤ |A{z
A} |{z}
|{z}
covector matrix vector
| {z }
covector
{z
}
|

covector matrix vector

|

scalar

| {z }
vector
{z
}

scalar

In all cases, the result is the same. Let us look at another example, v⊤ wv⊤ w where
v, w ∈ Rm . This is
⊤
⊤
⊤
2
v
| {zw} = (v w) ,
| {zw} v
scalar scalar

|

{z

scalar

}

the square of the scalar product v⊤ w. But we could also evaluate this as
⊤
w .
v⊤ wv
|{z}
| {z } |{z}
covector matrix vector
| {z }
vector
|
{z
}
scalar

Although the final result is the same, we better not compute it this way: as an intermediate
step, this evaluation order computes an m × m matrix, the outer product wv⊤ of w and
v. If m = 1, 000, this matrix has 1, 000, 000 (one million) entries, so its computation alone
requires 1, 000, 000 multiplications wi vj , one for each entry. Using the first evaluation
order, the final result (v⊤ w)2 can be computed with 1, 001 multiplications only.
79

As everything “is” matrix multiplication, the general question behind this is the following: given k matrices A1 , A2 , · · · Ak of dimensions a×n1 , n1 ×n2 , . . . nk−1 ×b, what is the
most efficient evaluation order for computing their product, the a × b matrix A1 A2 · · · Ak ?
In other words, how shall we put the brackets so that we need the least effort? Already
for k = 3, we can convince ourselves that the two possible evaluation orders (A1 A2 )A3
and A1 (A2 A3 ) usually require different efforts, so which one is more efficient? As k grows,
the number of possible evaluation orders quickly becomes too large to assess the effort
for all of them individually, so we need to be smarter. Fortunately, there is a dynamic programming algorithm that—given a, n1 , n2 , . . . , nk−1 , b—finds the most efficient evaluation
order quickly. This is covered in algorithms textbooks; see for example Introduction to
Algorithms [CLRS22, Section 14.2].
Everything “is” composition. Table 2.2 presents several kinds of multiplications that
look like matrix multiplications in the sense that they produce the same numbers; but
since they involve vectors, covectors, and scalars, they are logically quite different from
matrix multiplications. But we can develop a unifying view of all these multiplications
that also explains why the same numbers result from logically different multiplications.
The key insight is that all objects involved (matrices, vectors, covectors, and scalars) correspond to transformations in a natural way, and with the property that all the multiplications in Table 2.2 correspond to compositions of these transformations. Table 2.3 presents
the transformations.
Object
Matrix
A ∈
Vector
v
∈
⊤
Covector v
∈
Scalar
λ
∈

m×n

R
Rm
Rm
R

Corresponding transformation
TA : Rn → Rm , x
Tv
: R
→ Rm , x
Tv⊤ : Rm → R
, x
Tλ
: R
→ R
, x

7→
7
→
7
→
7
→

Ax
xv
P

m
i=1 vi xi

λx

Table 2.3: Transformations corresponding to matrices, vectors, covectors, and scalars
For covectors, this is nothing new, since a covector v⊤ is by Definition 1.20 already the
transformation Tv⊤ in Table 2.3. For a matrix A, the corresponding transformation in the
table is also well-known to us: it is the matrix transformation TA , see Definition 2.18. In
case of vectors and scalars, wewere
 so far not familiar with their corresponding transfor1
mations. For example, if v =
∈ R2 , then
2
   
1
x
2
Tv : R → R , x 7→ x
=
.
(2.2)
2
2x
Now the following can be proved (but we will not do this): whenever α and β are
some objects such that their product αβ is defined according to Table 2.2, it holds that
Tαβ = Tα ◦ Tβ .
80

This means that the composition of Tα and Tβ gives the transformation corresponding to
the product of α and β.
If α = A and β = B are matrices, we already know that TAB = TA ◦ TB , see Corollary 2.37, because we have defined the product AB precisely in such a way that this holds;
see Definition 2.36.
For the other products in Table 2.2, they also have to be like that in order to fulfill
Tαβ = Tα ◦ Tβ in all cases. Once we have this, associativity of mixed expressions (all
evaluation orders lead to the same final result) follows in the same way as previously for
matrix multiplication; see the proof of Lemma 2.42 (ii).
We can also see from Table 2.3 that the transformations corresponding to flat matrices
differ from the ones for scalars, vectors, and covectors only in a small detail: whenever the
former have R1 as domain or codomain, it is R for the latter. Formally, R = {λ : λ ∈ R}
and R1 = {(λ) : λ ∈ R} are different sets, but the difference is just a matter of putting
brackets or not around a number; this explains why in Table 2.2, the flat matrix multiplications produce the same numbers as their corresponding vector / covector /scalar
versions, and only the brackets are
 different. Taking up the example in (2.2), here is the
1
flat matrix version of it: if A =
, then
2
 
 
1
x
1
2
TA : R → R , (x) 7→
(x) =
.
2
2x
We can now also understand which mixed expressions are well-formed and what their
types are. For example, if x ∈ Rn and A is an m×n matrix, we can write x⊤ A⊤ Ax, because
Tx⊤ A⊤ Ax : R

Tx⊤
TA⊤
TA
Tx
← Rn ← Rm ← Rn ← R.

In the composition Tx⊤ A⊤ Ax = Tx⊤ ◦ TA⊤ ◦ TA ◦ Tx , the codomain of each transformation
equals the domain of the next transformation (left of it). As the result is Tx⊤ A⊤ Ax : R → R,
Table 2.3 tells us that x⊤ A⊤ Ax is a scalar.
Considering just x⊤ A⊤ , we have
Tx⊤
TA⊤
n
Tx⊤ A⊤ : R ← R
← Rm ,
so Tx⊤ A⊤ : Rm → R, and x⊤ A⊤ ∈ (Rm )∗ is a covector by Table 2.3.
But the mixed expression xA⊤ , for example, is ill-formed (incorrectly structured) and
does not correspond to a multiplication in Table 2.2. This is because we get a mismatch
between the codomain of TA⊤ and the domain of Tx :
n

TxA⊤ : R

Tx
TA⊤
n
← R̸=R
← Rm .

We conclude this section with yet another way to do matrix multiplication, namely
via covector-matrix multiplications. Recall that Definition 2.36 has introduced matrix
81

multiplication via matrix-vector multiplication, expressing the result in column notation.
Symmetrically, we can now also define it via covector-matrix multiplication and express
the result in row notation.

| |
|

|

| |

Observation 2.45 (Matrix multiplication in row notation). Let


u⊤
1
⊤


u
2


A=

..


.
⊤
ua

| |

(2.3)

|

|

| |

be an a × n matrix and B an n × b matrix. The product AB is the a × b matrix


u⊤
1B


u⊤
2B


.

..


.
⊤
ua B
Proof. Writing B in column notation as



|
|
|
B = x1 x2 · · · xb  ,
|
|
|

Definition 2.43 of covector-matrix multiplication shows that row i of (2.3) is

 ⊤
⊤
ui x1 u⊤
i x2 · · · ui xn ,
and this is exactly what AB has in row i according to matrix multiplication in scalar
product form (Observation 2.38).

2.3.5

CR decomposition

After having talked a lot about matrix transformations and linear transformations, we
come back to some “pure” matrix theory as we have started it in Section 2.1.
You know the prime factor decomposition of a natural number. For example, this
writes the number 1001 as
1001 = 7 · 11 · 13.
This decomposition tells you something about the “structure” of the number that you
cannot easily tell from just looking at the number.
In linear algebra, it is mostly matrix decompositions that are of interest. This means,
we want to write a matrix A as a product of other matrices, with the goal of learning more
about A, and potentially speeding up computations involving A.
In this section, we will see a first such decomposition.
82

Theorem 2.46 (CR decomposition). Let A be an m × n matrix of rank r (Definition 2.10). Let
C be the m × r submatrix of A containing the independent columns. Then there is a unique r × n
matrix R′ such that
A = CR′ .
It will become clear in Theorem 3.18 why we call the matrix R′ and not simply R.
Before we go to the proof, let us do an example. Consider the rank-1 matrix


2 4 6
A=
3 6 9
with one independent column (the first one). In this case, the CR decomposition is
 

2 
1 2 3 .
A=
3 | {z }
|{z} R′ ,1×3
C,2×1

The columns of R′ contain the scalars that we need in order to write each column as a
scalar multiple of the first one. Something very similar is going on in general.
Proof of Theorem 2.46. We know that A and C have the same column space (Lemma 2.11),
so every column of A can be written as a linear combination of the columns of C. If A
has columns v1 , v2 , . . . , vn ∈ Rm , we therefore have vj = Cxj for all j, where xj ∈ Rr is a
vector of r scalars. This uses that matrix-vector multiplication is simply another notation
for linear combinations; see Definition 2.4. But then we get




|
|
|
|
|
|
A = v1 v2 · · · vn  = C x1 x2 · · · xn  = CR′ ,
|
|
|
|
|
|
|
{z
}
R′ ∈Rr×n

using Definition 2.36 of matrix multiplication. So we have a decomposition of the required form, where the matrix C contains the independent columns of A; the equation
A = CR′ means that the matrix R′ contains scalars that express all columns as linear combinations of the independent columns. Since the columns of C are linearly independent,
there are unique such scalars by Lemma 1.24, so there is a unique matrix R′ that satisfies
the equation A = CR′ .
Here is an example where A has rank 2:

 


1 2 0 3
1 0 
1
2
0
3
2 4 1 4 = 2 1
.
0 0 1 −2
3 6 2 5
3 2 |
{z
}
|
{z
} | {z }
R′ , 2×4
A, 3×4

C, 3×2

83

To see that this is the CR decomposition, we look at the matrix A column by column.
Each of the four columns vj , j = 1, 2, 3, 4, is either independent (and then vj = 1vj is the
unique linear combination), or it is dependent and therefore a unique linear combination
of the previous independent columns. In the example, the first and the third column are
independent, a fact that you can simply believe at this point. We do not yet know how
to systematically find the independent columns of a matrix, and the scalars that express a
dependent column as a linear combination of the previous independent ones.

=

=

=

v1

1v1

2v1

=

 
3
4
5

=


      
1 2 0 3
1
2
0







2
4
1
A= 2 4 1 4
3 6 2 5
3
6
2

=

=

columns of A
v2
v3
v4
=

v1

3v1

v2
v3
v4
independent?

yes

no

1v3

−2v3

yes

no

The resulting CR decomposition is therefore indeed

 


1 2 0 3
1 0 
3
2 4 1 4 = 2 1 1 2 0
.
0 0 1 −2
3 6 2 5
3 2 |
{z
}
|
{z
} | {z }
R′ , 2×4
A, 3×4

C, 3×2

We will see in Theorem 3.18 how the CR decomposition can systematically be computed; in fact, we get it as by-product of transforming A into a standard form R. Lemma 4.19
and Theorem 4.32 interpret C and R′ as compact representations of the column space and
the row space of A. Here, we conclude with a “data compression” view of the CR decomposition.
Matrix compression. An m × n matrix A has mn entries, and if m and n are large, mn
can be very large. As a result, storing A or transmitting A over some network link might
be quite costly. If A has low rank r, the CR decomposition provides a compression of A
such that it takes up less space. To compress A, we do not store A but the two matrices
C and R′ of its CR decomposition. From C and R′ , we can reconstruct A whenever we
want via the matrix multiplication A = CR′ ; so no information is lost. But since C has mr
entries, and R′ has rn entries, we need to store only (m + n)r entries instead of mn. This
is not always a saving (in the example above it is not), but if r is small compared to m and
n, the savings can be substantial.
84

For example, if m = n = 1, 000, A has 1, 000, 000 (one million) entries. Now assume
that A has only rank 10. Then C and R′ together only have 2, 000 · 10 = 20, 000 entries.
This is fifty times less than A has.
Exercise 2.47. What are the matrices C and R′ in Theorem 2.46 if A is an m × m matrix of rank
m (the columns are linearly independent)? What are the matrices C and R′ if A is the m × n zero
matrix? The second question also requires you to think about m × 0 and 0 × n matrices. While
these are not extremely relevant in practice, it is good to make sure that our definitions and proofs
also work for them.

2.4

Invertible and Inverse matrices

We introduce the important class of invertible matrices, the ones that lead to an “undoable” matrix transformation TA : x 7→ Ax. It turns out that these are exactly the
square matrices with linearly independent columns. Moreover, the transformation
that is undoing TA is also a matrix transformation, and we call its matrix the inverse
of A. The inverse A−1 of an invertible matrix A is the higher-dimensional analog of
the reciprocal 1/a of a nonzero real number a that is also written as a−1 .

2.4.1

Undoing matrix transformations

In Figures 2.6 and 2.7, we have seen four matrix transformations TA : R2 → R2 , and what
they geometrically “do” (scale the input, mirror it, rotate it, or shear it). In each of these
four cases, we can “undo” the action (i.e. transform the output back to the input) with
another matrix transformation whose matrix we call A−1 ; Figure 2.13 shows this for the
scaling and mirroring transformations from Figure 2.6. You are invited to also find the
matrices A−1 for the transformations from Figure 2.7.




1 0
0 1
A=
A=
0 34
1 0
−→ “do” −→
X
A(X) ←− “do” ←−
A(X)

Ae2
Ae1

−→ “undo”
 −→

1 0
−1
A =
0 43

e2
e1

←− “undo”
 ←−

0 1
−1
A =
1 0

Ae1
Ae2

Figure 2.13: Left: To undo stretching by a factor of 3/4, stretch by the reciprocal factor of
4/3! Right: to undo mirroring, mirror again!
Not every matrix transformation is undoable. Consider A = 0 in which case TA (x) = 0
for all x. Since the output is always 0, we cannot uniquely reconstruct the input.
Before we try to understand which matrix transformations are undoable, let us formalize the concept of undoable functions in general.
85

Definition 2.48 (Injective, surjective, and bijective functions). Let X, Y be sets and f : X →
Y a function.
(i) f is called injective if for every y ∈ Y , there is at most one x ∈ X with f (x) = y.
(“For every possible output, at most one input leads to it.”)
(ii) f is called surjective if for every y ∈ Y , there is at least one x ∈ X with f (x) = y.
(“For every possible output, at least one input leads to it.”)
(iii) f is called bijective (undoable) if f is both injective and surjective.
(“For every possible output, exactly one input leads to it.”)
(iv) The inverse of a bijective function f is the function
f −1 : Y → X,

y 7→ the unique x ∈ X such that f (x) = y.

If f (x) = y, then f −1 (y) = x by definition of f −1 , so f −1 is indeed undoing f . This can
also be written as f −1 (f (x)) = x for all x ∈ X, or (using Definition 2.33 of composition) as
f −1 ◦ f = id, where id : X → X, x 7→ x is the identity function (in abuse of notation, we use
the same symbol id for every domain).
Vice versa, if f −1 (y) = x, then f (x) = y by definition of f −1 , so f is undoing f −1 , as you
may have expected. This can also be written as f (f −1 (y)) = y for all y ∈ Y , or f ◦ f −1 = id.
Along these lines, it is easy to prove that f −1 is also bijective, and that its inverse is f .
We summarize this in the following fact.
Fact 2.49 (Bijective functions and their inverses). If f : X → Y is bijective, then f −1 : Y → X
is also bijective, and (f −1 )−1 = f . Moreover, f −1 ◦ f = id (f −1 is undoing f ) and f ◦ f −1 = id
(f is undoing f −1 ).
Next we look at matrix transformations TA : Rn → Rm . If m ̸= n, TA can never be
bijective. We separately prove this for the two cases m < n (when A is wide) and m > n
(when A is tall); see Figure 2.1 for these matrix shapes.
Lemma 2.50 (Wide matrix transformations are not injective). Let TA : Rn → Rm be a matrix
transformation, and m < n. Then TA is not injective (and therefore also not bijective).
Intuitively, this seems clear. If m < n, then TA is “squeezing” a high-dimensional
space Rn into a low dimensional space Rm , in which case overlaps (inputs with the same
output) seem unavoidable. One has to be careful with this intuition, though. It is true for
“nice” functions such as matrix transformations, but not for all functions.
Proof of Lemma 2.50. We show that there is more than one input leading to output 0. Towards this, the main thing to observe is that a wide matrix A has linearly dependent
columns. To see this, we consider the first m columns. Either these are already linearly
dependent, so all columns are linearly dependent; or the first m columns are linearly
86

independent. But then they span Rm by Lemma 1.28, so the next column is a linear combination of the first m columns, and again, all columns are linearly dependent.
Knowing this, we can express 0 as a nontrivial linear combination of the columns, so
we have TA (x) = Ax = 0 for some nonzero vector x ∈ Rn of scalars; see also Observation 2.5 (ii). But this shows that TA is not injective: we have two different inputs, 0 and
some x ̸= 0, leading to the same output TA (0) = TA (x) = 0.
Lemma 2.51 (Tall matrix transformations are not surjective). Let TA : Rn → Rm be a matrix
transformation, and m > n. Then TA is not surjective (and therefore also not bijective).
Again, this seems quite intuitive: If m > n, then TA is “embedding” a low-dimensional
space Rn into a high-dimensional space Rm , but such an embedding cannot fill the whole
space (think of a line or a plane in 3-dimensional space). Also here, this intuition is only
true for nice functions.
Proof of Lemma 2.51. We construct a vector y ∈ Rm such that no input leads to output
y. As we argued in the previous proof of Lemma 2.50, every wide matrix has linearly
dependent columns; this in particular applies to A⊤ , the transpose of A, so we can express
0 as a nontrivial linear combination of the columns of A⊤ . In other words, there is a
nonzero vector y ∈ Rm such that A⊤ y = 0. We now want to show that there is no input
x ∈ Rn with output TA (x) = Ax = y.
For this, we use a proof by contradiction. This works as follows. If we want to prove
that some statement is true, we first assume the opposite. If we can logically derive some
nonsense from this assumption, we know that the assumption must have been false; so
the statement is true after all. The full story is quite a bit more subtle than we tell it here,
but we will not go into this and simply use the proof by contradiction principle.
In our case, we want to prove that there is no x such that Ax = y, so let us first assume
the opposite, namely that there is such an x. Also recall that we have constructed y ̸= 0
such that A⊤ y = 0. Now we consider the number
y⊤ Ax.
This is the product of a covector, a matrix, and a vector; we refer back to Section 2.3.4
where we have introduced such “mixed” multiplications and explained how they behave
like matrix multiplications. Hence, using associativity and the transpose-of-a-product
Lemma 2.40, we get
y⊤ Ax = y⊤ A x = 0⊤ x = 0.
|{z}
(A⊤ y)⊤ =0⊤

On the other hand, by our opposite assumption and using the other evaluation order, we
get
y⊤ Ax = y⊤ |{z}
Ax = y⊤ y = ∥y∥2 ̸= 0,
y

since y ̸= 0, so its Euclidean norm (length; see Definition1.11) is also nonzero. But now we
have logically derived the desired nonsense: y⊤ Ax is at the same time zero and nonzero.
87

So the assumption that some input x leads to output TA (x) = Ax = y must have been
false, and in reality no input leads to output y. Hence, TA is not surjective.
As a consequence of Lemma 2.50 and Lemma 2.51, the only candidates for bijective
(undoable) matrix transformations are the ones of the form TA : Rm → Rm (for which A
is an m × m square matrix). Not every m × m matrix A leads to a bijective TA (consider
A = 0 ∈ Rm×m for m > 0 as the easiest counterexample).
Now we want to show that if such a TA is bijective, then the inverse function TA−1 :
Rm → Rm is again a matrix transformation. Since matrix transformations and linear
transformations are the same objects (Observation 2.22 and Theorem 2.26), we argue with
linear transformations, which saves us from having to “invent” the matrix of TA−1 manually.
Lemma 2.52 (The inverse of a bijective linear transformation). Let T : Rm → Rm be a
bijective linear transformation. Then its inverse T −1 : Rm → Rm is also a linear transformation
(and bijective by Fact 2.49).
Proof. We prove linearity of T −1 according to Definition 2.21. For this, we must show that
T −1 (λ1 y1 + λ2 y2 ) = λ1 T −1 (y1 ) + λ2 T −1 (y2 ),

(2.4)

for all y1 , y2 ∈ Rm and all λ1 , λ2 ∈ R. To see this, we define x1 := T −1 (y1 ), x2 := T −1 (y2 ).
Since T is undoing T −1 (Fact 2.49), we have T (x1 ) = y1 and T (x2 ) = y2 . Linearity of T
thus gives
T (λ1 T −1 (y1 ) +λ2 T −1 (y2 )) = λ1 y1 +λ2 y2 .
|{z}
|{z}
| {z }
| {z }
x1

x2

T (x1 )

T (x2 )

Applying T −1 to both sides is undoing T on the left side (Fact 2.49, again), and we obtain
λ1 T −1 (y1 ) + λ2 T −1 (y2 ) = T −1 (λ1 y1 + λ2 y2 ).
This is the desired linearity (2.4) of T −1 .
Using this, we can now prove the following result that characterizes the undoable
matrix transformations in terms of matrix properties.
Lemma 2.53 (Bijective matrix transformations). Let A be an m × m matrix. The following
three statements are equivalent.
(i) TA : Rm → Rm is bijective.
(ii) There is an m × m matrix B such that BA = I.
(iii) The columns of A are linearly independent.

88

Proof. We prove this via the three circular implications (i)⇒(ii)⇒(iii)⇒(i).
(i)⇒(ii): If TA is bijective, it has an inverse TA−1 , see Definition 2.48 (iv). Since TA is a
linear transformation by Observation 2.22, we know from Lemma 2.52 that TA−1 is another
linear transformation and therefore also a matrix transformation TB by Theorem 2.26.
For a bijective function f , we have f −1 ◦ f = id (f −1 is undoing f ; see Fact 2.49). Here,
this reads as
TB ◦ TA = |{z}
id .
| {z }
Cor. 2.37

=

TBA

TI

This writes the same linear transformation as a matrix transformation in two different
ways, and by Theorem 2.26, both ways must result in the same matrix. So we get
BA = I.
(ii)⇒(iii): If there is an m×m matrix B such that BA = I, we will argue that x = 0 is the
only vector such that Ax = 0. This means that the columns of A are linearly independent;
see Observation 2.5 (ii). For the argument, consider any x ∈ Rm with Ax = 0. We then
get x = (BA)x = B(Ax) = B0 = 0.
(iii)⇒(i): If the columns of A are linearly independent, we will deduce that TA is both
injective and surjective, and hence bijective by Definition 2.48. Let us start with surjectivity: given any possible output y ∈ Rm , we need to find some input x ∈ Rm such that
TA (x) = Ax = y. In other words, we need to express y as a linear combination of the
columns of A; see Observation 2.5 (i). But this can be done since the m linearly independent columns of A span the whole space Rm by Lemma 1.28.
For injectivity, we still need to show that there is only one input leading to output y.
In other words, y can be expressed as a linear combination of the columns of A in only
one way. This is true by Lemma 1.24.
Because matrix multiplication is in general not commutative, the following is an interesting property.
Lemma 2.54. Let A, B be m × m matrices such that BA = I. Then also AB = I.
Proof. By Lemma 2.53, BA = I means that TA is bijective, in particular surjective. So for
every y ∈ Rm we have some x ∈ Rm with Ax = y and therefore get
ABy = AB(Ax) = A(BA)x = Ax = y.
So TAB : Rm → Rm is the (linear) identity transformation TI , and since a linear transformation has a unique matrix by Theorem 2.26, we must have AB = I.

2.4.2

Definitions and basic properties

Lemma 2.53 characterizes the matrices whose transformations are undoable. Here we
give those matrices a name and look into their properties.
89

Definition 2.55 (Invertible and singular matrix). Let A be an m × m matrix. A is called
invertible if there is an m × m matrix B such that BA = I. (By Lemma 2.53 this is the case if
and only if A has linearly independent columns.) Otherwise, A is called singular.
It is also fine to call a singular matrix non-invertible. You also see invertible matrices
being called non-singular. In some sources, the condition AB = I is used instead, in yet
others, it is AB = BA = I. Because of Lemma 2.54, this makes no difference.
Observation 2.56 (Alternative definitions of an invertible matrix). Let A be an m×m matrix.
A is invertible if and only if there is an m × m matrix B satisfying one (and therefore all) of the
following three equivalent conditions.
(i) AB = I.
(ii) BA = I. (This means that A is invertible according to Definition 2.55.)
(iii) AB = BA = I.
Moreover, the matrix B is unique.
Proof. By Lemma 2.54, we have (i)⇔(ii), and via this also get (i)⇒(iii) and (ii)⇒(iii). The
reverse implications (iii)⇒(i) and (iii)⇒(ii) are obvious, so we get equivalence of all three
conditions: (i)⇔(iii)⇔(ii). To see uniqueness, consider two matrices B, B ′ satisfying the
conditions. Then
(ii)
(i)
B = IB = (B ′ A)B = B ′ (AB) = B ′ I = B ′ .
We can now summarize the situation and define the inverse of an invertible matrix.
Definition 2.57 (Inverse matrix). Let A be an m × m matrix. A is invertible if and only if there
exists an m × m matrix B such that BA = I (or AB = I, or AB = BA = I). In this case, the
matrix B is unique and called the inverse of A. We denote it by A−1 .
As a consequence, A−1 is also invertible, and its inverse is the original matrix A.
Observation 2.58 (Inverse of the inverse). Let A be an invertible m × m matrix. Then A−1 is
also invertible and
(A−1 )−1 = A.
Proof. We have AA−1 = A−1 A = I by Definition 2.57, so A satisfies the conditions for the
inverse of A−1 .
 
a for some a ∈ R,
Let us look
at
the
situation
for
small
m.
If
m
=
1,
then
A
=

1
and I =
.
According
to
Definition
2.57,
the
invertibility
condition
is that there is some

B = b , b ∈ R, such that
    
b a = 1 .
| h{zi }
ba
In other words,
ba = 1, so b must be the reciprocal of a which
exactly if a ̸= 0.

 exists

Hence, a is invertible exactly if a ̸= 0, and then its inverse is 1/a .
90

Case 1 × 1.
 
A= a

⇒

A−1 =

1
a

(if a ̸= 0).

For m = 2, the situation is as follows.
Case 2 × 2.



a b
A=
c d

⇒

−1

A



1
d −b
=
a
ad − bc −c

(if ad − bc ̸= 0).

To see that this formula for A−1 is correct, you can simply check that A−1 satisfies
one of the conditions for B in Definition 2.57. Again, A is not always invertible, but the
restriction given here (ad − bc ̸= 0) is less obvious than for m = 1. It is not hard to see (try
it!) that this condition—expressed in words—reads as follows: The two columns of A are
linearly independent.
If ad − bc = 0, we only see that the given formula for A−1 fails due to division by zero,
but there could theoretically be another formula. But putting together a few things that
we already know, we see that A is indeed singular if ad − bc = 0: in this case, the columns
of A are linearly dependent, so they span at most a line; see for example Figure 1.23 (left).
Any b ∈ R2 outside of that line is not a linear combination of the columns, so there are
no scalars x ∈ R2 with Ax = b; see Observation 2.5. In still other words, the matrix
transformation TA : x 7→ Ax cannot produce output b, so it is not surjective and therefore
also not bijective (undoable). Then Lemma 2.53 tells us that there can be no matrix B
satisfying the conditions in Definition 2.57.
It is possible to check directly that there is no matrix B with BA = I if ad − bc = 0;
this boils down to showing that a system of linear equations in four variables (the entries
of B) has no solution. While this can certainly be done in some ad-hoc way, it is not
very insightful. The systematic approach to check for invertibility goes via the concept of
determinants to which we will only get in the second part of the lecture.
For invertible m × m matrices with m > 2, there is also a formula for the inverse, based
on Cramer’s rule, but also this requires determinants.
The next lemma lets us compute the inverse of a product of two invertible matrices.
Lemma 2.59. Let A and B be invertible m × m matrices. Then AB is also invertible, and
(AB)−1 = B −1 A−1 .
Recall that AB is the matrix of the linear transformation TAB = TA ◦ TB (first do TB ,
then TA ); see Corollary 2.37. To undo this, we need to apply the inverse transformations in
reverse order: TB −1 ◦TA−1 = TB −1 A−1 (first undo TA , then TB ). You can already consider this
a proof sketch, but there is also a more direct proof not involving linear transformations.
Proof. We simply verify that B −1 A−1 satisfies the invertibility condition of Definition 2.57
for the matrix AB:
(B −1 A−1 )(AB) = B −1 (A−1 A)B = B −1 IB = B −1 B = I.

91

Lemma 2.59 naturally extends to more matrices, for example (ABC)−1 = C −1 B −1 A−1 ,
but we skip the formal statement and its proof.
Inversion also commutes with transposition (Definition 2.12).
Lemma 2.60. Let A be an invertible m × m matrix. Then the transpose A⊤ is also invertible, and
−1
⊤
A⊤
= A−1 .
Proof. We have
A−1

⊤

A⊤ = (AA−1 )⊤ = I ⊤ = I,

using Lemma 2.40 for the first and Definition 2.57 for the second equality. Using the
definition again for matrix A⊤ , the statement follows.
What remains open at this point is how to check whether a given m × m matrix is
invertible, and how to compute its inverse A−1 . We will present an algorithm for this,
based on solving systems of linear equations, in Section 3.2.6 of the next chapter.

92

Chapter 3
Solving Linear Equations Ax = b
In the previous Chapters 1 and 2, we have mostly dealt with the “analytic geometry”
root of linear algebra. Based on what we have learned about vectors and matrices, we
can now look into the second root which is “systems of linear equations.” The need to
solve systems of equations is ubiquitous in science and engineering. A broadly applicable
description of those is the following: you have some unknown values (“the variables”),
and you have various pieces of information (“the equations”) about how these unknowns
interact. Based on this, you ideally want to find the values of the unknowns (“the solution”). Such systems of equations do not have to be linear, and even in high school, you
probably have seen some simple nonlinear systems.
Here is an example. A room of 16m2 is twice as deep as it is wide. What are width and
depth of the room? Here, the unknowns are width w and depth d of the room (in m). The
two pieces of information are wd = 16 and d = 2w. This is easy to solve. Plugging the
2
expression
√ for d from the√second equation into the first one gives 2w = 16, so the solution
is w = 8 and and d = 2 8. What is nonlinear here is the equation wd = 16, as it contains
the product of two variables. Also, the combined equation 2w2 = 16 is nonlinear, because
it contains a variable in its second power. Both these nonlinear equations are known as
quadratic equations. The equation d = 2w, on the other hand, is linear, since every variable
appears only in its first power, and is multiplied at most with a number, not with other
variables.
Many systems of equations that need to be solved in practice are nonlinear. Unfortunately, such systems are in general (with many variables and equations) pretty hard to
solve; the mathematical field of algebra deals with this challenge. Im comparison, systems
of linear equations are easy to solve, and this chapter will explain how. In fact, linear algebra is named this way because it only deals with systems of linear equations. Linear
algebra can therefore be considered as the “easy” branch of algebra. This might (or might
not) offer some consolation to those who also find linear algebra hard.
Although many relevant systems of equations are nonlinear, there are enough linear
ones (or nonlinear ones that can be “linearized”) in order to justify a dedicated theory
of systems of linear equations. This theory is (maybe surprisingly) still a topic of active
research in mathematics and computer science. Here, we cover classic material, though.
93

3.1

Systems of linear equations

How to solve systems of linear equations is one of the foundational problems of linear
algebra. Such systems appear in many applications today and can be very large. One
concrete application that we present is Google’s PageRank algorithm, another one is
the inversion of a matrix A. We then introduce systems of linear equations formally,
using the language of matrices and vectors.
We have already seen a system of linear equations in Section 0.3:
D = 2S
D = C +3
D + S + C = 17
With its three equations in three variables, this system encodes three pieces of information
about the ages of three children (Dominik, Susanne and Claudia). Solving the system
means to find values for the variables such that all the equations are satisfied.
In general, systems of linear equations can have arbitrarily many equations and variables; for a systematic treatment, we write such systems in a standard form, and we use
vectors and matrices to argue about them. In standard form, the variables are called
x1 , x2 , . . . and appear only left of “=” in the equations. In this form, the system from the
children’s age puzzle is
x1 − 2x2 = 0
x1 − x3 = 3
x1 + x2 + x3 = 17

(3.1)

Here, x1 stands for D, x2 for S and x3 for C.
Definition 3.1 (System of linear equations). A system of linear equations in m equations
and n variables x1 , x2 , . . . , xn is of the form
a11 x1 + a12 x2 + · · · + a1n xn
a21 x1 + a22 x2 + · · · + a2n xn

= b1
= b2
..
.

am1 x1 + am2 x2 + · · · + amn xn = bm ,
where the aij and bi stand for known real numbers, and the xi stand for unknown real numbers
that we want to compute such that they satisfy all the equations. In matrix-vector form, this can
be written as

   
x1
b1
a11 a12 · · · a1n
 a21 a22 · · · a2n   x2   b2 

   
Ax = b :  ..
..
..   ..  =  ..  .
.
.
 .
.
.
.  .   . 
xn
bm
am1 am2 · · · amn
{z
} | {z } | {z }
|
A, m×n

94

x∈Rn

b∈Rm

(Here we use matrix-vector multiplication in the form of Observation 2.6.) The matrix A is the
coefficient matrix, the vector b is the right-hand side, and the vector x is the vector of variables. Solving the system means to compute a vector x ∈ Rn such that Ax = b (or to report that
no such vector exists).
In the form Ax = b, system (3.1) reads as

   
0
x1
1 −2
0
1
0 −1 x2  =  3 .
17
x3
1
1
1
{z
} | {z } | {z }
|
x

A

b

|

|

| |

| |

A system Ax = b of linear equations can be understood in two different ways, depending on whether we look at A columnwise, or rowwise.
If A is in row notation,


u⊤
1


u⊤
2


A=
,
..


.
⊤
um
then solving Ax = b means to simultaneously solve the m equations
u⊤
i x = bi ,

i = 1, 2, . . . m.

This comes from matrix-vector multiplication with A in row notation, see Observation 2.8.
This “standard” interpretation of a system of linear equations is also the one behind the
“row picture” proof of Fact 1.5; see Figure 1.7.
If A is in column notation,


|
|
|
A = v1 v2 · · · vn  ,
|
|
|
then solving Ax = b means to express the right-hand side b as a linear combination of
the columns v1 , v2 , . . . , vn of A. This comes from matrix-vector multiplication in column
notation, see Definition 2.4 and Observation 2.5. Moreover, the system has a solution if
and only if b ∈ C(A), the column space of A (see Definition 2.9). This interpretation of a
system of linear equations is the one behind the “column picture” proof of Fact 1.5; see
Figure 1.8.

3.1.1

The PageRank algorithm

As an example where (large) systems of linear equations come up, we will discuss the
PageRank algorithm. This algorithm was published by the Stanford students Sergey Brin
and Lawrence Page in 1998 [BP98, pp. 109-110]. The first sentence of the abstract is the
following:
95

In this paper, we present Google, a prototype of a large-scale search engine
which makes heavy use of the structure present in hypertexts.
The rest is history: because of its PageRank algorithm with much better results than others, Google quickly became the dominant search engine.
To understand what PageRank does, let us look at an example. Figure 3.1 shows a link
graph where the circles represent web pages and the arrows indicate links between them.
For the whole internet, you can think of a similar figure with a billion (109 ) circles.

1
2
4

3
5

6

Figure 3.1: A link graph. The circles represent web pages, and an arrow from page p
to page q indicates that p has a link to q. The PageRank algorithm sorts the pages by
relevance.
Which of the six pages shown in Figure 3.1 do you think is most relevant? It is not clear
what we mean by that, so let us start by measuring relevance in terms of the number of
links pointing to a page. These are also called citations of the page. In scientific literature,
the number of citations that a paper has is indeed an established measure of relevance, so
it seems natural to also apply it to web pages.
This has been done long before PageRank, and according to this measure, page 2 (with
4 citations) is clearly the most relevant one; all other pages have at most 2 citations.
The main insight behind PageRank is that not all citations are worth the same. Here
are the two key points.
1. Citations from relevant pages should count more than citations from irrelevant ones.
It is too easy to collect many citations from irrelevant pages created just for that
purpose.
2. If a page cites a large number of other pages, an individual citation on that page
should count less. It is too easy to cite many (random) pages.
In both points, web page citations fundamentally differ from scientific citations, where
it is not so easy to manipulate citation counts in the described way (although it has become easier with predatory publishers that do not enforce quality standards).
96

Point 2 is easy to incorporate into the relevance measurement by simply giving less
weight to citations from pages that cite many other pages. How to address point 1 is less
clear. We want to define the relevance of a page depending on the relevances of the pages
that cite it, but this definition goes in circles. In Figure 3.1, page 2 cites page 3 which cites
page 5 which cites page 2.
A system of linear equations comes to the rescue. Let us focus on the situation in
Figure 3.1 and introduce variables x1 , x2 , . . . , x6 for the relevances of pages 1, 2, . . . , 6.
We concretely define the relevance of a page as the sum of the weighted relevances of
the pages that cite it, where the weighted relevance of a page is its relevance, divided by
the number of citations on the page (this addresses point 2). Putting this in formulas for
the relevance of page 2, we get the linear equation
x6
x1
+ x4 + x 5 + .
x2 =
2
4
We have contributions from each of the four pages that cite page 2. For page 6, the
weighted relevance is only x6 /4, because page 6 cites 4 pages. Repeating this for all pages,
we obtain a system of 6 equations in 6 variables.
Unfortunately, setting all the xj ’s to 0 is a solution from which we learn nothing. To
avoid this trivial solution, PageRank introduces a damping factor 0 < d < 1 and replaces the
equation for x2 (and all others in the same way) by
x
x6 
1
+ x4 + x5 +
.
x2 = (1 − d) + d
2
4
For d = 1, we get the previous system with the useless all-zero solution, but for d < 1, it
can be proved that the system has a unique solution with all page relevances summing
up to the number of pages. Brin and Page suggest to use d ≈ 0.85. Using d = 7/8, the
relevances (which are then called page ranks) can be computed for example by pasting
the following code into Wolfram Alpha.1
solve(
d=7/8,
x1=(1-d)+d*(x6/4),
x2=(1-d)+d*(x1/2+x4+x5+x6/4),
x3=(1-d)+d*(x1/2+x2),
x4=(1-d)+d*(x6/4),
x5=(1-d)+d*(x3/2+x6/4),
x6=(1-d)+d*(x3/2)
)
The solution (rounded to five digits) is
x1 = 0.31797, x2 = 1.6761, x3 = 1.7307, x4 = 0.31797, x5 = 1.0751, x6 = 0.88217.
This means, according to PageRank, page 3 is actually the most relevant one, followed by
pages 2, 5, 6. Pages 1 and 4 are the least relevant ones, with the same low page rank.
1

https://www.wolframalpha.com/

97

3.1.2

Matrix inversion

Here is another application of systems of linear equations. Let A be an m × m matrix.
According to the second equivalent condition in Definition 2.57, A is invertible if and
only if there is an m × m matrix B such that AB = I, the m × m identity matrix. Moreover,
if such a matrix B exists, it is unique and constitutes the inverse A−1 of A. Writing the
matrix B in column notation as


|
|
|
B = x1 x2 · · · xm  ,
|
|
|
the condition AB = I can by Definition 2.36 of matrix multiplication be written as


 
| |
|
|
|
|
Ax1 Ax2 · · · Axm  = e1 e2 · · · em ,
| |
|
|
|
|
{z
} |
{z
}
|
AB

I

where the ej ’s are the standard unit vectors in Rm . In other words, Axj = ej for all j.
Hence, to find xj , the j-th column of the inverse B = A−1 , we need to solve the system of
linear equations Ax = ej . Computation of inverse matrices therefore reduces to solving m
systems of linear equations. All these systems share the same coefficient matrix, only the
right-hand sides are different. This is a scenario that frequently happens, and algorithms
for solving systems of linear equations should (if possible) take this into consideration
in order to be more efficient in this scenario. For the algorithms that we present in this
chapter, we will see how this can be done.

3.2

Gauss elimination

We present Gauss elimination, a classic algorithm for solving systems of m equations
in the same number m of variables. Our version does not always work, but is particularly simple and reveals an important property of the coefficient matrix A. As
we show, the algorithm works exactly when the matrix A has linearly independent
columns (equivalently, when A is invertible). We will also highlight what Gauss elimination conceptually does, namely repeatedly perform row operations; these are the
key operations also in other methods for solving systems of linear equations, so we
examine their effects in detail.
For this section, we restrict to the case where the system Ax = b has a square coefficient matrix, i.e. A is assumed to be an m × m matrix. This is the important case of
“m equations in m variables.” For example, the PageRank algorithm described in Section 3.1.1 needs to solve a system of this kind, and also matrix inversion as described in
98

Section 3.1.2 reduces to systems with a square coefficient matrix A. The general case of
arbitrary A will be treated in Section 3.3.
Conceptually, Gauss elimination does the following: through row operations, it transforms the system Ax = b into a system U x = c with the same solutions but a “nice”
matrix U : an upper triangular one; see Definition 2.3 (iii). Systems with upper triangular
matrices can easily be solved through back substitution, a method that we describe next.
There are other “nice” matrix shapes that lead to easy solution methods (for example,
lower-triangular matrices); the fact that Gauss elimination as presented in most sources
transforms A into an upper triangular matrix has therefore no deeper reason.

3.2.1

Back substitution

If U is upper triangular according to Definition 2.3 (iii), the system U x = c can be solved
by back substitution. Let us look at a 3 × 3 example:

   
x1
19
2 3 4
0 5 6 x2  = 17 .
x3
14
0 0 7
| {z } | {z } | {z }
U

x

c

Here, the blue entries are the ones that are supposed to be zero in an upper triangular matrix (the others may or may not be zero). Due to this shape, equation 3 has only variable:
7x3 = 14.
We can solve this for x3 and get x3 = 2. Equation 2 has variables x2 and x3 , but as we
already know x3 = 2, we can plug in this value for x3 and then solve for x2 . In equation
1, we finally have all three variables, but if we plug in the known values for x2 and x3 , we
can solve for x1 and are done. Table 3.1 summarizes the steps.
equation
1
2
3

before substitution after substitution solution
2x1 + 3x2 + 4x3 = 19
2x1 + 11 = 19
x1 = 4
5x2 + 6x3 = 17
5x2 + 12 = 17
x2 = 1
7x3 = 14
x3 = 2

x








Table 3.1: Back substitution in an upper triangular system of 3 equations in 3 variables
If U is an upper triangular m × m matrix, this works in the same way. We go through
the equations in backwards order m, m − 1, . . . , 1. Equation i reads as
P
m
X
ci − m
j=i+1 uij xj
uij xj = ci , or (when solved for xi ) as xi =
.
uii
j=i
99

We already know the values for xi+1 , . . . , xm from the equations below. So we can substitute these variables with their known values and then get the value of xi . We note that
this only works if the diagonal entry uii is nonzero, since we have to divide by it. This
means that we need an upper triangular matrix with all diagonal entries nonzero. This is
not always achievable via Gauss elimination, but if it is, there is no choice in how to solve
for x. Hence, U x = b (and therefore also the initial system Ax = b) has a unique solution.
Algorithm 1 describes back substitution in pseudocode. This is a way to formalize an
algorithm, without having to write out all steps in a concrete programming language.
Pseudocode uses typical constructs that appear in programming languages, but it also
contains natural language and mathematical text to make it more human-readable.
In Algorithm 1, the solution vector x is initially set to 0 using the assignment symbol ←.
In the subsequent loop, its entries xm , xm−1 , . . . , x1 get their final values assigned in line 4.
Algorithm 1 Back substitution:
Returns the unique vector x such that U x = c. The matrix U must be upper triangular,
with all diagonal elements nonzero.
1: function B ACK SUBSTITUTION(U, c)
▷ U ∈ Rm×m , c ∈ Rm
2:
x ← 0 ∈ Rm
3:
for i = m, m −P1, . . . , 1 do
ci − m
j=i+1 uij xj
4:
xi ←
uii
5:
end for
6:
return x
7: end function
Here, we treat the entries of the vector x like variables in a programming language. Such
a variable has a value at any time, but this value can be updated (several times) while the
program is running. Generally, any symbol to the left of an assignment ← will refer to a
variable object in this programming language sense.

3.2.2

Elimination

If the input matrix A is not upper triangular, elimination is trying to transform the system
Ax = b into an equivalent system U x = c where U is an upper triangular matrix with all
diagonal elements nonzero. Equivalent means that both systems have the same solutions.
If elimination succeeds, we can solve the resulting system U x = c using back substitution
(Algorithm 1) and thus obtain a solution of the original system Ax = b.
Elimination transforms the system step by step. Again, we demonstrate this with a
3 × 3 example:

   
x1
19
2 3 4
4 11 14 x2  = 55 .
(3.2)
2 8 17
x3
50
{z
} | {z } | {z }
|
A

x

100

b

We would like to turn the three red nonzero entries into zeros so that the matrix becomes
upper triangular. We do this column by column, from left to right. To get rid of the 4, we
subtract 2 · (equation 1) from (equation 2) of the system:
(equation 2) : 4x1 + 11x2 + 14x3 = 55
− 2 · (equation 1) : 4x1 + 6x2 + 8x3 = 38
(equation 2’) :
5x2 + 6x3 = 17
This eliminates variable x1 from the second equation, and we obtain a transformed system
A′ x = b′ with a new second equation:

   
2 3 4
x1
19
0 5 6 x2  = 17 .
2 8 17
x3
50
|
{z
} | {z } | {z }
A′

x

b′

In this step, x was just a distraction, all that matters are the entries of A and b. To get
A′ from A, we subtract 2 · (row 1) from (row 2). The same operation transforms b to b′
(here, a row is just a single number).
Row subtractions. Subtracting a multiple of some row from another row is a row subtraction. This is our first kind of row operation. It can also be viewed as a linear transformation applied to all columns of A, and to b. We know that each linear transformation
comes from a matrix; see Section 2.2.3. In our example, the transformation is
 

 
 
v1
v1
v1
1 0 0
TE21 : v2  7→ v2 − 2v1  = −2 1 0 v2  .
v3
v3
v3
0 0 1
{z
}
|
E21

This leaves the first and third row (v1 and v3 ) unchanged but subtracts 2·(row 1) (this is
2v1 ) from (row 2) (this is v2 ).
Applying this linear transformation to all columns of A simply means to premultiply
A with E21 , because this precisely corresponds to the desired columnwise matrix-vector
multiplication; see Definition 2.36. To get the transformed right-hand side, we premultiply b with E21 . To summarize, matrix and right-hand side of the transformed system
A′ x = b′ can be computed as
A′ = E21 A,

b′ = E21 b,

E21 : ”subtract 2·(row 1) from (row 2)”.

E21 is called an elimination matrix. Generally, we use Eij to denote an elimination matrix
that is supposed to create a zero entry in row i and column j.
To argue that this transformation does not change the solutions, we need that it is
undoable, meaning that the matrix E21 is invertible (see Section 2.4.2). Indeed, a row
101

subtraction is undone by the corresponding row addition, and in our example, the matrix
for this row addition is


1 0 0
−1
−1
E21
= 2 1 0
E21
: ”add 2·(row 1) to (row 2)”.
0 0 1
Now we can make the argument that the original system Ax = b and the transformed
system E21 Ax = E21 b have the same solutions. First, whenever we have an x such that
Ax = b, we can multiply both sides from the left with E21 and also get E21 Ax = E21 b.
Vice versa, whenever we have an x such that E21 Ax = E21 b, we can multiply both sides
−1
−1
from the left with E21
(which cancels E21 due to E21
E21 = I) and also get Ax = b.
From E21 A = E21 b, it takes two more steps to turn the remaining two red entries
into zeros; in each step, the diagonal entry of the current column is used to eliminate the
nonzeros below it. This entry is called the pivot. Here are all three elimination steps.



 
2 3 4
19



fat number: the pivot
A = 4 11 14
b = 55
2 8 17
50
subtract 2·(row
1) from
↓

↓
 

 (row 2):
2 3 4
19
1 0 0
E21 A = 0 5 6
E21 b = 17
E21 = −2 1 0
2 8 17
50
0 0 1
subtract 1·(row
1)
from
(row
3):
↓
↓


 


2 3 4
19
1 0 0





0 1 0
E31 E21 b = 17
E31 E21 A = 0 5 6
E31 =
0 5 13
31
−1 0 1
subtract 1·(row
2) from
↓

 (row 3):

 
↓
1
0 0
2 3 4
19
1 0
E32 = 0
E32 E31 E21 A = 0 5 6
E32 E31 E21 b = 17
{z
}
{z
}
|
|
0 −1 1
0 0 7
14
c
U
↑ elimination matrices
done!
The result is precisely the system U x = c in upper triangular form that we have previously solved with back substitution in Section 3.2.1. As solutions never change during
elimination, the vector x ∈ R3 computed in Table 3.1 also solves the original system (3.2)
(you are invited to check this!).
This all looked very nice and simple, but what happens if a pivot is 0? Then we cannot
use it to eliminate nonzeros below it. However, if there is a nonzero entry anywhere below
the pivot in the same column, we can perform a row exchange to obtain a nonzero pivot.
Row exchanges. A row exchange is our second kind of row operation. It simply exchanges two rows of the current system of equations, in order to ensure a nonzero pivot.
102

Here is an example for this situation. In the example, we are immediately done after the
row exchange, but in general, we would now use the new nonzero pivot to eliminate the
nonzeros below it.


2 3 4
A = 4 6 14
b = ···
2 8 17
elimination in column 1:
↓

↓
2 3 4
E31 E21 A = 0 0 6
E31 E21 b = · · ·
0 5 13
pivot is
0:
exchange
(row
2)
and
(row
3):
↓

↓


2 3 4
1 0 0



P23 E31 E21 A = 0 5 13
P23 E31 E21 b = · · ·
P23 = 0 0 1
{z
}
{z
}
|
|
0 1 0
0
0
6
c
U
↑ permutation matrix
done!
A row exchange can also be interpreted as a linear transformation, one that reorders
the entries of each column; the matrix of such a transformation is a permutation matrix.
A row exchange is a special reordering that only exchanges two entries. As before with
row subtractions, we can argue that a row exchange is undoable and as a consequence
does not change the solutions of the linear system of equations. Here, this is even more
obvious: to undo the exchange of two rows, simply exchange them again.
Failure. Finally, there is an ugly case: the pivot is 0, and all entries below it are also 0, so
that no row exchange helps and we are stuck. We also consider it ugly if this happens in
the last column. In this case, we give up. Here are two examples.


2 3 4
A = 4 6 14
2 3 17
elimination in column 1:
↓


2 3 4
E31 E21 A = 0 0 6
0 0 13
pivot is 0, no row exchange helps: elimination fails.

b = ···
↓
E31 E21 b = · · ·



2 3 4
0 5 6
0 0 0
we can also
fail in the
last column!

Gauss elimination in pseudocode. Pseudocode for Gauss elimination with row operations (row subtractions and row exchanges) highlighted in red is given in Algorithm 2.
As explained, the algorithm may fail, and in Section 3.2.4, we take this as an opportunity to understand exactly when this happens. Some sources avoid this opportunity by
presenting versions of Gauss elimination that transform any square matrix A to an upper triangular matrix U , possibly with some zeros on the diagonal (which makes back
substitution a bit more tricky, since there might be no or several solutions).
103

Algorithm 2 Gauss elimination:
Returns a triple (U, c, result) such that the systems Ax = b and U x = c have the same solutions. If result = ”succeeded”, U is upper triangular with all diagonal elements nonzero.
1: function G AUSS E LIMINATION(A, b)
▷ A ∈ Rm×m , b ∈ Rm
2:
U ← A, c ← b
3:
for j = 1, 2, . . . , m do
▷ eliminate in column j
4:
if ujj = 0 then
▷ zero pivot
5:
if there is some k > j such that ukj ̸= 0 then
6:
exchange (row j) and (row k) (in both U and c)
▷ row operation
7:
else
▷ give up
8:
return (U, c, ”failed”)
9:
end if
10:
end if
▷ now, ujj ̸= 0
11:
for i = j + 1, j + 2, . . . , m do
▷ make uij = 0
uij
▷ we want uij − λujj = 0
12:
λ ← ujj
13:
subtract λ · (row j) from (row i) (in both U and c)
▷ row operation
14:
end for
15:
end for
▷ now, U is upper triangular, with all diagonal elements nonzero
16:
return (U, c, ”succeeded”)
17: end function

3.2.3

Row operations: the general picture

Previously, we have argued (on 3 × 3 examples) that row subtractions and row exchanges
as performed during Gauss elimination do not change the solutions of a system of linear
equations. Here, we want to understand this in general, by painting a somewhat more
abstract but at the same time simpler picture that also applies if A is not a square matrix.
This will allow us to use the results of this section also for Gauss-Jordan elimination, the
algorithm that we present in Section 3.3 to solve all systems Ax = b.
The key lemma is the following. We refer to this and the lemmas after it as invariance
results; an invariance result shows that some property of an object does not change (“is
invariant”) under a given transformation.
Lemma 3.2 (Invariance of solutions). Let A be an m × n matrix and M an invertible m × m
matrix. Then the two systems Ax = b and M Ax = M b have the same solutions x.
This is exactly the kind of statement that we have made in the examples for the original system and the transformed system, based on elimination and permutation matrices
being invertible. The following proof simply repeats the argument for any invertible matrix. We refer back to Section 2.4.2 for the theory of invertible and inverse matrices.
Proof of Lemma 3.2. Whenever we have an x such that Ax = b, we can multiply both sides
from the left with M and also get M Ax = M b. Vice versa, whenever we have an x such
104

that M Ax = M b, we can multiply both sides from the left with M −1 (which cancels M
due to M −1 M = I) and also get Ax = b.
Now, this has a number of interesting consequences about how the two matrices A
and M A are similar to each other in many important aspects. Here are three of them.
Lemma 3.3 (Invariance of the nullspace). Let A be an m×n matrix and M an invertible m×m
matrix. Then A and M A have the same nullspace, N(A) = N(M A).
Proof. We apply the previous Lemma 3.2 with b = 0 (and hence M b = 0 as well). So
Ax = 0 and M Ax = 0 have the same solutions. Since the nullspace of a matrix A (Definition 2.17) is nothing else but the set of solutions of Ax = 0, the statement follows.
Lemma 3.4 (Invariance of linear independence). Let A be an m×n matrix and M an invertible
m × m matrix. Then the following is true: A has linearly independent columns if and only if M A
has linearly independent columns.
Proof. Observation 2.5 (ii) can be reworded as follows: the columns of a matrix are linearly
independent if and only if its nullspace only contains the zero vector. Since both A and
M A have the same nullspace by Lemma 3.3, the statement follows.
Lemma 3.5 (Invariance of the row space). Let A be an m × n matrix and M an invertible
m × m matrix. Then A and M A have the same row space, R(A) = R(M A).
Proof. The row space is the column space of the transpose (Definition 2.14), so we need to
prove that C(A⊤ ) = C((M A)⊤ ). Since (M A)⊤ = A⊤ M ⊤ by Lemma 2.40, the statement to
prove becomes C(A⊤ ) = C(A⊤ M ⊤ ). To simplify notation, define B := AT and N := M ⊤
(N is also invertible by Lemma 2.60). With this, our target equation is C(B) = C(BN ). In
words, if we multiply an n × m matrix B with an invertible m × m matrix N from the right,
the column space stays the same. To see that the two column spaces indeed contain the
same vectors v, we argue as follows:
v ∈ C(B)
⇕ (Def. 2.9)
v = Bx for some x ∈ Rm
x := N y →
⇑
⇓
← y := N −1 x, so x = N y
v = BN y for some y ∈ Rm
⇕ (Def. 2.9)
v ∈ C(BN ).

Is it also true that A and M A have the same column space? No, multiplying with an
invertible matrix from the left will in general change the column space. To see this, we look
at our favorite rank-1 matrix


2 4
A=
,
3 6
105

and we choose the invertible (permutation) matrix


0 1
M=
.
1 0
Then we have



3 6
MA =
,
2 4

since M corresponds to a row exchange. Because the column space of a matrix is the span
of its columns (Definition 2.9), the situation is as in Figure 3.2.

y

y
 
4
6
 
2
3

0

 
3
2
x

0

 
6
4

x





2 4
3 6
Figure 3.2: The column spaces of A =
and M A =
are different.
2 4
3 6
However, the following “column-based” statement is true: A and M A have their independent columns at the same indices and therefore also have the same rank (see Definition 2.10).
Lemma 3.6 (Invariance of independent column indices and rank). Let A be an m×n matrix,
M an invertible m × m matrix. Then the following is true for all j ∈ [n]: column j of A is
independent if and only if column j of M A is independent. In particular, A and M A have the
same number of independent columns and therefore also the same rank.
Proof. Let v1 , v2 , . . . , vn be the columns of A. By Definition 2.36 of matrix multiplication,
the columns of M A are M v1 , M v2 , . . . , M vn . Let B be the submatrix of A containing the
first j − 1 columns v1 , v2 , . . . , vj−1 ; then, M B is the submatrix of M A containing the first
j − 1 columns M v1 , M v2 , . . . , M vj−1 . By Definition 2.10, the j-th column vj of A is independent exactly if the system Bx = vj has no solution. Similarly, the j-th column M vj
of M A is independent exactly if the system M Bx = M vj has no solution. By Lemma 3.2,
both systems have the same solutions, so column j is either independent in both matrices,
or dependent in both matrices.

106

3.2.4

Success and failure

In Section 3.2.2, we have seen that Gauss elimination may have to give up on solving a
system of m linear equations in m variables. Here we will get to the bottom of this. We
start by describing the row operation matrices in Algorithm 2 for general m.
The elimination matrix corresponding to the row subtraction in line 13 is the m × m
matrix


⧹

 ←j
1




⧹
Eij =


 −λ
 ←i
1
⧹
↑
↑
j
i
Here ⧹ stands for a contiguous sequence of diagonal 1’s, and all omitted entries are 0.
Hence, Eij is the identity matrix with an extra −λ in row i and column j. You can convince
yourself (using the rules of matrix-vector multiplication; see Section 2.1.1) that this is
indeed the matrix of the row operation “subtract λ · (row j) from (row i)”. Applying it
to all columns of U and to c, the current system U x = c gets transformed into the next
system Eij U x = Eij c. Moreover, Eij is invertible: its inverse Eij−1 has a λ (row addition to
undo the row subtraction!) where Eij has a −λ.
Similarly, you can check that the permutation matrix corresponding to the row operation “exchange (row j) and (row k)” in line 6 of Algorithm 2 is


⧹

 ←j
0
1




⧹
Pjk =



 ←k
1
0
⧹
↑
↑
j
k
Applying this to both U and c yields the next system Pjk U = Pjk c. The matrix Pjk is also
−1
invertible, with Pjk
= Pjk (exchange the two rows again to undo the row exchange!).
To summarize: every row operation in Gauss elimination transforms the current system U x = c into a next system M U x = M c where M is an (invertible) elimination or a
permutation matrix. Based on the invariance results from the previous Section 3.2.3, we
can now understand exactly when Gauss elimination succeeds.
Theorem 3.7. Let Ax = b be a system of m linear equations in m variables (so A is an m × m
matrix). The following two statements are equivalent.
(i) Gauss elimination as in Algorithm 2 succeeds.
(ii) The columns of A are linearly independent.
107

We will prove (i)⇒(ii) “normally”, but (ii)⇒(i) (“if (ii), then (i)”) is easier to prove by
contraposition. This proves the logically equivalent implication “if not (i), then not (ii)”.
You know this from natural language. Consider the sentence “If it rains, then the street
is wet.” Another (logically equivalent) way of saying this is “If the street is not wet, then
it does not rain”. The symbol for not (logical negation) is ¬, so the contraposition can be
written as ¬(i)⇒ ¬(ii).
Proof. (i) ⇒(ii): If Gauss elimination succeeds, it produces an upper triangular matrix U
with all diagonal entries nonzero. Such a matrix U has linearly independent columns: no
column is a linear combination of the previous ones, due to the nonzero diagonal elements
(and zeros to the left of them). Recall Corollary 1.23 (iii) for this alternative definition of
linear independence. Since linear independence is invariant under row operations by
Lemma 3.4, the original matrix A also has linearly independent columns.
¬(i)⇒ ¬(ii): If Gauss elimination fails in some column j, we get stuck at an intermediate matrix U with zeros in rows j, j + 1, . . . , m of column j. But in all previous columns,
elimination succeeded, and the diagonal entries are nonzero. Hence, U looks like this (the
white area symbolizes entries that are guaranteed to be 0):

U′

v

U=
←j

↑
j
From this, we will argue that the j-th column of U is a linear combination of the
previous columns, meaning that the columns of U are linearly dependent according to
Lemma 1.22 (iii). But then also the original matrix A has linearly dependent columns,
again by the invariance Lemma 3.4.
For the argument, we first observe that the vector v formed by the first j − 1 entries of
column j in U is a linear combination of the columns of U ′ , the “top-left” (j − 1) × (j − 1)
submatrix of U . To see this, we use that U ′ is upper triangular with all diagonal elements
nonzero, so that back substitution as in Section 3.2.1 gives us a (unique) vector x ∈ Rj−1
that solves the system U ′ x = v. This writes v as a linear combination of the columns of
U ′ . Since there are only zeros below U ′ and v, this linear combination in fact writes the
complete j-th column of U as a linear combination of the first j − 1 columns of U .
108

You may wonder whether this proof also works (or how it is to be interpreted) if Gauss
elimination already fails in the first column (j = 1). In this case, the complete first column
of the original matrix A is 0, and hence, the columns are linearly dependent for obvious
reasons; see also Table 1.3 and the discussion before it.
In summary, we obtain the following result.
Theorem 3.8 (Solving Ax = b with Gauss elimination). Let Ax = b a system of m linear
equations in m variables. If A has linearly independent columns, then Gauss elimination (Algorithm 2) with back substitution (Algorithm 1) computes the unique solution x of the system. If A
has linearly dependent columns, then Gauss elimination fails.

3.2.5

Runtime

Whenever computer scientists see an algorithm, they ask how fast it is. For any given
problem, there are different algorithms, and they may differ in their runtimes. There are
other criteria of algorithmic efficiency, for example memory consumption, but here we
focus solely on runtime. Naturally, we would like to identify the fastest algorithm. But
for this, we must first be able to measure the runtime of an algorithm. Here we follow the
common “lazy computer scientist” way of doing this in terms of big-O notation.
For this, we count the number of “basic” operations that an algorithm performs, depending on the input size. A basic operation is one that on an actual computer runs in
constant time (time not depending on the input size). Examples for basic operations are
additions or multiplications of two numbers, comparisons of two numbers, or updates
of variables. In the end, the whole algorithm is put together from basic operations, so
if we manage to count them, we know that the runtime of the algorithm is at most some
constant times this count, where the constant is the maximum time needed to execute any
basic operation. As this time depends on the computer that we run the algorithm on, we
want to abstract from it. For this, we sweep all constants under the rug and thus even get
away with counting the basic operations only up to a constant factor.
Concretely, suppose we can argue that there is a constant c (we do not care how large
it is) and some function g : N → R such that Gauss elimination as in Algorithm 2 requires
at most c · g(m) basic operations on any system of m equations in m unknowns. Then we
say that the number of basic operations and also the runtime is O(g(m)). There is a small
catch here: It is common to have g(0) = 0, but even for m = 0, the algorithm performs
some basic operations. Since the term c · g(0) = 0 cannot count them (no matter how large
c is), there is a silent agreement that the bound O(g(m)) only applies whenever g(m) > 0,
even though we use it for all m. Usually, the case g(m) = 0 corresponds to “small” values
of m for which we do not really care what happens. We can use a function g that never
outputs 0, but this is usually a bit cumbersome. For example, we want to write O(m3 )
as a shorthand for O(g(m)) with g(m) = m3 . To avoid g(0) = 0, we would have to write
O((m + 1)3 ). As lazy computer scientists, we do not want to do this.
We are now prepared to analyze the runtimes of Gauss elimination and back substitution in big-O notation.
109

Theorem 3.9 (Runtime of Gauss elimination). Let Ax = b be a system of m linear equations
in m variables. In time
O(m3 ),
Gauss elimination (Algorithm 2) returns an equivalent system U x = c (in case of success, U is
upper triangular with nonzero diagonal entries).
If m is large, then m3 is very large, so Gauss elimination is not a practically efficient
algorithm for large systems of linear equations.
Proof. Algorithm 2 performs at most one row subtraction for every matrix entry below the
diagonal. Being generous (and lazy), we can say that there are at most m2 such entries,
because this is the total number of entries of A.
Each row subtraction requires O(m) basic operations. This is a pretty sloppy count,
and with some effort, we could be much more specific, for example by exactly counting
the multiplications, subtractions, and other basic operations that happen in a given row
subtraction. However, this would also require us to really think about the basic operations: what do we count as a basic operation? Do we in the end cover all operations that
happen in the algorithm?
The beauty of the big-O-notation is that we can avoid this kind of (usually exhaustive)
work. Knowing that we need to do at most m2 row subtractions, each one requiring time
O(m), we know that we need time O(m3 ) for all row subtractions.
Now, there are also some other operations, most notably row exchanges that also take
time O(m) each. But as there is at most one row exchange per column, the total time for
row exchanges is only O(m2 ). And O(m3 ) + O(m2 ) is still O(m3 ).
Finally, there is some “overhead” such as updating the loop variables j and i. But all
this only adds up to O(m2 ) as well, so it is again covered by O(m3 ).
Theorem 3.10 (Runtime of Back substitution). Let U x = b be a system of m linear equations
in m variables, where U is upper triangular with nonzero diagonal entries. In time
O(m2 ),
back substitution (Algorithm 1) returns the unique solution x.
Proof. The main work is the evaluation of a sum (of at most m terms), for each of the m
rows. This leads to a total of O(m2 ) basic operations (including overhead).
We see that as m grows, the runtime O(m2 ) of back substitution becomes negligible
compared to O(m3 ) for Gauss elimination.

3.2.6

Computing inverse matrices

Now we want to apply Gauss elimination to compute the inverse A−1 of a square matrix
A (or report that A is singular; see Definition 2.55). We recall from Section 3.1.2 that the
110

j-th column xj of A−1 is the solution of the system Ax = ej where ej is the j-th standard
unit vector. If A is invertible (the columns are linearly independent), running m Gauss
eliminations with back substitution will therefore give us all the m columns x1 , x2 , . . . , xm
of A−1 . If A is singular (the columns are linearly dependent), already the first of those runs
will fail and thus provide us with the information that A is singular. See Theorem 3.8 for
these two cases. In both cases, the total runtime is at most O(m4 ), since we have at most
m runs, each of which needs time O(m3 ) by Theorems 3.9 and 3.10.
We can improve on this by using the fact that all the m systems Ax = ej have the same
coefficient matrix A. In this case, all our m runs of Gauss eliminations will transform A
into the same upper triangular matrix U , because the transformation steps (row operations) only depend on the entries of A, not on the right-hand side. The runs only differ in
the right-hand sides on which they however all perform the same row operations.
We can therefore do all the work with a single run that performs each row operation
on all the m right-hand sides. The shared coefficient matrix A is transformed only once.
There is a very elegant way to derive such a version from “normal” Gauss elimination
as in Algorithm 2. Instead of providing one right-hand side b ∈ Rm , we provide an
m × m matrix B whose columns are the m right-hand sides for which we want to solve
the system with the shared coefficient matrix A. Then, performing a row operation on
all the right-hand sides can be done by performing it on the matrix of right-hand sides.
Algorithm 3 presents the pseudocode for this. It is virtually a copy of Algorithm 2.
Algorithm 3 Gauss elimination with m right-hand sides:
Returns a triple (U, C, result) such that for all j ∈ [m], the two systems Ax = bj and
U x = cj have the same solutions, where bj is the j-th column of B, and cj the j-th column
of C. If result = ”succeeded”, U is upper triangular with all diagonal elements nonzero.
1: function G AUSS ELIMINATION(A, B)
▷ A ∈ Rm×m , B ∈ Rm×m
2:
U ← A, C ← B
3:
for j = 1, 2, . . . , m do
▷ eliminate in column j
4:
if ujj = 0 then
▷ zero pivot
5:
if there is some k > j such that ukj ̸= 0 then
6:
exchange (row j) and (row k) (in both U and C)
▷ row operation
7:
else
▷ give up
8:
return (U, C, ”failed”)
9:
end if
10:
end if
▷ now, ujj ̸= 0
11:
for i = j + 1, j + 2, . . . , m do
▷ make uij = 0
uij
▷ we want uij − λujj = 0
12:
λ ← ujj
13:
subtract λ · (row j) from (row i) (in both U and C)
▷ row operation
14:
end for
15:
end for
▷ now, U is upper triangular, with all diagonal elements nonzero
16:
return (U, C, ”succeeded”)
17: end function
111

Theorem 3.11 (Runtime of Gauss elimination with m right-hand sides). Let Ax = bj , j ∈
[m], be m systems of m linear equations in m variables, where the bj ’s are the columns of the input
matrix B. In time
O(m3 ),
Gauss elimination (Algorithm 3) returns equivalent systems U x = cj , j ∈ [m], where the cj ’s
are the columns of the output matrix C (in case of success, U is upper triangular with nonzero
diagonal entries).
Proof. The arguments are the same as in the proof of Theorem 3.9. The operations that
“dominate” the runtime of Algorithm 3 are again the at most m2 row subtractions. The
only difference is that now, a single row subtraction updates 2m entries instead of m + 1,
but in big-O notation, this difference does not matter.
Algorithm 4 gives the full pseudocode to test for invertibility of a square matrix A,
and to compute A−1 if A is indeed invertible. The matrix of right-hand sides that we have
to provide as input for Algorithm 3 is the m × m matrix with columns e1 , e2 , . . . , em : the
identity matrix.
Algorithm 4 Inverse matrix computation:
returns a pair (M, result) such that result = ”succeeded” if and only if A is invertible. If
result = ”succeeded”, then M = A−1 .
1: function I NVERSE(A)
2:
(U, C, result) ← G AUSS ELIMINATION(A, I)
▷ A, I ∈ Rm×m
3:
if result = ”failed” then
4:
return (A, ”singular”)
5:
else
6:
for j = 1, 2, . . . , m do
▷ solve U x = cj which is equivalent to Ax = ej
7:
cj ← j-th column of C
8:
xj ← B ACK SUBSTITUTION(U, cj )
9:
end for

|
|
|
10:
A−1 ← x1 x2 · · · xm 
|
|
|
−1
11:
return (A , ”succeeded”)
12:
end if
13: end function
The runtime directly follows from Theorem 3.11 (runtime of Gauss elimination with
m right-hand sides) and Theorem 3.10 (runtime of back substitution).
Theorem 3.12 (Runtime of Inverse computation). Let A be m × m matrix. In time
O(m3 ),
Algorithm 4 either reports that A is singular, or computes the inverse matrix A−1 .
112

3.2.7

Solving Ax = b from A−1 , LU, and LUP

Suppose that A is an m × m matrix with linearly independent columns and b ∈ Rm , so
that Gauss elimination succeeds (Theorem 3.7) and solves Ax = b in total time O(m3 ),
see Theorems 3.9 and 3.10. Solving n such systems therefore takes time O(m3 n). But if all
these n systems share the same coefficient matrix A, we can do much better.
For this, we compute A−1 with Algorithm 4 once in the beginning, in time O(m3 ).
After this, Ax = b can be solved much faster for any given right-hand side b. Indeed, by
multiplying the equation Ax = b with A−1 from the left, A cancels, and we see that the
solution is x = A−1 b. To get this solution requires only one matrix-vector multiplication
of A−1 with b which takes time O(m2 ); this is easiest to see from the definition of matrixvector multiplication with A in table notation (Observation 2.6).
Hence, solving n systems with the same A in this way takes time O(m3 + m2 n). For
large n, this saves a factor of m over the “naive method”.
For practical purposes, A−1 is not necessarily the best “device” to solve Ax = b quickly
for many b’s. On the one hand, this device is not even applicable if A is singular (but that
is ok; we will anyway get to this case only in the next Section 3.3). On the other hand, and
this is the more important issue, the entries of A are usually floating-point numbers on an
actual computer, and these can only approximate real numbers. In computing A−1 with
floating-point numbers, roundoff errors happen, and in the worst case, these are so severe
that the matrix A−1 being computed has not much to do with the true inverse anymore.
The field of numerical mathematics is concerned with methods to control such roundoff
errors. For example, there are devices, known as LU decomposition and LUP decomposition, that are typically more “roundoff-error-friendly” than A−1 , and that can still be used
to solve Ax = b quickly for any given b. The LU and LUP decompositions are also a
bit faster to compute than A−1 , but not in the big-O sense: the standard algorithms for
computing these decompositions are also based on Gauss elimination and do not beat
the O(m3 ) time bound that we have for computing A−1 via Theorem 3.12. But they have
smaller constants “behind” the big-O and are therefore more efficient in practice.
The way you can conceptually think about LU and LUP decompositions is that they
“remember” the O(m2 ) row operations performed during Gauss elimination in Algorithm 2 for some first right-hand side b: think of a long list of the form “subtract 2·(row 1)
from (row 2), subtract 3·(row 1) from (row 3), exchange (row 2) and (row 3),. . . ”. For every
new right-hand side b′ , they simply replay all these row operations on b′ . This simulates
what Algorithm 2 would have done with the same matrix A but right-hand side b′ . That
way, the transformed right-hand side c′ is obtained in time O(m2 ), and back substitution
can then be used to solve U x = c′ (which is equivalent to Ax = b′ ) in time O(m2 ) as
well. LU and LUP decompositions avoid the above explicit list of row operations, and
instead use matrices as their “memory”. LU decomposition is applicable if there are no
row exchanges, while LUP decomposition always works.
We will not further go into LU or LUP decompositions here. Thinking of them as
remembering all the row operations as well as the final matrix U is good enough to have
a conceptual understanding of them.
113

3.3

Gauss-Jordan elimination

In this section, we present an algorithm for solving any system Ax = b (or detecting
that there is no solution). The algorithm is very similar in spirit to Gauss elimination. The most important theoretical contribution of Gauss-Jordan elimination is a
reduction of A to a unique standard form from which we can easily read off many
properties of A that are difficult to see directly. On top of providing an efficient way
of solving Ax = b, this standard form also yields the CR decomposition of A.
In Section 3.2, we have seen how to solve systems Ax = b where A is a square matrix with linearly independent columns (equivalently, an invertible matrix; see Definition 2.55). This is a very nice case in the sense that there is always a unique solution
x = A−1 b that can be computed with Gauss elimination and back substitution (Sections 3.2.2 and 3.2.1), or directly from the inverse A−1 (once we have computed it), using
matrix-vector multiplication (Section 3.2.7).
We will now see an algorithm for the general case (A may be non-square and/or may
have linearly dependent columns). This is a simple extension of Gauss elimination, and
the approach is the same: through row operations, we transform the system Ax = b into
a system Rx = c with the same solutions, and with R being “nice” so that the system
Rx = c is easy to solve. Here, “nice” means that R is in reduced row echelon form. To
achieve this form, we will slightly enlarge our arsenal of row operations, by adding row
divisions (divide a row by some λ ̸= 0).

3.3.1

Reduced row echelon form

In Gauss elimination, we are trying to transform a square matrix A into a certain standard
form (upper triangular matrix with nonzero diagonal entries). This fails if A has linearly
dependent columns (see Theorem 3.7). Now, we define another standard form (reduced
row echelon form) into which we can transform every matrix, even if it has linearly dependent columns, or is not a square matrix. See Figure 3.3 for an example.

2 3

6

8

1 0
1

0
0
1

0
0
0
1

Figure 3.3: A 6 × 10 matrix in reduced row echelon form RREF(2, 3, 6, 8). White entries
are 0, unlabeled gray entries may or may not be 0. The gray area is the “staircase”. The
numbers above the matrix are the indices of the “downward step” columns.

114

A matrix in reduced row echelon form (RREF) looks a bit like an upper triangular matrix. The gray area in Figure 3.3 has the form of a “staircase”, and whenever the staircase
makes a “downward step” in a column, this column is a standard unit vector, with the 1 in
the row where the downward step happens. In Figure 3.3, there are four such downward
steps, in columns 2, 3, 6, and 8. Only downward steps of one are allowed, meaning that
after a total of r downward steps, exactly the first r rows have some gray entries (r = 4 in
Figure 3.3).
Given this, the downward steps determine the shape of the staircase, and we will
sometimes add the columns of these steps to the definition of RREF; for example, the
matrix in Figure 3.3 is actually in RREF(2, 3, 6, 8), and from this information, you can already reconstruct the picture if you know the number of rows and columns of the matrix.
As a small check of your understanding at this point, you can try to figure out how an
m × m matrix in RREF(1, 2, . . . , m) looks like. Now for the actual definition.
n
Definition 3.13 (Reduced row echelon form). Let R = [rij ]m
i=1,j=1 be an m × n matrix. R is in
reduced row echelon form (RREF) if there is some natural number r ≤ m and column indices
1 ≤ j1 < j2 < · · · < jr ≤ n (the indices of the “downward step” columns) such that the following
two conditions hold.

(i) For every i ∈ [r], column ji of R is the standard unit vector ei .
(ii) All entries rij “below the staircase” are 0. Formally, an entry rij is below the staircase if
(a) i > r (the entry is below row r), or
(b) i ≤ r and j < ji (the entry is in the part of row i to the left of column ji ).
If we want to describe the shape of R precisely, we say that R is in RREF(j1 , j2 , . . . , jr ).
We can convince ourselves that the matrix in Figure 3.3 is indeed in RREF(2, 3, 6, 8)
according to this definition. Condition (i) is clear with r = 4, and to check condition
(ii), we observe that all entries below row 4 are white (zero); these are the entries below
the staircase according to condition (a). For the ones below the staircase according to
condition (b), we have to check that whenever an entry in some row i ≤ r is to the left of
column ji (where the downward step happens in that row), it is white. This also holds.
For example, these are the entries that satisfy the “below the staircase” condition (b) for
i = 3 and should therefore be white:

i=3

j1 j2

j3

j4

1 0
1

0
0
1

0
0
0
1

j < j3

115

The m × m identity matrix I is in RREF(1, 2, . . . , m). As a consequence of condition (i),
this is actually the only m × m matrix in RREF(1, 2, . . . , m). The m × n zero matrix is in
RREF(), meaning that r = 0 (there are no downward steps at all).
Matrices in RREF are nice in the sense that we can read off many properties easily.
Here is a first example.
Lemma 3.14. A matrix R in RREF(j1 , j2 , . . . , jr ) has independent columns j1 , j2 , . . . , jr and
therefore rank r.
Proof. Recall that the rank of a matrix is the number of independent columns, the ones
that are not linear combinations of previous columns (Definition 2.10). Now, if a matrix
R is in RREF(j1 , j2 , . . . , jr ), the independent columns are precisely the ones with indices
j1 , j2 , . . . , jr . Indeed, in each of these columns, R makes a downward step and has a 1 in a
row where all previous columns have zeros. Such a downward step column is therefore
not a linear combination of the previous columns. But any other column v is a linear
combination of the previous (downward step) columns. There is no harm in skipping the
easy but a bit technical formal proof in exchange for this “proof by example”:

2 3

6

8

1 0
1

0 v1 0
0 v2 0
1 v3 0
1

e1 e2

e3 v e4


v1
 v2 


 v3 

v=
 0  = v1 e1 + v2 e2 + v3 e3


 0 
0


However, in order to understand how a formal proof uses Definition 3.13, let us do
it anyway. Suppose v ∈ Rm is the j-th column of R (where j is not one of j1 , j2 , . . . , jr ).
There are two cases.
The first case is j > jr , so v is to the right of all downward step columns. For all i > r,
entry vi is below the staircase by Definition 3.13 (a) and hence equal to 0. In this case, v is
an “obvious” linear combination of the previous (meaning all) downward step columns,
since those are the first r standard unit vectors by Definition 3.13 (i):
v=

r
X

vi ei

(using vi = 0 for i > r).

i=1

The second case is j < jr ; let s be the smallest index such that j < js (in the picture
above, s = 4 and js = 8). Thus, j < js , js+1 , . . . , jr . By Definition 3.13 (b), entries vi , i ≥ s,
are below the staircase and thus 0. Then v is an obvious linear combination of the previous
downward step columns j1 , j2 , . . . , js−1 < j, the first s − 1 standard unit vectors:
v=

s−1
X

vi ei

(using vi = 0 for i ≥ s).

i=1

116

3.3.2

Direct solution

It is easy to solve a system Rx = c (or decide that there is no solution) whenever R is in
RREF. In fact, this is even easier than doing back substitution for a system U x = c with
U an upper triangular matrix (see Section 3.2.1). This is why we call the method “direct
solution”. The key are the standard unit vectors in the downward step columns.
Suppose that R is an m × n matrix in RREF(j1 , j2 , . . . , jr ) There are two cases: if ci ̸= 0
for some i > r, then Rx = c has no solution, because already the i-th equation r⊤
i x = ci
⊤
has no solution. Here, ri denotes the i-th row of R which is below the staircase and
hence equal to 0⊤ by Definition 3.13 (a). Hence, r⊤
i x = 0 ̸= ci , no matter what x is. For
this argument, we made use of the rowwise interpretation of a system of linear equations,
see the discussion after Definition 3.1.
The other case is that ci = 0 for all i > r. In this case, there is a canonical (standard)
solution: we define x ∈ Rn by

ci , if j = ji for some i
xj =
(3.3)
0, otherwise
This has the effect that in the linear combination Rx (see Definition 2.4 of matrix-vector
multiplication), only the r downward step columns e1 , e2 , . . . , er have (potentially) nonzero scalars c1 , c2 , . . . , cr , and these are exactly the right ones such that Rx = c. Here is an
illustration.

j1 j2

j3

j4

1 0
1

0
0
1

0
0
0
1

R

0
c1
c2
c1
c2
0
0 = c3
c3
c4
0
0 ← if ̸= 0 here, no solution
c4
0 ←
0
c
0
x

Formally, since column ji of R is ei by Definition 3.13 (i), the vector x as in (3.3) indeed
gives
r
X
Rx =
ci ei = c (using ci = 0 for i > r).
i=1

We note that the vector x in (3.3) is typically not the only solution of Rx = c. Our
running example is a 6 × 10 matrix whose first column is 0. Hence, we could for example
change the canonical solution by setting x1 to any value (instead of 0), and we would still
have a solution. Section 4.4 discusses how to find all solutions.
Algorithm 5 gives the pseudocode for the direct solution method. Its operations do
not depend on R, only on where the downward step columns are. The runtime bound
immediately follows from inspecting the pseudocode.
117

Algorithm 5 Direct solution:
Returns a pair (x, result) such that Rx = c if result = “solution”. If result = “no solution”,
there is no solution. The matrix R must be in RREF(j1 , j2 , . . . , jr ).
1: function D IRECT SOLUTION(R, j1 , j2 , . . . , jr , c)
▷ R ∈ Rm×n , c ∈ Rm
n
2:
x←0∈R
3:
if ci ̸= 0 for some i > r then
4:
return (x, “no solution”)
5:
end if
6:
for i = 1, 2, . . . , r do
7:
x j i ← ci
8:
end for
9:
return (x, “solution”)
10: end function
Theorem 3.15 (Runtime of Direct solution). Let Rx = c be a system of m linear equations in
n variables, where R is in RREF(j1 , j2 , . . . , jr ). In time
O(m + n),
direct solution (Algorithm 5) returns a solution x or reports that there is no solution.

3.3.3

Elimination

Here, we proceed as in Gauss elimination (Section 3.2.2) and use row operations to transform any system Ax = b into a system Rx = c with the same solutions, where R is in
RREF. After this, we can use direct solution (see previous section) to either report that
the system is unsolvable, or to compute the canonical solution.
In the following example, we only show how elimination transforms A into R. The
pseudocode in Algorithm 6 below will also take care of the right-hand side(s).
As in Gauss elimination, we proceed column by column. The nonzero pivots that
we find will determine the “downward step” columns j1 , j2 , . . . of the resulting matrix in
RREF. Our example matrix is the 3 × 5 matrix


2 4 2 2 −2
1 .
A = 6 12 6 7
(3.4)
4 8 2 2
6
Already in row 1 of column 1, we have a nonzero pivot that we can use for a downward step in this column:


2 4
2
2 −2
 6 12
6
7
1
4 8
2
2
6

118

In contrast to Gauss elimination, we first apply a row division in order to make the pivot
equal to 1. This is because the first downward step column needs to be transformed into
the first standard unit vector e1 , according to the requirements of RREF (Definition 3.13).
Only after that, we perform row subtractions to eliminate the nonzero entries in column 1.
As a bonus, this requires no further divisions:


2 4
2
2 −2
 6 12
6
7
1
4 8
2
2
6
divide (row 1) by 2:
↓


1 2
1
1 −1
 6 12
6
7
1
4 8
2
2
6
subtract 6·(row 1) from (row 2): 
↓

1 2
1
1 −1
0 0
0
1
7
4 8
2
2
6
subtract 4·(row 1) from (row 3): 
↓

1 2
1
1 −1
0 0
0
1
7
0 0 −2 −2 10
We have made our first downward step and move on to column 2 where we attempt to
make the next downward step in row 2:


1 2
1
1 −1
0 0
0
1
7
0 0 −2 −2 10
But we have a zero pivot, and no row exchange can bring a nonzero entry from further
down into the pivot position. In Gauss elimination, we called this “the ugly case” and
simply gave up. But here, this is an unusually good case. There is simply no downward
step to be made in column 2, and we can directly move on to column 3. In this column, it
is possible to make a row exchange to get a nonzero pivot in row 2:


1 2
1
1 −1
0 0
0
1
7
0 0 −2 −2 10
exchange (row 2) and (row 3): 
↓
1 2
1
1 −1
0 0 −2 −2 10
0 0
0
1
7
divide (row 2) by −2:

↓
1 2
1
1 −1
0 0
1
1 −5
0 0
0
1
7
119

So we make the second downward step in column 3, but in order to transform this column
into the second standard unit vector e2 , we also need to eliminate above the pivot:


1 2
1
1 −1
0 0
1
1 −5
0 0
0
1
7
subtract 1·(row 2) from (row 1): 
↓
1 2
0
0
4
0 0
1
1 −5
0 0
0
1
7
No elimination below the pivot is necessary here, so our downward step is done, and we
move to column 4. We already have a pivot of 1 in the third row, so we need no row
exchange and no row division. All that is left to do for this third downward step is one
row subtraction above the pivot to produce e3 :


1 2
0
0
4
0 0
1
1 −5
0 0
0
1
7
subtract 1·(row 3) from (row 2): 
↓
1 2
0
0
4
1
0 −12
R = 0 0
0 0
0
1
7
Further downward steps are neither possible nor necessary, so we can stop even before having looked at the last column. Indeed, the matrix R that we have now is in
RREF(1, 3, 4), without any zero rows at the end:


1 2
0 0
0 0

0
1
0


0
4
0 −12
1
7

(3.5)

But in general, the result of Gauss-Jordan elimination can be a matrix R with trailing
zero rows. As a simple example, think of the same starting matrix A as in (3.4), with a
zero row appended in the end. This zero row will then survive until the final matrix R.
Algorithm 6 presents the full Gauss Jordan elimination algorithm. As in Algorithm 3
for Gauss elimination, we write down a variant that transforms m right-hand sides at
the same time (collected in a matrix B). There is a variable r whose value at any time
corresponds to the number of downward steps already made.
There are also two special commands, break and continue. The break command
leaves its surrounding loop and jumps to the command directly after the loop. We use
this in line 5 when have already made the maximum of m downward steps and are done.
The continue command skips everything that happens after it in the surrounding loop,
but does not leave the loop; so we immediately jump to the beginning of the next loop
120

repetition. We use this in line 12 to handle the (formerly ugly, but now unusually good)
case of a zero pivot that cannot be fixed by row exchanges.
Algorithm 6 Gauss Jordan elimination with m right-hand sides:
Returns a sequence (R, j1 , j2 , . . . , jr , C) such that R is in RREF(j1 , j2 , . . . , jr ), and for all
j ∈ [m], the two systems Ax = bj and Rx = cj have the same solutions, where bj is the
j-th column of B, and cj the j-th column of C.
1: function G AUSS -J ORDAN ELIMINATION(A, B)
▷ A ∈ Rm×n , B ∈ Rm×m
2:
R ← A, C ← B, r ← 0
▷ r: number of downward steps so far
3:
for j = 1, 2, . . . , n do
▷ eliminate in column j
4:
if r = m then
▷ no further downward steps possible, done!
5:
break
▷ . . . loop execution and go to line 21
6:
end if
7:
s←r+1
▷ row of (potential) next downward step
8:
if usj = 0 then
▷ zero pivot
9:
if there is some k > s such that ukj ̸= 0 then
10:
exchange (row s) and (row k) (in both R and C)
▷ row operation
11:
else
▷ no downward step in column j
12:
continue
▷ . . . in line 3 with the next column
13:
end if
14:
end if
▷ now usj ̸= 0
15:
divide (row s) by usj (in both R and C)
▷ row operation
16:
for i = 1, 2, . . . , s − 1 and i = s + 1, s + 2, . . . , m do
▷ make uij = 0
17:
subtract uij · (row s) from (row i) (in both R and C)
▷ row operation
18:
end for
▷ now, the j-th column of R equals es
19:
r ← r + 1, jr ← j
▷ next downward step was made in column j
20:
end for
21:
return (R, j1 , j2 , . . . , jr , C)
▷ R is in RREF(j1 , j2 , . . . , jr )
22: end function
Theorem 3.16 (Runtime of Gauss-Jordan elimination with m right-hand sides). Let Ax =
bj , j ∈ [m], be m systems of m linear equations in n variables, where the bj ’s are the columns of
the input matrix B. In time
O(m2 (m + n)),
Gauss-Jordan elimination (Algorithm 6) returns equivalent systems Rx = cj , j ∈ [m], where the
cj ’s are the columns of the output matrix C, and R is in RREF(j1 , j2 , . . . , jr ).
Proof. As in Gauss elimination, the runtime is dominated by the row operations. A row
operation now takes time O(m + n), since a row has n entries from R and m entries from
C. The number of row operations is O(m2 ), because there are at most m downward steps,
and in each of them, all m rows are being updated through row operations. The resulting
bound of O(m2 (m + n)) also covers all other operations.
121

3.3.4

Standard form and CR decomposition

The theoretical implications of Algorithm 6 are far-reaching. As an immediate result, we
get that every matrix A can be transformed into reduced row echelon form, our standard
form according to Definition 3.13. The following theorem says what this precisely means.
Theorem 3.17 (Output of Gauss-Jordan elimination). Let A be an m × n matrix, and let
(R, j1 , j2 , . . . , jr , M ) be the output of Algorithm 6 with input (A, I), where I is the m×m identity
matrix. Then M is invertible, R = M A, and R is in RREF(j1 , j2 , . . . , jr ).
Proof. Starting from the two matrices A and I, Algorithm 6 is repeatedly applying row
operations to both. These operations are row subtractions and row exchanges as also
used by Gauss elimination, as well as row divisions. In Section 3.2.3, we have seen that
such row operations correspond to multiplication with an invertible matrix from the left.
This also applies to row divisions as the new kind of row operations that Algorithm 6 is
using (you are invited to write down the matrix for dividing row s by scalar λ ̸= 0).
Therefore, the final right-hand side matrix C = M (obtained after some number ℓ of
row operations, starting from B = I) is M = Mℓ Mℓ−1 . . . M1 I = Mℓ Mℓ−1 . . . M1 , where
Mi is the (invertible) matrix of the i-th row operation. Since the product of invertible
matrices is invertible (see Lemma 2.59 and the discussion after it), M is invertible. The
final matrix R in RREF(j1 , j2 , . . . , jr ) is obtained from A via the same row operations, so
we have R = Mℓ Mℓ−1 . . . M1 A = M A.
In this proof, we have silently assumed that Algorithm 6 is correct in the sense that it
indeed produces a final matrix R in RREF. A formal proof of this is possible (by induction
on n, the number of columns of A), but this would not provide greater insights than our
informal explanation of Gauss-Jordan elimination starting on page 118.
In Algorithm 6, we have some choices: in a row exchange, there can be several candidate rows k, and we are free to pick any of them for the exchange. Depending on these
choices, the same input (A, I) could lead to different outputs. But maybe surprisingly, the
resulting matrix R will always be the same.
Theorem 3.18 (Uniqueness of RREF, and relation to the CR decomposition). Let A be an
m×n matrix. There is a unique m×n matrix R (the one resulting from Gauss-Jordan elimination
on A according to Theorem 3.17), with the following two properties.
(i) R = M A for some invertible m × m matrix M .
(ii) R is in RREF.
More precisely, R is in RREF(j1 , j2 , . . . , jr ), where j1 , j2 , . . . , jr are the indices of the independent
columns in A, and


′
R

 |{z}
r×n
,
R=


0
|{z}
(m−r)×n

′

′

with R the unique matrix such that A = CR in Theorem 2.46 (CR decomposition).
122

It is therefore justified to call the matrix R in the theorem the standard form of A. Moreover, from R we get the rank r of A as well as the CR decomposition A = CR′ , where
C is the submatrix of A containing columns j1 , j2 , . . . , jr , and R′ is the submatrix of R
containing the first r rows.
Let us illustrate this with an example. In Section 2.3.5, we have considered the 3 × 4
matrix


1 2 0 3
A = 2 4 1 4
3 6 2 5
and manually computed its CR decomposition



1 0 
1
2
0
3
A = 2 1
.
0 0 1 −2
3 2
To get this via Gauss-Jordan elimination, we first transform A into RREF. This is a
particularly simple case, since no row exchanges and row divisions are necessary, and
also no eliminations above the pivot. All it takes are two row subtractions in the first
column, and one in the third column:


1 2 0
3
4
A = 2 4 1
3 6 2
5
elimination in column 1:

↓
1 2 0
3
0 0 1 −2
0 0 2 −4
elimination in column 3:

↓
1 2 0
3
R = 0 0 1 −2
0 0 0
0
The resulting matrix R is in RREF(1, 3), so r = 2 and Theorem 3.18 tells us that columns
1 and 3 are the independent ones in A, which then leads to submatrices




1 0
1
2
0
3
′
C = 2 1 (of A) and R =
(of R).
0 0 1 −2
3 2
These matrices are precisely the ones that we also found back in Section 2.3.5.
Proof of Theorem 3.18. We know from Theorem 3.17 that Gauss-Jordan elimination on A
produces some matrix R satisfying properties (i) and (ii). Now we we show that every
matrix R satisfying these two properties is the one given under “More precisely. . . ” which
also shows uniqueness. So let R be such a matrix.
123

Because of R = M A with M invertible, invariance Lemma 3.6 applies and shows that
the independent columns j1 , j2 , . . . , jr of A are also the independent columns of R. So we
have R in RREF(j1 , j2 , . . . , jr ), see Lemma 3.14. Hence, R is of the form


′
R

 |{z}
r×n

R=


0
|{z}
(m−r)×n

for some matrix R′ , using Definition 3.13 (a). It remains to show that A = CR′ , where C is
the submatrix containing the independent columns of A, the ones at indices j1 , j2 , . . . , jr .
We actually show
M A = M CR′ ,
(3.6)
and multiplying this from the left with M −1 cancels M , resulting in A = CR′ . For (3.6), the
key is the matrix M C which is the submatrix of M A with columns j1 , j2 , . . . , jr . This may
be clear to you, but if not, here is the argument: if A has columns v1 , v2 , . . . , vn , then C
has columns vj1 , vj2 , . . . , vjr . By Definition 2.36 of matrix multiplication, M A has columns
M v1 , M v2 , . . . , M vn , and M C has columns M vj1 , M vj2 , . . . , M vjr . Hence, M C is indeed
the submatrix of M A with columns j1 , j2 , . . . , jr .
Since M A = R is in RREF(j1 , j2 , . . . , jr ), we know the submatrix M C: it has the standard unit vectors e1 , e2 , . . . , er ∈ Rm as columns, see Definition 3.13 (i). This statement
can alternatively be written in the form


I
|{z}
 r×r 
,
MC = 


0
|{z}
(m−r)×r

|

{z

m×r

}

and hence(3.6) follows via


I
 ′ 
 |{z}
r×r
′
R = 
M CR = 



0
|{z}


(m−r)×r

|

R′
|{z}
r×n

0
|{z}



 = M A.


(m−r)×n

{z
R

}

Here, we use that matrix multiplication is “rows of the left matrix times the right matrix”,
see Observation 2.45; so we can put the product together from the partial products IR′ =
R′ (the first r rows) and 0R′ = 0 (the last m − r rows).

3.3.5

Computing inverse matrices, and solving Ax = b from R and M

Gauss-Jordan elimination provides a direct way of computing inverse matrices. This is
even simpler than doing it via Gauss elimination as in Section 3.2.6.
124

Theorem 3.19 (Computing inverses with Gauss-Jordan elimination). Let A be an m × m
matrix, and let (R, j1 , j2 , . . . , jr , M ) be the output of running Algorithm 6 with input (A, I).
Then A is invertible if and only if R = I, and in this case, A−1 = M .
Proof. Since R = M A by Theorem 3.17, the two matrices A and R have their independent
columns at the same positions (see invariance Lemma 3.6).
A is invertible if and only if A has linearly independent columns (see Definition 2.55),
and this is equivalent to all columns of A being independent, see Definition 2.10 and
Corollary 1.23 (iii). Hence, A is invertible if and only if its independent columns are
at indices 1, 2, . . . , m (all indices). For R, this says that R is in RREF(1, 2, . . . , m) (see
Lemma 3.14), and this is equivalent to R = I by Definition 3.13 (i).
Therefore, if A is invertible, the equation R = M A reads as I = M A, so M = A−1 by
Definition 2.57 of the inverse matrix.
Finally, we come back to the ultimate goal of this chapter: solve an arbitrary system
Ax = b of linear equations (or report that there is no solution). With Gauss-Jordan elimination, we can now do this and even be more efficient if several systems share the same
coefficient matrix.
Theorem 3.20 (Solving Ax = b with Gauss-Jordan elimination). Let A be an m × n matrix, and let (R, j1 , j2 , . . . , jr , M ) be the output of Algorithm 6 with input (A, I). According to
Theorem 3.16, this output is produced in time
O(m2 (m + n)).
Given any right-hand side b ∈ Rm , we can compute a solution x of the system Ax = b (or report
that there is no solution) in time
O(m2 + n).
Proof. By Theorem 3.17, M is invertible, R = M A holds, and R is in RREF(j1 , j2 , . . . , jr ).
Invariance Lemma 3.2 shows that the two systems Ax = b and M Ax = M b have the
same solutions, using that M is invertible. Using R = M A and c := M b, we arrive at the
equivalent system Rx = c. The vector c can be computed in time O(m2 ) using matrixvector multiplication (in table notation according to Observation 2.6, it is easiest to the
runtime). Using that R is in RREF, we can apply direct solution and solve Rx = c (or
report that there is no solution) in time O(m + n), see Theorem 3.15. In total, we need time
O(m2 + m + n) = O(m2 + n).
If m = n (the square case), we see a behavior as in Section 3.2.7 for solving Ax = b
from A−1 (if the inverse exists): After spending O(m3 ) preprocessing time for A, we can
solve Ax = b in time O(m2 ) per right-hand side. This is by a factor of m faster than a
fresh elimination for every right-hand side.
If m < n (the quite typical wide case; see Figure 2.1), preprocessing takes time O(m2 n),
after which we only need time (m2 + n) per right-hand side. For n ≥ m2 (the very wide
case), this is O(n) and therefore even by a factor of m2 faster than a fresh Gauss-Jordan
elimination for every right-hand side.
125

Chapter 4
The Three Fundamental Subspaces
So far, we have said that vectors are elements of some space Rm , and for m = 2, 3, we
have drawn them as arrows in the 2-dimensional plane or in 3-dimensional space. But
this is by far not the full picture. In fact, we have already seen other spaces: the column
space, the row space, and the nullspace of an m × n matrix A. These three fundamental
subspaces1 (of a matrix) are the main characters of this chapter. The column space is a
subspace of Rm , while row space and nullspace are subspaces of Rn . Here, “subspace” is
still an informal notion, used for a space that occupies some part of another space.
But what is a “space”, actually? Officially, Rm is just a set, and the column space of an
m × n matrix is just a subset of it. We call a set a space when we think of it as a “natural
habitat” for vectors. Then the question is: what is it that makes a set a natural habitat
for vectors? We know that vectors have two natural behaviors: they add up (v + w), and
they scale (λv). In a natural habitat, they should be able to do that, but not every set is a
natural habitat in this sense. For example, the surface of the earth is a natural habitat for
humans, but not for vectors (think of vectors “living on earth” as arrows from the center
of the earth to the surface). Adding up such vectors or scaling them unfortunately leads
to vectors hanging in the air, or being buried underground, so natural vector behavior is
not possible on the surface of the earth.
In mathematics, a natural habitat for vectors is formalized as a vector space. This is
an abstract notion that covers all Rm ’s, the three fundamental subspaces of a matrix, and
many other spaces.
A vector space is the right level of abstraction if we want to develop some theory
that does not only work in Rm , but also in other spaces such as the three fundamental
subspaces. The main theoretical terms that we introduce in this chapter are subspace,
basis, dimension, and isomorphism.
In the end, we will apply them to the three fundamental subspaces and also answer
a question that we have left open in Chapter 3: how can we compute all solutions of a
system of linear equations Ax = b?
1
Strang [Str23] has four fundamental subspaces, but we omit the left nullspace here, as we think it is less
relevant.

126

4.1

Vector spaces

In this section, we introduce the abstract concept of a vector space as a “natural habitat” for vectors. We will see that the Rm ’s are (important) examples of vectors spaces,
but we also get to know other examples. The vector space abstraction allows us to
view subspaces such as the column space of a matrix as vector spaces themselves.

4.1.1

Definition and examples

On a high-level, a vector space is a set whose elements (which we call vectors) can add
up (v + w) and scale (λv), without the results leaving the set. If the set in question is some
Rm , we have used this property without giving it any thought. For example, if you add
up two vectors in Rm , you obviously get another vector in Rm (see Definition 1.2). If the
set in question is not some Rm , we cannot take this property for granted (see the “surface
of the earth” example in the chapter introduction). In more exotic spaces, it might not
even be clear what it means to add up two elements.
The following definition of a vector space takes care of this. It says precisely under
which conditions a set can be called a natural habitat for vectors. These conditions are
completely abstract, and on this abstract level, there is only one sensible answer to the
question “What is a vector?” This answer is that a vector is an element of a vector space.
To make things a bit less abstract, we only talk about real vector spaces here. The word
real does not stand for true or proper, but indicates that the scalars are real numbers. Each
Rm is a real vector space, and immediately after the definition, we will see another example of a real vector space. There are also vector spaces where the scalars are other kinds
of numbers (complex numbers are an important case, and so are bits, the elements of the
set {0, 1}), but we will not discuss them here. So we omit the word real, simply say vector
space, but mean real vector space.
Do not be scared by the length of the following definition; in particular, there is no
need to learn the axioms by heart. You can look them up whenever needed.
Definition 4.1 (Vector space). A vector space is a triple (V, +, ·) where V is a set (the vectors),
and
+ : V × V → V is a function (vector addition),
· : R × V → V is a function (scalar multiplication),
satisfying the following axioms of a vector space for all u, v, w ∈ V and all λ, µ ∈ R.
1.
2.
3.
4.
5.
6.
7.
8.

v+w =w+v
u + (v + w) = (u + v) + w
There is a vector 0 such that v + 0 = v for all v
There is a vector −v such that v + (−v) = 0
1·v =v
(λ·µ)v = λ · (µ · v)
λ(v + w) = λv + λw
(λ+µ)v = λv + µv
127

commutativity
associativity
zero vector
negative vector
identity element
compatibility of · and · in R
distributivity over +
distributivity over + in R

Here, we use red color to indicate that there are two different “+”, and also two different “·”. The red ones stand for the normal addition and multiplication of real numbers.
The black ones are for vector addition and scalar multiplication. We will omit the coloring (and even the “·”) in the following, but there is (hopefully) only limited potential for
confusion. After all, if you want to know which “+” or “·” is meant in a given context,
you simply need to check what is on the left side and the right side. We have done this
kind of overloading before when we used the normal addition symbol “+” both for real
numbers and for vectors in Rm .
Now for the actual axioms: with Rm in mind, they seem obvious, given how we have
defined vector addition and scalar multiplication. We therefore get
Observation 4.2. (Rm , +, ·), with “+” as in Definition 1.2 and “·” as in Definition 1.3, is a
vector space.
Previously, we have called this vector space Rm which—in hindsight—is an abuse of
notation. But this is acceptable, since our “+” and “·” are what mathematicians call the
canonical (standard) choices for vector addition and scalar multiplication in Rm , so there
is no strict need to mention them.
But in general, V could be any set, with + and · defined in non-canonical ways, so
we explicitly need to include these functions and also make sure that they behave as
expected, where the expectations come from what happens in Rm . This is what the axioms
are about.
To illustrate the concept, we present a new vector space, the vector space of real polynomials in one variable. We could also consider polynomials in several variables, but for
the vector space example that we want to give here, one variable is enough. As for vector
spaces, we omit the word real, since we do not consider any other polynomials here. You
know polynomials from high school, an example is 2x2 + x + 1. What may be new to you
is that polynomials can be considered as vectors in a vector space.
Definition 4.3 (Polynomial). A polynomial p is a formal sum of the form
p=

m
X

p i xi ,

i=0

for some m ∈ N. Here x is a variable, and the numbers p0 , p1 , . . . , pm ∈ R are the coefficients of
p. The largest i such that pi ̸= 0 is the degree of p. If all pi are 0, we have the zero polynomial
0 = 0 whose degree we define to be −1.
We note that m does not have to be the same for all polynomials, but for every polynomial, we have some m. For example, p = 2x2 + x + 1 is a polynomial of degree 2, and
q = 5x − 2 is a polynomial of degree 1.
A formal sum is one that we cannot simplify (to one number, for example), because
the individual summands are of different types (likes apples and oranges) that cannot be
added up. Here, the different types are the different powers of x: x0 = 1, x, x2 , . . .
128

To turn the set of polynomials into a vector space, we need to define addition of two
polynomials, and multiplication of a polynomial with a scalar. There are canonical ways
of doing this. In the example, we would define
(2x2 + x + 1) + (5x − 2) = 2x2 + 6x − 1,
i.e. we add corresponding powers of x. And scalar multiplication simply scales all coefficients, as in
5(2x2 + x + 1) = 10x2 + 5x + 5.
Theorem
denote the set of polynomials in one variable x. Given two polynomials
Pm 4.4.i Let R[x] P
p = i=0 pi x and q = ni=0 qi xi , we define p + q to be the polynomial
max(m,n)

p+q=

X

(pi + qi )xi ,

i=0

where we set pi = 0 for i > m and qi = 0 for i > n. For a scalar λ ∈ R, we further define λp as
the polynomial
m
X
λp =
(λpi )xi .
i=0

Then (R[x], +, ·) is a vector space.
We omit the proof, since it is quite boring; it boils down to checking the obvious. Here
is a second example: the vector space of m × n matrices. Again, we omit the easy but
boring proof.
Theorem 4.5. Let Rm×n be the set of m × n matrices, with addition A + B and scalar multiplication λA defined in the usual way, see Definition 2.2. Then (Rm×n , +, ·) is a vector space.
Proving the obvious. As boring as this may be, it is still surprising that we only need to
check 8 “obvious” axioms to guarantee proper behavior of a vector space. Indeed, there
are many other things that we expect from knowing how things work in Rm . For example,
we expect that there is only one zero vector in a vector space (V, +, ·), but this does not
appear among the axioms. So we need to prove that it follows from the axioms.
Fact 4.6. Let (V, +, ·) be a vector space. V contains exactly one zero vector (a vector satisfying
axiom 3 of Definition 4.1: v + 0 = v for all v).
Proof. Take two zero vectors 0 and 0′ . Then
0′ = 0′ + 0 (by axiom 3, since 0 is a zero vector)
= 0 + 0′ (by axiom 1, commutativity)
= 0
(by axiom 3, since 0′ is a zero vector).
So 0 and 0′ are equal.
129

Doing this may feel a little bit like learning to walk again after a serious leg injury:
hard work for something that we have previously taken for granted. Here is another
such “relearning step.”
Fact 4.7. Let (V, +, ·) be a vector space. For every v ∈ V , there is exactly one negative vector −v
(a vector satisfying axiom 4 of Definition 4.1: v + (−v) = 0).
Proof. First of all, we need to realize that there is no way of simply computing −v as we
do it in Rm , by negating all entries. A vector v might not have any such “entries”, and
there is no “−” operator in (V, +, ·) that we could apply. In the proof, we can only use
the 8 axioms (and everything we have already derived from them). We could attempt to
compute −v as (−1)v using scalar multiplication; this can indeed be shown to produce
a negative vector but does not rule out the existence of another negative vector. Here is
how we go about that:
Take two negative vectors u and u′ of v. Then
u′ =
=
=
=
=
=
=

u′ + 0
u′ + (v + u)
(u′ + v) + u
(v + u′ ) + u
0+u
u+0
u

(by axiom 3, zero vector)
(by axiom 4, since u is a negative of v)
(by axiom 2, associativity)
(by axiom 1, commutativity)
(by axiom 4, since u′ is a negative of v))
(by axiom 1, commutativity)
(by axiom 3, zero vector).

So u and u′ are equal.
Having understood why a vector space is a triple (V, +, ·) and not simply a set V of
vectors, we will continue our (now more educated) abuse of notation and still write V for
the vector space, with the understanding that vector addition and scalar multiplication
are clear from the context. We also still write 0 for the zero vector when V is clear from
the context.

4.1.2

Subspaces

Definition 4.8 (Subspace). Let V be a vector space. A nonempty subset U ⊆ V is called a
subspace of V if the following two axioms of a subspace are true for all v, w ∈ U and all
λ ∈ R.
(i) v + w ∈ U ;
(ii) λv ∈ U .
These axioms guarantee that vector addition and scalar multiplication cannot take us
out of the subspace. In this sense, a subspace is a natural “sub-habitat” within a bigger
habitat V (see Figure 4.1) for some examples). As a consequence of the axioms, a subspace
of V always contains at least the zero vector.
130

y

y

x
z

y

x
z

x
z

Figure 4.1: Subspaces of R3 : a line through 0 (all scalar multiples of one vector); a plane
through 0 (all linear combinations of two linearly independent vectors); the right subset
is not a subspace, since it misses 0.
Lemma 4.9. Let U ⊆ V be a subspace of a vector space V . Then 0 ∈ U .
Proof. Take any u ∈ U (U is nonempty). By subspace axiom (ii), we have 0u = 0 ∈ U .
This proof seems quite clear, but it is actually incomplete. While the equation 0u = 0
certainly holds in any Rm , this does not automatically mean that it holds in all vector
spaces. We need to prove it—another case of learning to walk again. We promise, it is the
last one! In fact, the axioms of vector spaces have been carefully designed by mathematicians before us, with the goal of ensuring that everything that seems obvious is actually
true. So we will relax and rely on this in the future.
Fact 4.10. Let V be a vector space, v ∈ V . Then 0v = 0.
Proof. This may not be the shortest proof (can you find a shorter one?), but it works:
=
=
=
=
=
=

0v
0v + 0
0v + (0v + (−0v))
(0v + 0v) + (−0v)
(0+0)v + (−0v)
0v + (−0v)
0

(by axiom 3 of Definition 4.1, zero vector)
(by axiom 4, negative vector)
(by axiom 2, associativity)
(by axiom 8, distributivity over + in R)
(by the rules of R)
(by axiom 4, negative vector)

Figure 4.1 above gives two examples and one counterexample of subspaces of R3 .
Next we see three subspaces that we have already encountered, without thinking about
them as subspaces of a vector space: the column space, the row space, and the nullspace
of a matrix. These are the three fundamental subspaces in the chapter title.
131

Lemma 4.11 (The column space is a subspace). Let A be an m × n matrix. Then the column
space C(A) = {Ax : x ∈ Rn } is a subspace of Rm .
Proof. Let v, w be in C(A). Then there exist vectors x, y ∈ Rn such that v = Ax, w = Ay.
Hence,
A(x + y) = Ax + Ay = v + w ⇒ v + w ∈ C(A).
| {z }
∈Rn

This was subspace axiom (i). For axiom (ii), let λ ∈ R. Then
A(|{z}
λx ) = λAx = λv

⇒

λv ∈ C(A).

∈Rn

In both chain of equalities, the first equality comes from linearity of matrix transformations, see Lemma 2.19 and its proof.
Since the row space of a matrix is the column space of the transpose (Definition 2.14),
we immediately get
Corollary 4.12 (The row space is a subspace). Let A be an m × n matrix. Then the row space
R(A) = C(A⊤ ) is a subspace of Rn .
Finally, the nullspace of a matrix (Definition 2.17) is also a subspace. We leave this as
an exercise.
Exercise 4.13 (The nullspace is a subspace). Let A be an m × n matrix. Then the nullspace
N(A) = {x ∈ Rn : Ax = 0} is a subspace of Rn .
If A is an m×n matrix and b ∈ Rm , the set of solutions of the system of linear equations
Ax = b is a subset of Rn , but not a subspace if b ̸= 0. Indeed, in this case, 0 is not a
solution, so the set of solutions cannot be a subspace by Lemma 4.9. If b = 0, the set of
solutions is the nullspace of A and therefore a subspace by Exercise 4.13.
Knowing that a subspace always contains 0, we can actually say more.
Lemma 4.14 (Subspaces are vector spaces). Let V be a vector space, and let U be a subspace of
V . Then U is also a vector space (with the same “+” and “·” as V ).
Proof. Formally, to turn “+” and “·” into functions that work for U , we have to restrict
their domains to U × U and R × U , respectively. The subspace axioms (i) and (ii) make
sure that we can then also restrict their codomains to U without losing anything.
Next, we need to check the 8 axioms. All but axiom 4 are true for all vectors in V ,
since V is a vector space; in particular, they hold for all vectors in U , so there is nothing
to check. In Case of axiom 3, we are also using that 0 ∈ U (Lemma 4.9). What remains is
axiom 4: we need to make sure that for all u ∈ U , −u is actually in U ; so far we only know
that it is in V . But this holds, since (−1)u ∈ U by subspace axiom (ii), and “obviously”
(−1)u = −u. If you are up for it, you can prove the obvious, otherwise, you can safely
believe it.
132

Subspaces of R[x]. Let us look at some subspaces of R[x], the vector space of polynomials (Theorem 4.4). A polynomial without constant term is a polynomial of the form
p=

m
X

p i xi .

i=1

An example is x2 + 3x. It is clear that these polynomials form a subspace of R[x], since the
sum of two polynomials without constant term is again a polynomial without constant
term, and so is each scalar multiple of a polynomial without constant term.
A quadratic polynomial is a polynomial p of the form
p = p 0 + p 1 x + p 2 x2 .
We do not require p2 ̸= 0, so 3x is also a quadratic polynomial. Again, it is easy to see that
the quadratic polynomials form a subspace of R[x]. In fact, this subspace looks a lot like
R3 : each quadratic polynomial p is determined by three real numbers p0 , p1 , p2 , so we can
also describe it by a vector
 
p0
vp = p1  ∈ R3 .
p2
Moreover, “+” and “·” on quadratic polynomials translate to “+” and “·” on the corresponding vectors:
vp+q = vp + vq , vλp = λvp .
(4.1)
Therefore, the subspace of quadratic polynomials is just R3 in disguise. The mathematical
term is that the two spaces are isomorphic. We will formalize this in Section 4.2.5 below.
Subspaces of Rm×n . Let us turn to Rm×n , the vector space of m × n matrices (Theorem 4.5). Here, we first observe that this is not really a new vector space, as Rm×n is isomorphic to Rmn : an m×n matrix is one way of grouping mn numbers, an mn-dimensional
vector is another way. In both cases, vector addition and scalar-multiplication are defined
entry-wise, so it does not really matter how we group the numbers.
The difference is that we think of a vector in Rmn as one column with mn entries
(Definition 1.1), while a matrix in Rm×n is a table with its mn entries arranged in m rows
and n columns (Definition 2.1).
The table view leads to subspaces of Rm×n that would not make intuitive sense in Rmn .
For the examples, we consider R2×2 .
Our first subspace is the set of symmetric matrices according to Definition 2.3 (v), the
ones of the form


a b
.
b d
To see that this is a subspace, it suffices to observe that the sum of two symmetric matrices
is symmetric, and that scaling a symmetric matrix keeps it symmetric. The second and
133

slightly more creative subspace consists of the matrices of trace 0, the ones of the form


a b
, where a + d = 0.
c d
Generally, the trace of a square matrix is the sum of the diagonal entries. Again, it is easy
to see that the subspace axioms are satisfied. However, matrices of trace 1 do not form a
subspace, and there are many other non-subspaces, for example the invertible matrices,
the ones of rank 1, etc. For the three sets of matrices just mentioned, the “non-subspace”
property is easy to see from Lemma 4.9: the zero matrix neither has trace 1, nor is it
invertible, nor has it rank 1. A non-subspace containing the zero matrix is the set of all
matrices with only nonnegative entries. Find a violation of the subspace axioms for this!

4.2

Bases and dimension

The dimension of a vector space is an important measure of its complexity. So far, we
only have an intuitive understanding of dimension, according to which Rm has dimension m. In this section, we define bases of vector spaces and prove via the Steinitz
exchange lemma that every vector space has a basis, and that all bases have the same
size. This allows us to define the dimension of a vector space as the size of an arbitrary basis of it. We introduce linear transformations between vectors spaces and
show how they can be used to formally define when two vector spaces are isomorphic. Maybe surprisingly, all (real) vector spaces of the same dimension are isomorphic. A basis is also useful as a minimal description of the vector space; computing a
vector space means to compute a basis of it.
According to our intuition, Rm should have dimension m. But if V is some other vector
space, we may not have such an intuition, so we need to define the dimension of a vector
space. We expect this definition to tell us that Rm indeed has dimension m.
What, for example, is the dimension of the vector space of polynomials introduced in
Section 4.1.1)? As it “contains” R3 (in form of the quadratic polynomials, see page 133),
as well as R4 (in form of the cubic polynomials), R5 , and so on, we expect the dimension
to be infinite.

4.2.1

Linear combinations and related concepts in vector spaces

The crucial concept that we develop below is that of a basis. A basis of a vector space V
consists of linearly independent vectors whose span is V . For that, we need the concept
of linear independence and span in an abstract vector space.
In Rm , we have first defined linear combinations of vectors (Definition 1.4). Based
on this fundamental concept, we have introduced linear (in)dependence and the span,
proving various results around them (Section 1.3). Formally, we would have to redo
134

all this for an abstract vector space V —after convincing ourselves that it can be done
in the same way—in order to make use of linear combinations and related notions also
in other vector spaces. However, we promised that we will stop proving the “obvious”
after Fact 4.10; mechanically redoing Rm theory for abstract vector spaces is part of the
“obvious”, so we skip it.
But there is something new for abstract vector spaces. Previously, we have worked
with sequences of vectors in Rm . This was important in order to handle for example the
sequence of columns of a matrix that may contain duplicates. For an abstract vector space
V , we can mechanically redo this, but in addition, we will also work with sets of vectors.
The reason is that we may have to argue with infinite sets of vectors once we move away
from Rm , and such sets are cumbersome (or even impossible) to describe with sequences.
Here is an example where infinite sets of vectors are needed.
It is easy to find a basis of Rm : simply take the m standard unit vectors e1 , e2 , . . . , em .
But in R[x], the vector space of polynomials, finitely many polynomials p1 , p2 , . . . , pn
cannot form a basis: if di is the degree of pi , then xd with d = max(d1 , d2 , . . . , dn ) is the
highest power of x occurring in p1 , p2 , . . . , pn . But then the polynomial xd+1 ∈ R[x] is not
a linear combination of p1 , p2 , . . . , pn , so these polynomials do not even span R[x].
In order to deal with such situations, we define linear combination, linear (in)dependence and span for a (possibly infinite) set of vectors.
Definition 4.15 (Linear combination of a set of vectors). Let V be a vector space, G ⊆ V a
(possibly infinite) subset of vectors. A linear combination of G is a sum of the form
n
X

λj vj ,

j=1

where F = {v1 , v2 , . . . , vn } is a finite subset of G.
Hence, a linear combination of G is obtained by selecting finitely many elements of G
and taking their “normal” linear combination according to Definition 1.4. If G itself is
finite, then this is a normal linear combination of the elements of G.
There are two things to note here. First, there is no defined order of elements in a set,
so the sequence v1 , v2 , . . . , vn lists the elements of F in some arbitrary order. We have
to make sure that this order does not matter. This is the case: since vector addition in a
vector space is commutative by Definition 4.1 (i), the result of the linear combination does
not depend on how we order the vectors.
Second, the sequences v1 , v2 , . . . , vn arising from Definition 4.15 do not contain any
duplicates, since a set contains each of its elements only once.
Here is an important fact. In Rm , we never even thought about it. You could again call
it “obvious”, but since it is about the essence of a vector space (being a natural habitat for
vectors), we still want to prove it.
Lemma 4.16 (A vector space is closed under linear combinations). Let V be a vector space.
Every linear combination of V is again in V .
135

This is quite intuitive: the codomains of the functions + and · in Definition 4.1 are
V , meaning that the two “natural vector behaviors”, namely vector addition (v + w) and
scalar multiplication (λv), do not take us out of the vector space. And linear combinations
simply combine these two natural behaviors.
P
Proof. Let nj=1 λj vj be a linear combination of V . Using that V is a vector space (Definition 4.1), we get wj := λj vj ∈ V for all j, by definition of the scalar multiplication
(· : R × V → V ). By definition of vector addition (+ : V × V → V ), we also have
w1 + w2 ∈ V . Applying this again yields (w1 + w2 ) + w3 ∈ V , and so on, until we get the
desired conclusion w1 + w2 + · · · + wn ∈ V . (Under the hood, this is a proof by induction, and it uses the “obvious” fact that brackets can be omitted in writing down a sum of
vectors).
You may wonder why we do not allow infinite linear combinations. Infinite sums in
themselves can be defined, you may for example know the formula
∞
X
j=0

xj =

1
1−x

for x ∈ R, |x| < 1.

The problem is that Lemma 4.16 may fail for infinite linear combinations, and we do
not want this. Consider the vector space of polynomials R[x], and the infinite “linear
combination”
∞
X
xj
j=0

of the unit monomials 1, x, x2 , . . .. This is not a polynomial: according to Definition 4.3, a
polynomial contains only finitely many different monomials (powers of x).
Therefore, the vector space axioms only imply that finite linear combinations of vectors
are again vectors, but infinite ones may be undefined or take us out of the vector space,
as we have just seen.
We conclude with the definitions of linear (in)dependence and span of a set of vectors,
based on linear combinations. This is analogous to what we have done in Definitions 1.21
and 1.25 for sequences of vectors.
Definition 4.17 (Linear independence and span of a set of vectors). Let V be a vector space,
G ⊆ V a (possibly infinite) subset of vectors.
The set G is called linearly dependent if there is an element v ∈ G such that v is a linear
combination of G \ {v}. Otherwise, G is called linearly independent.
The span of G, written as Span(G), is the set of all linear combinations of G.

4.2.2

Bases

With linear independence and span as in Definition 4.17, we can now formally define a
basis of a vector space.
136

Definition 4.18 (Basis). Let V be a vector space. A subset B ⊆ V is called a basis of V if B is
linearly independent and Span(B) = V .
Examples. The set {e1 , e2 , . . . , em } of standard unit vectors (Section 1.2.2) is the standard
basis of Rm . For example, if m = 2, then
 
 
1
0
e1 =
, e2 =
.
0
1
These two vectors are linearly independent and span R2 : for every vector
 
v
v = 1 ∈ R2 ,
v2
we have v = v1 e1 + v2 e2 . For general m, the standard unit vectors are seen to be linearly
independent by the private nonzero argument: every standard unit vector has a nonzero
entry (a 1-entry, actually) at a coordinate where all other standard unit vectors have 0entries. We call such an entry a private nonzero. A vector with a private nonzero cannot be
a linear combination of the other vectors, and if every vector has a private nonzero, the
vectors are linearly independent.
Lemma 4.19. Let A be an m × n matrix. The set of independent columns of A (Definition 2.10)
is a basis of the column space C(A).
Proof. C(A) is a subspace by Lemma 4.11 and thus also a vector space by Lemma 4.14. The
independent columns are in the column space and linearly independent: by definition,
no independent column is a linear combination of the previous columns, and this means
that the independent columns are in fact linearly independent; see Corollary 1.23 (iii).
Furthermore, the independent columns span the column space, as we have shown in
Lemma 2.11.
For the subspace of symmetric 2 × 2 matrices


a b
,
b d
the following set of three symmetric matrices is a basis:

 
 

1 0
0 1
0 0
,
,
.
0 0
1 0
0 1
They are linearly independent: every matrix has at least one private nonzero, a 1-entry
where all other matrices have 0-entries. It remains to observe that every symmetric matrix
is a linear combination of these three matrices. Indeed, we have








a b
1 0
0 1
0 0
=a
+b
+d
.
b d
0 0
1 0
0 1
137

For the trace-0 matrices

a basis is



a b
, where a + d = 0,
c d

 
 

1
0
0 1
0 0
,
,
.
0 −1
0 0
1 0

Linear independence is again easy due to private nonzeros, and as d = −a in a trace-0
matrix, we can obtain every trace-0 matrix as a linear combination:








a
b
1
0
0 1
0 0
=a
+b
+c
.
c −a
0 −1
0 0
1 0
For the vector space R[x] of polynomials, the infinite set of unit monomials
{xi : i ∈ N}
is a basis. By Definitions 4.3 (of a polynomial) and 4.15 (of a linear combination), every
polynomial is a linear combination of unit monomials (recall that x0 = 1). It remains to
argue that the unit monomials are linearly independent. Indeed, every unit monomial
xi has its “private nonzero”, the i-th power of x and can therefore not be obtained as
a linear combination of other monomials. Here, we really use Definition 4.17 of linear
(in)dependence and span for an infinite set.
The subspace of polynomials without a constant term has
{xi : i ∈ N, i > 0}
as a basis, and for the subspace of quadratic polynomials, a basis is
{1, x, x2 }.
Finally, what is the basis of {0}, the smallest possible subspace of a given vector space?
It is the empty set . Indeed, this is linearly independent by Definition 4.17: in the empty
set, no vector is a linear combination of the others. And Span(∅) = {0}, since an empty
sum yields 0; see the discussion in Section 1.1.5.
There are typically many bases. The above examples should not trick us into believing
that there is always only one basis of a vector space. For example, {e1 , e2 , . . . , em } is the
canonical basis of Rm , but there are many other choices.
Observation 4.20. Every set B = {v1 , v2 , . . . , vm } ⊆ Rm of m linearly independent vectors is
a basis of Rm .
Proof. B is linearly independent, and then Span(B) = Rm is true by Lemma 1.28.

138

As a second example, let us consider the column space C(A) of a matrix A. In Section 2.3.5, we have asked you to believe that the independent columns of


1 2 0 3
A = 2 4 1 4
3 6 2 5
are the first and the third column, so they form a basis of C(A) by Lemma 4.19. But we
could also have defined the “backwards independent” columns, by going through the
columns of A from right to left, and selecting a column if it is not a linear combination of
the ones succeeding it. This also results in a basis of C(A), by the same arguments as for
the “forward independent” columns. If we do this in our example, we end up with the
fourth and the third column as a basis.
More generally, we could go through the columns in any order and pick up the ones
that are not linear combinations of the ones previously considered. This potentially gives
us many different bases of C(A).
However, what we find in both Rm and C(A) is that the alternative bases still have the
same number of vectors: m in case of Rm and 2 in case of the column space example. In
Section 4.2.3, we prove that this is not a coincidence but true for every vector space.
Existence of a basis. In all examples so far, we have been able to find bases, but is it
actually true that every vector space has a basis? The answer is yes, but we will only
prove this for finitely generated vector spaces, see the next Definition 4.21. The general
proof involves some machinery that is standard but beyond the scope of these notes. The
Wikipedia article about bases of vector spaces is a good entry point for further reading.2
Definition 4.21 (Finitely generated vector space). A vector space V is called finitely generated if there exists a finite subset G ⊆ V with Span(G) = V .
For example, Rm is finitely generated (by G = {e1 , e2 , . . . , em }) but R[x], the vector
space of polynomials, is not, as we have argued on page 135.
Theorem 4.22. Let V be a finitely generated vector space, and let G ⊆ V be a finite subset with
Span(G) = V . Then V has a basis B ⊆ G.
Proof. This is what we call an “algorithmic proof”. It constructs B by an algorithm. Here
is how it goes.
If G is linearly independent, then B := G is a basis by Definition 4.18, so we can stop. “line 1”
Otherwise, some vector v ∈ G is a linear combination of the other ones (Definition 4.17),
so Corollary 1.27 gives Span(G \ {v}) = Span(G) = V . The corollary is about sequences,
not sets, but since G is a finite set, we can apply the corollary to any ordering of G. Now
we replace G with G \ {v} (which still spans V ) and go back to line 1. Because the set G
gets smaller in every step, the algorithm must eventually stop with a basis in line 1.
2

https://en.wikipedia.org/wiki/Basis_(linear_algebra), accessed September 2, 2025

139

A formal correctness proof would go via a precise formulation of the algorithm, either as a loop (correctness proof with loop invariants), or a recursive algorithm (correctness
proof by induction). The latter approach is very close to directly proving the theorem by
induction on g = |G| (this is a good exercise). Our proof above, while hopefully clear and
easy to understand, is of a somewhat informal “and-so-on” nature, but knowing how we
could make it formal if necessary, this is acceptable.
We also see why we need “finitely generated” here. Starting the algorithmic proof
above from some infinite set G, we can still remove one element from G in each step, as
long as G is linearly dependent, but we cannot argue that this ever stops: after removing
an element from an infinite set, we still have an infinite set, so we are not making progress.

4.2.3

The Steinitz exchange lemma

The Steinitz exchange lemma is a cornerstone result in the theory of vector spaces. Let V be
a finitely generated vector space, let F ⊆ V be a finite set of linearly independent vectors,
and G ⊆ V a finite set of vectors with Span(G) = V . The lemma makes two statements.
The first one is |F | ≤ |G|. This makes sense: In R2 , for example, we can have at most 2
linearly independent vectors, and it takes at least 2 vectors to span R2 .
The second statement sounds a bit technical at first: we can enlarge F by some elements from G such that the enlarged set has at most the size of G and also spans V . But
despite the technical language, this statement is immensely useful.
The name of the lemma comes from the fact that we can think of it as “exchanging
elements between G and F ”.
Lemma 4.23 (Steinitz exchange lemma). Let V be a finitely generated vector space, F ⊆ V a
finite set of linearly independent vectors, and G ⊆ V a finite set of vectors with Span(G) = V .
Then the following two statements hold.
(i) |F | ≤ |G|.
(ii) There exists a subset E ⊆ G of size |G| − |F | such that Span(F ∪ E) = V .
Note that the set E in (ii) may contain elements of F . In this case, |F ∪ E| < |G|.
We have in fact already seen a special case of the Steinitz exchange lemma in the form
of Lemma 1.28 (m linearly independent vectors v1 , v2 , . . . , vm ∈ Rm span Rm ). To derive
this from Lemma 4.23, we use V = Rm , F = {v1 , v2 , . . . , vm } and G = {e1 , e2 , . . . , em } (the
set of m standard unit vectors spans Rm ). Since we already have |F | = |G|, we get |E| = 0,
so E must be the empty set, and Span(F ) = Span(F ∪ E) = Rm .
Proof. Although the Steinitz exchange lemma is more general than Lemma 1.28, we can
use (almost) the same proof. We start with the set G and in each step replace one of its
elements with an element from F , without changing the span. Once all elements of F
have been “swapped in”, we are done (with E being the elements of G that remain). We
actually copy the proof of Lemma 1.28 to the extent possible. Here it comes:
140

Suppose that F = {v1 , v2 , . . . , vm } and G = {w1 , w2 , . . . , wn }, so we list the elements
of F and G in some arbitrary order. Now we consider the sequence
v 1 , w1 , w2 , . . . , wn ,
obtained by adding v1 before the vectors from G. Since Span(w1 , w2 , . . . , wn ) = V , we
know that v1 ∈ V is a linear combination of w1 , w2 , . . . , wn , and hence the n + 1 vectors
are linearly dependent by Definition 1.21. Then we also know that one of the vectors is
a linear combination of the previous ones in the sequence, see Lemma 1.22 (iii). This vector cannot be v1 because v1 also starts the linearly independent sequence v1 , v2 , . . . , vm in
which no vector is a linear combination of the previous ones by Corollary 1.23 (iii). Therefore, one of the wi ’s is a linear combination of the previous vectors. Removing that vector
does not change the span (see Corollary 1.27), so we get a sequence v1 , u2 , u3 , . . . , un (the
ui ’s name the n − 1 remaining vectors from G) with
Span(v1 , u2 , u3 , . . . , un ) = V.
So we have successfully replaced one of the vectors of G with v1 , without changing the
span. Now we do the same with v2 : we add v2 directly after v1 and argue as before that
the n + 1 vectors
v1 , v2 , u2 , u3 , . . . , un
must be linearly dependent; therefore one of them is a linear combination of the previous ones. This can neither be v1 nor v2 , because v1 , v2 also start a sequence of linearly
independent vectors. Hence, some ui is a linear combination of the previous vectors and
can be removed without changing the span. We call the n − 2 remaining vectors from G
u3 , u4 , . . . , un (a slight abuse of notation, as this redefines the ui ’s) and get
Span(v1 , v2 , u3 , u4 , . . . , un ) = V.
By now, the pattern should be clear. After m replacement steps, we have n − m remaining vectors um+1 , um+2 , . . . , un from G and get our desired statement (ii):
Span(v1 , v2 , . . . , vm , um+1 , um+2 , . . . , un ) = V.
{z
} |
|
{z
}
F

E, with |E|=n−m=|G|−|F |

There is one caveat: this argument only works if m ≤ n which is what we still need
to prove as statement (i) of the lemma. But the argument itself proves it: whenever we
add the next element of F to our current sequence, there must still be some element of G
left, because we are guaranteed to find an element of G that we can remove. Therefore,
|F | ≤ |G|, indeed.
The Steinitz exchange lemma has an important corollary. Because of its importance,
we also call it a theorem. It confirms what we have observed in examples before: even if
a vector space has different bases, all of them have the same number of vectors.
141

Theorem 4.24 (All bases have the same size). Let V be a finitely generated vector space and
let B, B ′ ⊆ V be two bases of V . Then |B| = |B ′ |.
Proof. B and B ′ are linearly independent, and Span(B) = Span(B ′ ) = V (Definition 4.18).
Applying statement (i) of the Steinitz exchange Lemma 4.23 with F = B, G = B ′ yields
|B| ≤ |B ′ |; with F = B ′ , G = B, we get |B ′ | ≤ |B|.
There are vector spaces that are not finitely generated, such as the vector space R[x] of
polynomials defined in Section 4.1.1. While Theorem 4.24 does not apply to such vector
spaces, it can be generalized to the infinite case where |B| = |B ′ | then means “the same
kind of infinity.”

4.2.4

Dimension

Now we can define the dimension of a vector space, at least if it is finitely generated.
Definition 4.25 (Dimension). Let V be a finitely generated vector space. Then dim(V ), the
dimension of V , is the size of an arbitrary basis B of V .
This definition uses that every finitely generated vector space has a basis to begin with
(Theorem 4.22), and that all bases have the same size (Theorem 4.24).
From the examples of bases in Section 4.2.2, you can therefore immediately deduce the
dimensions of the corresponding vector spaces. As expected, dim(Rm ) = m.
Figure 4.2 shows three subspaces of R3 , of dimensions 0 (point), 1 (line), and 2 (plane).

y

y

x
z

y

x
z

x
z

Figure 4.2: Three subspaces of R3 : The unique subspace of dimension 0, the point at the
origin with an empty basis (left); a subspace of dimension 1, a line through the origin with
a basis of size 1 (middle); a subspace of dimension 2, a plane through the origin with a
basis of size 2. (right)
In many cases, the dimension of a vector space directly corresponds to the degrees of
freedom in its definition, informally defined as the number of values that can vary independently. For example, Rm has m degrees of freedom, since all m coordinates of a vector
142

can vary independently within Rm . The vector space of symmetric 2 × 2 matrices


a b
b d
has 3 degrees of freedom, the values a, b, d. Consequently, its dimension is 3. For the 2 × 2
matrices of trace 0,


a b
, where a + d = 0,
c d
we have 4 defining values a, b, c, d, but writing a trace-0 matrix as


a
b
,
c −a
we saw that there are also only 3 degrees of freedom, the values a, b, c. So the dimension
of this vector space is 3 as well.
But there are vector spaces without such obvious ways of identifying degrees of freedom. For example, the column space C(A) of a matrix A has dimension r = rank(A)
(because the r independent columns form a basis; see Lemma 4.19). But it is not clear
from the definition of C(A) whether there are any r values that can vary independently.
Theorem 4.29 and the discussion after this show the existence of such degrees of freedom
for every vector space, even if they are initially hidden.

4.2.5

Linear transformations between vector spaces

In Section 2.2.2, we have defined linear transformations as functions T : Rn → Rm satisfying linearity: T (λ1 x1 + λ2 x2 ) = λ1 T (x1 ) + λ2 T (x2 ) for all x1 , x2 ∈ Rn and all λ1 , λ2 ∈ R.
However, the right level of abstraction is to define linear transformations as functions
between two vector spaces V and W . Previously, we have only used V = Rn , W = Rm .
Bijective (undoable) linear transformations between vector spaces are particularly interesting. A bijective linear transformation between V and W can be used to compute a
basis of one of the spaces from a basis of the other one (see Lemma 4.27 below). We will
make use of this in computing a basis of the nullspace of a matrix in Section 4.3.3.
Definition 4.26 (Linear transformation between vector spaces). Let V, W be two vector
spaces. A function T : V → W is called a linear transformation between vector spaces if
the following linearity axiom holds for all x1 , x2 ∈ V and all λ1 , λ2 ∈ R.
T (λ1 x1 + λ2 x2 ) = λ1 T (x1 ) + λ2 T (x2 ).
In order for linearity to even make sense, we need λ1 x1 + λ2 x2 ∈ V (so that this vector
is a legal input for T ), and λ1 T (x1 ) + λ2 T (x2 ) ∈ W (so that this vector is a legal output
of T ). For V = Rn and W = Rm , we did not have to think about this at all, and here, we
have to think about it only for a second: by Definition 4.1, adding up and scaling vectors
143

does not take us out of the vector space, so everything is fine. Or, how Lemma 4.16 puts
it: a vector space is closed under linear combinations.
Linear transformations and linear functionals that we have introduced separately in
Definition 2.21 are now both special cases of linear transformations between vector spaces.
For a linear functional, we use W = R. Indeed, with its normal “+” and “·”, the set of real
numbers is “obviously” a vector space according to Definition 4.1.
Here, we are particularly interested in bijective (undoable) linear transformations between vector spaces. The key lemma is the following (we refer to Definition 2.48 for the
term bijective, but we also explain it again during the proof).
Lemma 4.27 (Bijective linear transformations preserve bases). Let T : V → W be a bijective
linear transformation between vector spaces V and W . Let B = {v1 , v2 , . . . , vℓ } ⊆ V be a finite
set of some size ℓ, and T (B) = {T (v1 ), T (v2 ), . . . , T (vℓ )} ⊆ W the transformed set. Then
|T (B)| = |B|. Moreover, B is a basis of V if and only if T (B) is a basis of W . We therefore also
have dim(V ) = dim(W ).
In words, bijective linear transformations between vector spaces “preserve” bases and
dimension (but the lemma only proves this in the finitely generated case). If there is a
bijective linear transformation between V and W , the two spaces are isomorphic (“of the
same form”), see Definition 4.28 below.
Proof of Lemma 4.27. The equation |T (B)| = |B| might seem obvious, but the set notation
is a bit deceptive. If different inputs vi , vj would lead to the same output T (vi ) = T (vj ),
we would get |T (B)| < |B|, because T (B) as a set contains this output only once. However, this situation cannot occur: T being bijective means that for every output, exactly
one input leads to it. Said in another way: different inputs lead to different outputs, so
the ℓ inputs in B also lead to ℓ outputs in T (B).
Next we prove the “if” direction: if B is a basis of V , then T (B) is a basis of W . The
other (“only if”) direction follows by applying the “if” direction to the inverse T −1 which
is a bijective linear transformation T −1 : W → V (we use the “obvious” generalization of
Lemma 2.52 to T : V → W ). For T −1 , the “if” direction says that if T (B) is a basis of W ,
then T −1 (T (B)) = B is a basis of V . This is our desired “only if” direction. Here we use
that T −1 ◦ T = id (T −1 is undoing T ; see Fact 2.49).
It remains to prove the “if” direction: if B is a basis of V , then T (B) is a basis of W .
For this, we use the generalization of linearity that we have proved in Lemma 2.25 for the
special cases V = Rn and W = Rm or W = R, but it “obviously” holds for abstract vector
spaces V, W as well: for all x1 , x2 , . . . , xℓ ∈ V and all λ1 , λ2 , . . . , λℓ ∈ R, we have
!
ℓ
ℓ
X
X
T
λj xj =
λj T (xj ).
(4.2)
j=1

j=1

Now for the actual proof. If B is a basis, we have to show according to Definition 4.18
that T (B) is (a) linearly independent, and (b) Span(T (B)) = W .

144

(a): We show linear independence of T (B) according to Corollary 1.23 (ii): the zero
vector can be written as a linear combination
ℓ
X

λj T (vj ) = 0

j=1

of T (B) only in the trivial way, meaning λj = 0 for all j. To see this, we apply (4.2) to such
a linear combination and get
!
ℓ
ℓ
X
X
0=
λj T (vj ) = T
λj vj .
j=1

j=1

T is bijective and therefore injective (for every possible output, at most one input leads to
it). Output 0 can therefore only come from input 0 (use the “obvious” generalization of
Lemma 2.24 to linear transformations between vector spaces); hence
ℓ
X

(4.3)

λj vj = 0.

j=1

Since B is a basis of V and therefore linearly independent, (4.3) implies λj = 0 for all j,
see Corollary 1.23 (ii).
(b): We show Span(T (B)) = W by writing every vector w ∈ W as a linear combination
of T (B). Since T is bijective and therefore surjective (for every possible output, at least
one input leads to it), there is a vector v ∈ V such that T (v) = w. Because B is a basis of
V , we have Span(B) = V , so v is a linear combination of B,
v=

ℓ
X

λj vj .

j=1

Applying T to both sides gives
w = T (v) = T

ℓ
X

!
λj vj

j=1

(4.2)

=

ℓ
X

λj T (vj ),

j=1

writing w as a linear combination of T (B).
Having a bijective transformation T : V → W , means that from “knowing” V (having
a basis of it), we also “know” W , and vice versa (we talk about this kind of “knowledge”
in more detail in Section 4.2.6 below). Moreover, the two spaces have the same dimensions. In this case, we consider V and W as “essentially the same” (on the very abstract
level of category theory that we will not further go into here). The mathematical term for
this is that V and W are isomorphic.
145

Definition 4.28 (Isomorphic vector spaces, isomorphism). Let V, W be two vector spaces. If
there is a bijective linear transformation T : V → W (Definition 4.26), then V and W are called
isomorphic, and T is called an isomorphism between V and W .
We have already seen an example of two isomorphic vector spaces on page 133. Let
V be the vector space of degree-2 polynomials, and W = R3 . We have observed before
that both a degree-2 polynomial as well as a vector in R3 are determined by a triple of
real numbers, and in both cases, the vector operations (vector addition, scalar multiplications) are the same on the level of these triples. For example, adding the two degree-2
polynomials 3x2 + 1 and 2x − 5 results in the degree-2 polynomial 3x2 + 2x − 4. On “triple
level”, this is (1, 0, 3) + (−5, 2, 0) = (−4, 2, 3). The corresponding vector addition
     
1
−5
−4
0 +  2 =  2
3
0
3
does the same on triple level. Therefore, although defined differently, degree-2 polynomials and vectors in R3 are conceptually the same. The notion of an isomorphism captures
this. Indeed, here is the canonical isomorphism between degree-2 polynomials (vector
space V ) and W = R3 :
T : p 7→ vp ,
with vp as defined on page 133. In (4.1), we have in fact already said what it takes to check
linearity of T according to Definition 4.26.
Another case of isomorphism is V = Rmn and W = Rm×n , see page 133.
All m-dimensional vector spaces are isomorphic. If we consider isomorphic vector
spaces as “the same”, then the diversity among vector spaces is much smaller than you
might have expected. In fact, any two (real) vector spaces of the same dimension m are
isomorphic! We will not give the full proof but the main ingredients, from which it is not
hard to work out the full proof. No deep mathematics is needed here.
The main ingredient is the following nice property of a basis. It does not only span the
vector space, it even writes every vector as a linear combination in a unique way. This is
true for all vector spaces, but for simplicity, we only cover the finitely generated case.
Theorem 4.29 (A basis writes each vector as a unique linear combination). Let V be a
finitely generated vector space of dimension m and B = {v1 , v2 , . . . , vm } ⊆ V a basis of V . For
every v ∈ V , there are unique scalars λ1 , λ2 , . . . , λm such that
v=

m
X

λj vj .

j=1

Proof. Since Span(B) = V by Definition 4.18 of a basis, we are sure to find some such
scalars. Now we proceed as in the proof of Lemma 1.24: given two linear combinations
v=

m
X

λj vj =

j=1

m
X
j=1

146

µj v j ,

we subtract them, after which λj = µj for all j follows from linear independence of B
according to Corollary 1.23 (ii).
From this, we conclude that any two (real) vector spaces of the same dimension m are
isomorphic. The reason is the following: take any vector space V of dimension m and fix
a basis B = {v1 , v2 , . . . , vm } ⊆ V with an arbitrary ordering of its vectors. Given this, any
vector v ∈ V can be described by its unique coordinate vector (λ1 , λ2 . . . , λm ) ∈ Rm , where
the λj are the ones guaranteed by Theorem 4.29. One can check that the function
T : V → Rm , v 7→ v’s coordinate vector
is an isomorphism between V and Rm according to Definition 4.28.
Coming back to the degrees of freedom mentioned in Section 4.2.4, we can now declare
the m entries of the coordinate vector as the degrees of freedom of V . Indeed, they can
vary independently, and each choice uniquely leads to an element of V .

4.2.6

Computing a vector space

All vector spaces and subspaces (except the trivial one, {0}) that we have seen in this
chapter have infinitely many elements. Computing a vector space can therefore not mean
to output all its elements. Instead, we want to output something that describes all the
elements, and this output should ideally be as small as possible.
This is precisely what a basis of a vector space (Definition 4.18) does. Indeed, if V
is a vector space with a basis B ⊆ V , we know that V = Span(B), the set of all linear
combinations of B. This is the description of V . Moreover, in the finitely generated case
(Definition 4.21), B is a smallest description in the sense that no smaller set of vectors can
span V . This may be plausible, but we have not proved it, so let us do it now.
Lemma 4.30 (Less than dim(V ) vectors do not span V ). Let V be a finitely generated vector
space. Let G ⊆ V be a finite subset of size |G| < dim(V ) (Definition 4.25). Then Span(G) ̸= V .
Proof. We want to prove the implication |G| < dim(V ) ⇒ Span(G) ̸= V . This is logically
equivalent to the contraposition Span(G) = V ⇒ |G| ≥ dim(V ), so we can instead prove
the second implication (recall Theorem 3.7 (ii)⇒(i) which we have also proved by contraposition). This is now easy: If Span(G) = V , Theorem 4.22 tells us that V has a basis
B ⊆ G. Since |B| = dim(V ) by Definition 4.25, |G| ≥ dim(V ) follows.
Therefore, computing a finitely generated vector space for us means to find a basis of it.
Figure 4.2 visualizes the representation of a space by a basis. Knowing the basis vectors,
the space in question is uniquely determined: it consists of all linear combinations of
the basis vectors. Vector spaces that are not finitely generated cannot be computed in
this way, since we cannot even output the (infinite) basis. But the three fundamental
subspaces that we will compute below are all finitely generated, and we will show how
to find bases of them and therefore also their dimensions.
147

4.3

Computing the three fundamental subspaces

An m × n matrix A defines three fundamental subspaces: Column space, row space,
and nullspace. We show how they can be computed by which we mean to find bases
for them. Once we have such bases, we also know the dimensions of the fundamental
subspaces, and how they relate to each other: If rank(A) = r, then both row and
column space have dimension r, while the nullspace has dimension n − r. We finally
apply these results to compute all solutions of a system of linear equations Ax = b.
For a given m × n matrix A, we now want to compute the three fundamental subspaces
C(A) (column space), R(A) (row space), and N(A) (nullspace). If we have bases for them,
we also know the dimensions of these spaces, by Definition 4.25.
The key to understanding these subspaces is Gauss-Jordan elimination. In Section 3.3.3,
we have shown how this algorithm can transform every matrix A into a unique standard
form, a matrix R in reduced row echelon form (RREF). As our running example, we
again use the matrix that we have previously considered in Sections 2.3.5 and 3.3.4. Here,
Gauss-Jordan elimination leads to the following standard form R (see Page 123):




1 2 0 3
1 2 0
3
(4.4)
A = 2 4 1 4 → R = 0 0 1 −2 .
3 6 2 5
0 0 0
0
From this, we have already been able to read off the CR decomposition of A, see Theorem 3.18. Now we will see how to read off (bases of) the three fundamental subspaces.

4.3.1

Column space

We recall Definition 2.9. The column space of an m × n matrix A is the set of all linear
combinations of the columns of A,
C(A) = {Ax : x ∈ Rn } ⊆ Rm .
We know from Lemma 4.11 that C(A) is a subspace of Rm and therefore also a vector
space by Lemma 4.14. For computing it, we have already done all the work. We summarize the situation in the following theorem.
Theorem 4.31. Let A be an m × n matrix, and let R in RREF(j1 , j2 , . . . , jr ) be the result of
Gauss-Jordan elimination on A according to Theorem 3.17. Then A has its independent columns
at indices j1 , j2 , . . . , jr , and these columns form a basis of the column space C(A). In particular,
dim(C(A)) = rank(A) = r.
Proof. The independent columns of A form a basis of C(A) by Lemma 4.19, and R reveals
their indices j1 , j2 , . . . , jr ; see Theorem 3.18. We thus have dim(C(A)) = r by Definition 4.25 and rank(A) = r by Definition 2.10.
148

In the running example (4.4), R is in RREF(1, 3), so the basis B of C(A) resulting from
the theorem is given by the two independent columns 1 and 3,
    
0 
 1



2 , 1 .
B=


3
2

4.3.2

Row space

We recall Definition 2.14. The row space of an m × n matrix A is the set of all linear
combinations of the rows of A, officially defined as the column space of the transpose:
R(A) = C(A⊤ ) ⊆ Rn .
The row space is a subspace of Rn by Corollary 4.12 and therefore also a vector space
by Lemma 4.14.
We could compute R(A) = C(A⊤ ) via Gauss-Jordan elimination on A⊤ , according to
the previous Theorem 4.31. But Gauss-Jordan elimination on A already provides us with
all we need. The upshot is that the first r rows of R (also forming the matrix R′ in the CR
decomposition A = CR′ according to Theorem 3.18) are a basis of the row space.
Theorem 4.32. Let A be an m × n matrix, and let R in RREF(j1 , j2 , . . . , jr ) be the result of
Gauss-Jordan elimination on A, according to Theorem 3.17. Then the first r columns of R⊤ form
a basis of the row space R(A). In particular,
dim(R(A)) = r.
Proof. We have R = M A with M invertible (Theorem 3.17), hence R and A have the same
row space by invariance Lemma 3.5. From R, a basis of the row space can be read off
immediately (for simplicity, we argue with the rows of R although we should officially
argue with the columns of R⊤ ): by RREF (Definition 3.13), R starts with r nonzero rows
and ends with m−r zero rows. Hence, the r nonzero rows already span the row space, and
they are also linearly independent, as each of them has a private nonzero (in the column
where the downward step happens). So the first r rows of R (officially, the first r columns
of R⊤ ) are a basis of R(R) = R(A) by Definition 4.18. We then have dim(C(A)) = r by
Definition 4.25
In our running example (4.4), the basis B of R(A) resulting from the theorem is
   
1
0 


   

2
 ,  0 .
B= 
0  1





3
−2
The previous two Theorems 4.31 and 4.32 have a very interesting consequence that
also deserves to be called a theorem, although it is now simply a corollary. This is the
149

following result that is sometimes summarized as row rank equals columns rank. We have
previously proved this in Section 2.1.4 for matrices of rank 1, but now we can show that
it holds for all matrices.
Theorem 4.33. Let A be an m × n matrix. Then
rank(A) = rank(A⊤ ).
Proof. By Theorem 4.31,
rank(A) = dim(C(A)).

(4.5)

In words, the rank of a matrix is the dimension of its column space. Applying this to the
transpose A⊤ gives
rank(A⊤ ) = dim(C(A⊤ )) = dim(R(A)),
(4.6)
using Definition 2.14 of the row space. It remains to observe that both dim(C(A)) in (4.5)
and dim(R(A)) in (4.6) evaluate to the same number r from Theorems 4.31 and 4.32, the
number of downward steps in the reduced row echelon form R of A.
Corollary 4.34 (The rank ist at most the smaller of the two matrix dimensions). Let A be
an m × n matrix of rank r. Then r ≤ min(n, m).
Proof. The rank of a matrix is the number of independent columns. Theorem 4.33 says
that rank(A) = rank(A⊤ ). Therefore we have r ≤ n (the number of columns of A) and
r ≤ m (the number of columns of A⊤ ).

4.3.3

Nullspace

We recall Definition 2.17. The nullspace of an m × n matrix A is the set of all solutions of
the system Ax = 0,
N(A) = {x ∈ Rn : Ax = 0} ⊆ Rn .
By invariance Lemma 3.3, the matrix R resulting from A via Gauss-Jordan elimination
has the same nullspace as A, and as for column and row space before, a basis of the
nullspace is easy to read off from R. The technical realization of this is a bit more involved
than for column and row space, so we look at the running example (4.4) first. Here,


1 2 0
3
R = 0 0 1 −2 ,
0 0 0
0
with N(A) = N(R). We further observe that N(R) = N(R′ ) where


1 2 0
3
′
R =
0 0 1 −2

150

is obtained from R by removing the zero row at the end. Indeed, the system of equations
Rx = 0 defining N(R) is equivalent to R′ x = 0 defining N(R′ ), since the extra equation
in Rx = 0 is 0⊤ x = 0. This equation always holds and can therefore be ignored.
Splitting the sum in the matrix-vector product R′ x (Definition 2.4) into two sums, one
for j = 1, 3, and one for j = 2, 4, we can write R′ x = 0 as

  
 
1 0 x1
2
3 x2
+
= 0.
0 1 x3
0 −2 x4
The submatrix of R′ with the independent columns 1, 3 is the identity matrix. We can
therefore directly solve for x1 , x3 and get
 

 
x1
2
3 x2
=−
.
(4.7)
x3
0 −2 x4
Now we have a full understanding of all x ∈ N(R′ ): for the values of x2 , x4 , we can
choose arbitrary real numbers, and the values of x1 , x3 are then determined via (4.7). We
also expect N(R′ ) to have dimension 2, because the nullspace—the solutions of (4.7)—
can be described by vectors in R2 , concretely the x2 x4 -plane. In technical terms, N(R′ ) is
isomorphic to R2 (Definition 4.28).
Indeed, a basis of N(R′ ) is obtained in the form of the two special solutions, the ones
coming from the standard unit vectors e1 , e2 in the x2 x4 -plane:
  special
  solutions
 
x2
1
0
x4
0
1

     
x1
2
3 x2
−2
=
−
x3
0 −2 x4
0




−3
2

Let us double check on the running example (4.4) that these two special solutions are
indeed in the original nullspace N(A) that we want to compute. For this, we verify that
 
 

 −3

 −2
1 2 0 3  
1 2 0 3  
2 4 1 4  1 = 0, and 2 4 1 4  0 = 0.
 2
 0
3 6 2 5
3 6 2 5
0
1
But why do these solutions form a basis of N(A) according to Definition 4.18? This
follows from the general construction of a basis of N(A) that we present next.
We proceed in two steps. First, we prove that N(R) = N(A) is isomorphic to Rn−r .
Knowing this, we can apply the “basis preservation” Lemma 4.27 to construct a basis of
N(R) from a basis of Rn−r . We conveniently use the standard basis {e1 , e2 , . . . , en−r } of
Rn−r . This second step works exactly like in the example above, but now we can use
Lemma 4.27 to conclude that the result is indeed a basis of N(R) = N(A).
151

Lemma 4.35 (Nullspace isomorphism). Let R be an m × n matrix in RREF(j1 , j2 , . . . , jr ),
and let k1 , k2 , . . . , kn−r be the column indices not in {j1 , j2 , . . . , jr }. Then T : N(R) → Rn−r ,


xk 1
 xk 
 2 
T : x 7→  .. 
 . 
xkn−r
is an isomorphism (bijective linear transformation) between N(R) and Rn−r . In particular, we get
dim(N(R)) = n − r from Lemma 4.27.
In the example on the previous page, we have k1 = 2, k2 = 4, so in this case T is
 
x2
T : x 7→
.
x4
We have seen in the example why this is bijective (for every possible output, exactly one
input from the nullspace leads to it): given arbitrary output values x2 , x4 , we get unique
values x1 , x3 through (4.7) and therefore a unique input vector x ∈ N(R) leading to the
given values of x2 and x4 . It is also easy to check that T is linear (Definition 4.26). T is in
fact a parallel projection (we saw such a projection in Figure 2.8), but one of the simplest
kind: the output results from just omitting some coordinates (x1 and x3 ) of the input. In
this case, linearity is an immediate consequence of vector addition (Definition 1.2) and
scalar multiplication (Definition 1.3).
Proof of Lemma 4.35. Let R′ be the submatrix of R containing the first r rows (the nonzero
ones). We have N(R) = N(R′ ), because all m − r extra equations in Rx = 0 are 0⊤ x = 0
and can therefore be ignored, since they always hold.
Because R′ is in RREF(j1 , j2 , . . . , jr ) as well, simply without zero rows at the end, column ji of R is ei , the i-th standard unit vector in Rr ; see Definition 3.13 (i). The submatrix
of R′ with columns j1 , j2 , . . . , jr is therefore the identity matrix, and in analogy to (4.7), we
can rewrite R′ x = 0 as
 


xj1
xk 1
 xj 
 xk 
 2
 2 
(4.8)
 ..  = −Q  ..  ,
 . 
 . 
xj r
xkn−r
where Q is the r × (n − r) submatrix of R′ with columns k1 , k2 , . . . , kn−r . This matrix Q is
in general not a square matrix, unlike in (4.7).
Now we can show as in the example that T is bijective: for every possible output (already giving some coordinates of the input x ∈ N(R)), the remaining coordinates of x are
uniquely determined via (4.8), so there is exactly one input leading to the given output.
We omit the easy calculations that prove linearity of T according to Definition 4.26.

152

It remains to find a basis of N(R). For this, we use Lemma 4.27. We want the basis
B = {v1 , v2 , . . . , vn−r } of V = N(R) for which T (B) = {e1 , e2 , . . . , en−r }, the standard basis
of W = Rn−r . In the example, we called the elements of B the special solutions.
Finding them is easy: For all i ∈ [n − r], the condition is T (vi ) = ei , so vi is the unique
input for T leading to output ei . We have previously seen how to construct this via (4.8).
We therefore have the following result.
Theorem 4.36. Let A be an m × n matrix, and let R in RREF(j1 , j2 , . . . , jr ) be the result of
Gauss-Jordan elimination on A according to Theorem 3.17. Let k1 , k2 , . . . , kn−r be the column
indices not in {j1 , j2 , . . . , jr }, and let Q be the r × (n − r) submatrix of R containing the first r
rows and columns k1 , k2 , . . . , kn−r .
A basis of the nullspace N(A) is given by the n − r vectors v1 , v2 , . . . , vn−r , where vi is the
vector x ∈ N(R) with


 
xk1
xj 1
 xk 
 xj 
 2 
 2
 ..  = ei ,  ..  = −Qei .
 . 
 . 
xkn−r
xjr
We note that Qei is the i-th column of Q. In particular,
dim(N(A)) = n − r.
Summary. Here a visual summary (on our running example). It shows the computation
of (bases of) the three fundamental subspaces of A (column space, null space, row space),
based on Gauss-Jordan elimination applied to A (Theorem 3.17). The CR decomposition
A = CR′ (Theorem 2.46) is obtained as a by-product (Theorem 3.18).
C(A)

N(A)
x2 ,x4
z}|{
z}|{
  

1
0
0
1

R(A)

x2 ,x4

Bases:

  
1
0
2 1
3
2

↑ 
↑


1 0
1 2 0 3
2 1 ← 2 4 1 4 →
3 2
3 6 2 5
| {z }
|
{z
}
C

A=CR′

Theorem 4.31

x1 ,x3

x2 ,x3

z }| {
z }| { 
−2
−3
0
2

1
0
0
|

↑
↑
2 0
3
0 1 −2
0 0
0
{z
}

→

   
1
0
2  0
   
0  1
3
−2
↑ 
 ↑
1 2 0
3
0 0 1 −2
|
{z
}
R′ in RREF(1,3)

R in RREF(1,3),Q

Theorem 4.36

153

Theorem 4.32

4.4

All solutions of Ax = b

Finally, we have come full circle: having motivated linear algebra from its two historical roots (analytic geometry and linear equations) in Section 0.3, we come back to
linear equations. We are now in a position to precisely understand (and compute) the
set of all solutions of a system of linear equations Ax = b. If there is some solution,
all solutions can be obtained by suitably shifting the nullspace of A away from the
origin. We also understand the kinds of systems that (typically) have a solution, and
the ones that do not.
Using Gauss elimination and back substitution (Sections 3.2.2 and 3.2.1), we have seen
how to compute the unique solution x of Ax = b when A is an invertible square matrix.
In the general case, we can apply Gauss-Jordan elimination and direct solution (Sections 3.3.3 and 3.3.2) to either find the canonical solution, or to conclude that there is no
solution. Now, we want to understand and compute the space of all solutions, so let us
define this first.
Definition 4.37 (Solution space of a system of linear equations). Let A be an m × n matrix
and b ∈ Rm . The set
Sol(A, b) := {x ∈ Rn : Ax = b} ⊆ Rn
is the solution space of Ax = b.
If b ̸= 0, Sol(A, b) is unfortunately not a subspace of Rn , simply because it does
not contain the zero vector (see Lemma 4.9). Luckily, the vector space theory we have
developed in this chapter still applies, but with a little twist.
Let us look at a concrete example, a system with one equation in two variables:
2x + 3y = 6.
The solutions, all pairs (x, y) satisfying 2x + 3y = 6, form a line in R2 , not containing the
origin; see the top line in Figure 4.3.

y
2x + 3y = 6
2x + 3y = 0
x





Figure 4.3: Sol(A, b) for A = 2 3 , b = 6 (upper line) and b = 0 (lower line).
154

Replacing the right-hand side 6 by 0 results in another line which
 contains the origin
and is actually a subspace: the nullspace of the 1 × 2 matrix 2 3 . We
to
 know
 how

compute
see
4.3.3. In this example, we

nullspaces,

 Section

 see that Sol( 2 3 , 6 ) and
Sol( 2 3 , 0 ) = N( 2 3 ) are parallel lines. To get Sol( 2 3 , 6 ) from N( 2 3 ), we
simply have to “shift” the nullspace “away from the origin”.
This picture generalizes: For b ̸= 0, and if there is a solution at all, Sol(A, b) can be
obtained by shifting the nullspace of A by an arbitrary solution of Ax = b.
Theorem 4.38 (Solution space from shifting the nullspace). Let A be an m × n matrix, b ∈
Rm . Let s be some solution of Ax = b. Then
Sol(A, b) = {s + x : x ∈ N(A)}.
Hence, we can also compute Sol(A, b), despite the fact that it is not a subspace. To
describe all solutions, we just need some solution s (for example the canonical one from
Section 3.3.2) and a basis B = {v1 , v2 , . . . , vn−r } of N(A) (see Theorem 4.36). Then
(
)
n−r
X
Sol(A, b) = s +
λi vi : λi ∈ R for i ∈ [n − r] .
i=1

Sol(A, b) can be a point, a line, a plane, . . . :
y
y
Sol(A, b)
s
Sol(A, b)
s

x

v1

N(A)
z

y
Sol(A, b)
s
v1

x

v1

x

N(A)
z

z

N(A)

From these pictures, you can also try to get an intuition why the result of shifting N(A)
does not depend on the particular solution s that we choose.
Proof of Theorem 4.38. We first show that every solution y ∈ Sol(A, b) is of the form s + x
with x ∈ N(A). For this, we write y as
y = s + (y − s),
| {z }
x

and due to Ax = A(y − s) = Ay − As = b − b = 0 we have x ∈ N(A). The second
equality in this chain uses linearity of matrix transformations, see Lemma 2.19. For the
other direction, we show that every vector y of the form y = s + x with x ∈ N(A) is in
Sol(A, b). For this, we compute
Ay = A(s + x) = As + Ax = b + 0 = b.

155

4.4.1

Affine subspaces

Generally, a shifted copy of a subspace in a vector space is called an affine subspace. In the
context of affine subspaces, a “normal” subspace is also sometimes called linear subspace.
There is a full theory of affine spaces which are essentially vector spaces without an origin.
The elements of an affine space are called points. You can think of them as locations
in space whose coordinates are “absolute” and not defined relative to a distinguished
origin 0. Natural point behavior is to move (p + v), where v is a vector from the vector
space “behind” the affine space. This also allows to subtract points (p − q), resulting in
a difference vector. But unlike vectors, points do not add up or scale. In summary, affine
spaces are natural habitats for points.3

4.4.2

The number of solutions

A system of linear equations has two characteristic numbers: m, the number of equations,
and n, the number of variables. But if we want to understand the solution space, there
is a third important characteristic number: r, the rank of the coefficient matrix A. This is
revealed by Gauss-Jordan elimination on A: r is the number of “downward steps” in the
reduced row echelon form of the resulting matrix R; see Theorem 4.31.
The number of solutions of Ax = b can be 0 (the system is unsolvable), 1 (there is
a unique solution), or infinite. In the latter case, we would like to be more precise and
know the dimension of the solution space. We have already done all the work for this
and summarize it in the following result.
Theorem 4.39 (Dimension of the solution space). Let A be an m × n matrix of rank r. If
Ax = b has a solution, then Sol(A, b) has dimension n−r, where dim(Sol(A, b)) := dim(N(A)).
If dim(Sol(A, b)) = 0, 1, 2, . . ., the solution space is a point, a line, a plane,. . .
Proof. If there is a solution at all, the solution space is obtained by shifting the nullspace
N(A) away from the origin by some solution s (Theorem 4.38). We declare Sol(A, b) to be
of the same dimension as the nullspace, and this dimension is n − r by Theorem 4.36.
Theorem 4.39 covers all cases in which there is a solution. Next, we want to understand in more detail in which cases this happens. As we will see, this typically depends
on whether r = m or r < m. The case r > m cannot happen, see Corollary 4.34.
Theorem 4.40 (Systems of rank m are solvable). Let A be an m × n matrix of rank r = m.
Then Ax = b has a solution for every b ∈ Rm .
Proof. From Theorem 4.31, we know that dim(C(A)) = r = m, so C(A) is a subspace of
Rm of the same dimension m. Then we must have C(A) = Rm . This seems obvious,
but actually needs an argument. The argument is that every basis B of C(A) contains m
linearly independent vectors in Rm , and these already span Rm by Lemma 1.28.
3

https://en.wikipedia.org/wiki/Affine_space, accessed September 2, 2025

156

It follows that every b ∈ Rm is in C(A), and this is the same as saying that the system
Ax = b has a solution for every b ∈ Rm .
If r < m, the column space C(A) has dimension smaller than m. Then Ax = b is “typically” unsolvable, because “almost all” b are not in C(A). For an illustration, consider
Figure 2.2 (right) where the column space is a line in R2 . If we pick a “typical” b from R2 ,
we expect b to be outside of that line. This can properly be formalized: a subspace of Rm
of dimension smaller than m is a set of measure 0. A “typical” vector (a vector randomly
chosen from a continuous probability distribution over Rm ) has probability 0 to land in a
set of measure 0. We do not go the formal route here and summarize the discussion as
follows.
Informal Theorem 4.41 (Systems of rank less than m are typically unsolvable). Let A be
an m × n matrix of rank r < m. For a “typical” b, the system Ax = b has no solution.
It may happen that in a given application, we have an “untypical” right-hand side
that leads to a solvable system, despite r < m. The previous informal theorem is a rule of
thumb about what to expect without knowing anything specific about the system.
Depending on the shape of the coefficient matrix A (see Figure 2.1), systems of linear
equations come in three categories.
Definition 4.42 (Square, underdetermined, overdetermined system). Let A be an m × n
matrix, b ∈ Rm .
(i) If m = n (A is a square matrix), the system Ax = b is called square.
(ii) If m < n (A is a wide matrix), the system Ax = b is called underdetermined.
(iii) If m > n (A is a tall matrix), the system Ax = b is called overdetermined.
The notion underdetermined comes from the fact that there are more variables than
equations, in which case it is not possible to uniquely “nail down” the values of the variables from the information given (the equations). Indeed, in the underdetermined case,
the nullspace N(A) has dimension n − r > m − r ≥ 0 (see Theorem 4.36 for the dimension
and Corollary 4.34 for m − r ≥ 0), so it is at least a line. Hence, if there is a solution at all,
there are infinitely many (see Theorem 4.39).
In contrast, an overdetermined system has more equations than variables. The notion
indicates that such a system contains more information about the variables than necessary. Indeed, we know that having as many equations as variables (a square system) is in
principle sufficient to determine the values of the variables uniquely: all we need is that
r = m = n (see Theorem 4.40 together with Theorem 4.39).
We conclude with two more informal theorems.
Informal Theorem 4.43 (Overdetermined systems are typically unsolvable). Let A be a tall
matrix. For a “typical” b, the overdetermined system Ax = b has no solution.

157

Proof. We have r ≤ n (Corollary 4.34) and n < m since A is tall. Hence, r < m, so there is
“typically” no solution by Informal Theorem 4.41.
For a square or an underdetermined system, we want to say that it is “typically” solvable. But for this, we have to consider a “typical” matrix A instead of a “typical” righthand side b. For example, if A = 0, the system is unsolvable for “typical” b (the only
solvable case is b = 0), but if A is “typical”, the system turns out to be solvable for all b.
So here, solvability strongly depends on the “typicality” of A.
Informal Theorem 4.44 (Square and Underdetermined systems are typically solvable).
Let b ∈ Rm , m ≤ n. For a “typical” matrix A ∈ Rm×n , the square or underdetermined system
Ax = b has a solution.
For this, we use that “typical” matrices with m ≤ n have rank r = m, so Theorem 4.40
applies. The formal argument is that matrices with r < m are a set of measure 0 (in the
space of m × n matrices with m ≤ n). To make this argument, we need determinants that
will only be introduced in the second part of the course. But we can get a glimpse of this
by looking at the 2 × 2 case. We can argue that a “typical” 2 × 2 matrix


a b
A=
c d
has rank 2. To (informally) see this, recall that the rank is the number of independent
columns; see Definition 2.10. The columns are two “typical” vectors
   
a
b
,
c
d
in R2 , and we get rank smaller than 2 only when they are linearly dependent, i.e. point
into the same (or opposite) direction. But this is “untypical”.

158

Bibliography
[BP98]

Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30(1):107–
117, 1998. https://www.sciencedirect.com/science/article/pii/
S016975529800110X.

[CLRS22] Cormen, Thomas H., Leiserson, Charles E., Rivest, Ronald L., and
Stein, Clifford.
Introduction to Algorithms, Fourth Edition.
The MIT
Press,
2022.
https://mitpress.mit.edu/9780262046305/
introduction-to-algorithms/.
[CSB06]

V. Claus, A. Schwill, and R. Böving. Duden Informatik A - Z: Fachlexikon für
Studium, Ausbildung und Beruf, 4. Auflage. Dudenverlag, 2006.

[Fel93]

Michael R. Fellows. Computer science and mathematics in the elementary
schools. In Harvey B. Keynes Naomi Fisher and Philip D. Wagreich, editors, Mathematicians and Education Reform 1990–1991, volume 3 of CBMS Issues in Mathematics Education. American Mathematical Society, 1993. https:
//ianparberry.com/research/cseducation/fellows1991.pdf.

[Str23]

Gilbert Strang. Introduction to Linear Algebra (Sixth Edition).
Cambridge Press, 2023.

159

Wellesley-

Index
∅, 37
∞-norm, 28
0, 13
1, 30
m-tuples, 12
1-norm, 28
absolute value, 25
abuse of notation, 13
affine combination, 19
affine space, 156
affine subspace, 156
AI chips, 45
Al-Khwarizmi, 7
algebra, 7, 93
algorithm, 7
analytic geometry, 8
applicable, 49
apply, 35
arrows, 11
assignment symbol, 100
associative, 15
associativity, 75
axiom, 63
axioms of a subspace, 130
axioms of a vector space, 127
back substitution, 99
base case, 66
basis, 126, 134, 137
below the staircase, 115
big-O notation, 109
bijective, 86
bits, 127

canonical, 117, 128
canonical basis, 138
Carl Friedrich Gauss, 66
Cartesian coordinate system, 12
Cartesian coordinates, 9
category theory, 145
citations, 96
codomain, 35
coefficient matrix, 95
coefficients, 128
collinear, 36, 40
column notation, 46
column rank, 52
column space, 51
column vector, 12
commutative, 15
commutative diagram, 59
complex numbers, 127
composition, 71
compression, 84
computer science, 7
computing, 147
condition, 36
cone, 20
conic combination, 19
constant time, 109
continuous probability distribution, 157
contraposition, 108, 147
convex combination, 19, 60
coordinate vector, 12, 147
coordinate-wise, 14
coordinates, 13
coplanar, 41

160

corollary, 39
covector, 34, 35
covector-matrix multiplication, 76
Cramer’s rule, 91
damping factor, 97
defining property, 63
definition, 13
degree, 128
degrees of freedom, 142, 147
dependent, 52
determinants, 91, 158
diagonal, 48, 54
difference vector, 156
dimension, 126
distributivity, 24, 75
domain, 35
domino effect, 66
dual space, 35
dynamic programming, 80
elimination, 100
elimination matrix, 101
empty set, 37
equivalent, 33, 38
Euclidean norm, 25
finitely generated, 139
floating-point numbers, 113
follows, 39
function, 35
function value, 35
fundamental subspaces, 148
Gauss elimination, 98
generalized associativity, 76
Hadamard product, 23
hyperplane, 32
identity, 48, 64
identity function, 86
if, 38
if and only if, 38
ill-formed, 81

image, 68
implies, 38
independent, 52, 55
independent column, 51
index, 12
induction hypothesis, 66
induction step, 66
Informatik, 7
information, 7
injective, 86
interval, 31
invariance, 104
inverse, 86, 90
invertible, 90
isomorphic, 133, 144, 146
isomorphism, 126, 146
kernel, 68
Kronecker delta, 48
lemma, 29
limited induction, 67
line segment, 20
linear combination, 16, 135
linear equations, 8
linear form, 35
linear functional, 63
linear subspace, 156
linear transformation, 63
linear transformation between vector spaces,
143
linear transformations, 45
linearity, 58
linearity in both arguments, 24
linearly dependent, 36, 136
linearly independent, 36, 136
link graph, 96
loop invariants, 140
lower triangular, 49
LU decomposition, 113
LUP decomposition, 113
maps, 35
matrix, 46
161

matrix transformation, 58
matrix transposition, 54
Matrix-vector multiplication, 49
matrix-vector product, 49
natural numbers, 12
negation, 108
negations, 39
non-invertible, 90
non-singular, 90
nontrivial linear combination, 38
nullspace, 57
numerical mathematics, 113
observation, 24
only if, 38
origin, 12
orthogonal, 32
outer product, 76, 78
overdetermined, 157
overloading, 128
parallel, 63
parallel projection, 61, 152
parallelogram, 14
permutation matrix, 103
perspective projection, 61
pivot, 102
points, 156
polygon, 59
polynomial without constant term, 133
positive-definiteness, 24
predatory publishers, 96
preprocessing, 125
private nonzero, 137
product, 49, 72, 78
projection, 61
proof by contradiction, 87
proof by induction, 66
pseudocode, 100
Pythagorean theorem, 25
quadratic equations, 93
quadratic polynomial, 133

rank, 52
real numbers, 12
real polynomials, 128
real vector space, 127
recursive algorithm, 140
reduced row echelon form, 114, 115
replay, 113
right-hand side, 95
rotation, 60
row addition, 102
row division, 119
row divisions, 114
row exchange, 102
row notation, 46
row operations, 99, 103
row rank, 52, 55
row rank equals columns rank, 150
row space, 55
row subtraction, 101
row vector, 34, 35
scalar, 15
scalar division, 27
scalar multiple, 15, 47
scalar multiplication, 15
scalar product, 24
scaling, 60
school book addition, 7
sequences, 12
set, 22
set builder notation, 20, 22
set of measure 0, 157
shear, 60
size of a set, 25
solution space, 154
span, 40, 136
special solutions, 151, 153
square, 157
square matrix, 47
standard basis, 137, 153
standard form, 84, 114, 123
standard unit vectors, 27
Steinitz exchange lemma, 44, 140
162

stretching, 60
stretching factors, 60
submatrix, 53
subset, 41
subspace, 126, 130
sum, 14, 47
summation index, 21
surjective, 86
symmetric, 49
symmetry, 24
system of linear equations, 94
taking out scalars, 24
tall matrices, 48
three fundamental subspaces, 126
trace, 134
transpose, 34, 35, 54
trivial linear combination, 39
trivial solution, 97
underdetermined, 157
unit circle, 26
unit cube, 61
unit monomials, 136
unit vector, 26
upper triangular, 48, 99
variables in a programming language, 100
vector, 12
vector addition, 14
vector of variables, 95
vector space, 126, 127
weighted, 97
wide matrices, 48
zero matrix, 47
zero polynomial, 128
zero vector, 13

163

