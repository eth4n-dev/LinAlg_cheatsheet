Linear Algebra for
Computer Scientists
Lecture Notes
Part II

A. S. Bandeira and R. Weismantel
ETH Zürich

Last update on October 29, 2025
1

2

“R EAD ME ” FOR PART II
These lecture notes serve as a continuation of Part I, taught by Prof. Bernd Gärtner. Please read the Preface
there. Please note there may be some changes in notation.
For Part II we will roughly continue to follow, in structure and content, [Str23], with several deviations,
for instance, the detailed description of the scenarios that are possible when one considers a system of
equations.
Linear Algebra is a beautiful topic. It connects Algebra with Geometry, as you will likely see later in
your academic life. For example, when Joseph Fourier invented Fourier Series to develop a theory of heat
transfer he was essentially finding good orthonormal bases for functions. Linear Algebra has countless
applications making it a key component/ technology of almost all quantitative pursuits. We sincerely hope
you will enjoy the course as much as we enjoy teaching this beautiful subject!
As your mathematical level matures over the semester, the notes will have less illustrations and examples,
but more definitions and mathematical statements. We will put a focus on the algebraic derivation of the
important results. The geometry and additional examples will be shown in the lecture. Our recommendation is to read the notes with pen & paper next to you and to draw the picture yourself. This “translation”
you will be doing — from mathematical statement to picture — will help you greatly in learning the
material.

There are also countless high-quality videos and other content online about Linear Algebra, for example there is also an excellent series of videos by Gil Strang filmed ∼15 years ago: https://www.
youtube.com/playlist?list=PLE7DDD91010BC51F8.
Strang actually retired just a few years ago, at almost 90 years of age! You can see his last lecture online:
https://www.youtube.com/watch?v=lUUte2o2Sn8

Moreover, there are many excellent animations online giving lots of great intuition on several Linear
Algebra topics and phenomena. While it is a great idea to take advantage of this, we would recommend
first trying yourself to develop an intuition of the concept/phenomenon (e.g. by drawing a picture) and
using these tools only after — use them to improve your intuition, not to create it!

As these Lecture Notes are being continuously updated, and sometimes the discussion in lectures leads us
into proving an extra result, or suggests a remark, etc, we will try to add them in. This might change the
numbering of things downstream.

3

C ONTENTS
“Read me” for Part II

2

5.

4

Orthogonality and Projections

5.1.

Orthogonality of vectors and subspaces

4

5.2.

Projections

8

6.

Applications of orthogonality and projections

12

6.1.

Least Squares Approximation

12

6.2.

The set of all solutions to a system of linear equations

15

6.3.

Orthonormal Bases and Gram Schmidt

19

6.4.

The Pseudoinverse, also known as Moore–Penrose Inverse

23

7.

The determinant

27

7.1.

2 × 2 - matrices

27

7.2.

The general case

30

7.3.

Cofactors, Cramer’s rule and beyond

33

8.

Eigenvalues and Eigenvectors

36

8.1.

Complex Numbers

36

8.2.

Introduction to Eigenvalues and Eigenvectors

38

8.3.

Properties of eigenvalues and eigenvectors

43

9.

Diagonizable matrices and the singular value decomposition

46

9.1.

From eigenvalues to diagonizable matrices

47

9.2.

Symmetric Matrices and the Spectral Theorem

51

9.3.

Singular Value Decomposition

57

10.

Beyond the topics we discussed

59

Appendix A.

Some Important Preliminaries and Remarks on Notation

62

Appendix B.

A “Simple proof” of the Fundamental Theorem of Algebra

62

References

63

4

5. O RTHOGONALITY AND P ROJECTIONS
5.1. Orthogonality of vectors and subspaces.
Let us begin by describing shortly the rational of studying orthogonality. Our task is to explore orthogonality as a geometric and algebraic tool in order to be able to decompose a space into subspaces. Among many
results our knowledge will then put us in position to understand how to solve systems of linear equations.
We begin by introducing orthogonality.
Definition 5.1.1. Two vectors v, w ∈ Rn are called orthogonal if vT w = ∑ni=1 vi wi = 0. Two subspaces V
and W are orthogonal if for all v ∈ V and w ∈ W , the vectors v and w are orthogonal.
As an example, consider a vector (a, b)T ∈ R2 with entries a, b ∈ R. Then (−b, a)T is orthogonal to (a, b)T .
By direct calculation one can also see that the two subspaces
V = {λ (1, 2, 3)T | λ ∈ R} ⊆ R3 and W = {µ(1, −2, 1)T | µ ∈ R}
are orthogonal. Indeed, it is enough here to verify that (1, 2, 3)T and (1, −2, 1)T are orthogonal. This is
a general phenomenon. In order to check whether two subspaces V and W are orthogonal it is enough to
verify it for the vectors forming a basis of V and W , respectively.

Lemma 5.1.2. Let v1 , . . . , vk be a basis of subspace V . Let w1 , . . . , wl be a basis of subspace W . V and
W are orthogonal if and only if vi and w j are orthogonal for all i ∈ {1, . . . , k} and j ∈ {1, . . . , l}.

Proof. Let us begin by proving the statement from left to right: Suppose V and W are orthogonal. Since
vi ∈ V for all i ∈ {1, . . . , k} and w j ∈ W for all j ∈ {1, . . . , l}, we have that vTi w j = 0 for all i ∈ {1, . . . , k}
and j ∈ {1, . . . , l}.
For the converse direction, assume that vTi w j = 0 for all i ∈ {1, . . . , k} and j ∈ {1, . . . , l}. Let v ∈ V and
w ∈ W . Then, there exist real multipliers such that
k

l

v = ∑ λi vi and w = ∑ µ j v j .
i=1

j=1

Then
k

k

l

vT w = ∑ λi vTi w = ∑ ∑ µ j λi vTi w j = 0.
i=1

i=1 j=1

□

One can say more about the structure of orthogonal subspaces. One such central statement is presented
next.

5

Lemma 5.1.3. Let V and W be two orthogonal subspaces of Rn . Let v1 , . . . , vk be a basis of subspace
V . Let w1 , . . . , wl be a basis of subspace W . The set of vectors {v1 , . . . , vk , w1 , . . . , wl } are linearly
independent.

Proof. Consider the linear combination
k

(∗)

l

∑ λi vi + ∑ µ j w j = 0.

i=1

j=1

We want to show that λi = 0 for all i ∈ {1, . . . , k} and µ j = 0 for all j ∈ {1, . . . , l}.
Let v = ∑ki=1 λi vi . The relation (∗) is equivalent to v = − ∑lj=1 µ j w j . We obtain
l

vT v = − ∑ µ j vT w j = 0.
j=1

Hence, v = 0 and with this relation (*) turns into ∑lj=1 µ j w j = 0.
This implies that λi = 0 and µ j = 0 for all i ∈ {1, . . . , k}, j ∈ {1, . . . , l} since v1 , . . . , vk and w1 , . . . , wl are
linearly independent vectors.
□
Lemma 5.1.3 allows us to derive an important fact about orthogonal subspaces. Namely we can take bases
of the two subspaces V and W and their union gives a basis for the subspace
V +W = {λ v + µw | λ , µ ∈ R, v ∈ V, w ∈ W }.
Note that if V and W are subspaces of Rn , then V +W is indeed also a subspace of Rn . You should by now
be able to prove this result.
The proof of Lemma 5.1.3 showed that if a vector v is in the intersection of two orthogonal subspaces,
then v = 0. This gives the following statement.

Corollary 5.1.4. Let V and W be orthogonal subspaces. Then V ∩W = {0}.
Moreover, if dim(V ) = k and dim(W ) = l, then dim(V +W ) = k + l ≤ n.

So far we have explored general subspaces V and W that are orthogonal. Next consider a subspace V .
Then there is a special orthogonal subspace attached to V .
Definition 5.1.5. Let V be a subspace of Rn . We define the orthogonal complement of V as
V ⊥ = {w ∈ Rn | wT v = 0 for all v ∈ V }.

6

It is important to know that V ⊥ is a subspace of Rn . It might again be a good exercise to verify this fact.
The concept of orthogonal subspaces allows us to decompose the space Rn . An important example of this
idea is to take a matrix A ∈ Rm×n and decompose Rn into two orthogonal subspaces: the nullspace of A
and the column space of AT .

Theorem 5.1.6. Let A ∈ Rm×n be a matrix.
N(A) = C(AT )⊥ = R(A)⊥ .

Proof. Let us first show that N(A) ⊆ C(AT )⊥ .
Let x ∈ N(A). Take any b ∈ C(AT ). By definition, b = AT y for some y ∈ Rm . Then bT x = yT Ax = 0.
Hence, x ∈ C(AT )⊥ .
Conversely, we want to show that C(AT )⊥ ⊆ N(A). To this end let x ∈ C(AT )⊥ . Then bT x = 0 for all
b ∈ C(AT ). Take y := Ax ∈ Rm Then b := AT y ∈ C(AT ) and hence, xT b = 0. We obtain
0 = xT b = xT AT y = xT AT Ax = ∥Ax∥2 .
This implies that Ax = 0, i.e., x ∈ N(A).

□

From the lecture in Chapter 3.5 we know already that if r = dim(R(A)), then n − r = dim(N(A). This fact
together with Theorem 5.1.6 allows us to prove a general decomposition theorem of the space Rn .
Theorem 5.1.7. Let V,W be orthogonal subspaces of Rn .
The following statements are equivalent.
(i) W = V ⊥ .
(ii) dim(V ) + dim(W ) = n.
(iii) Every u ∈ Rn can be written as u = v + w with unique vectors v ∈ V , w ∈ W .

Proof. Let v1 , . . . , vk be a basis of V and w1 , . . . , wl a basis of W . From Lemma 5.1.2 V and W are orthogonal if and only if vTi w j = 0 for all i ∈ {1, . . . , k} and j ∈ {1, . . . , l}.
(i) implies (ii): Define A ∈ Rk×n to be the matrix with row vectors v1 , . . . , vk . Then V = R(A) = C(AT ).
Moreover, W = V ⊥ = N(A) from Theorem 5.1.6. From our remark before dim(V ) = k and hence,
dim(W ) = n − k.
(ii) implies (iii): From Lemma 5.1.3 the vectors in the set {v1 , . . . , vk , w1 , . . . , wl } are linearly independent. Since by assumption l = n − k, this set is a basis of Rn . Hence, every vector u ∈ Rn has a unique

7

representation in form of
k

l

u = ∑ λi vi + ∑ µ j w j , where λ1 , . . . , λk , µ1 , . . . , µl ∈ R.
i=1

j=1

Define the unique vectors v := ∑ki=1 λi vi , w := ∑lj=1 µ j w j . This gives the statement.
(iii) implies (i): We need to show that W = V ⊥ . Note that W ⊆ V ⊥ since W is orthogonal to V . To show the
reverse inclusion, take any vector u ∈ V ⊥ ⊆ Rn Hence, from our assumption in (iii) we know that u = v + w
where v ∈ V and w ∈ W . Then
0 = uT v = vT v + vT w = vT v = ∥v∥2 .
Hence, v = 0 and it follows that u = w ∈ W .

□

Indeed Theorem 5.1.7 allows us to decompose the space Rn according to a given subspace V ⊆ Rn . We
write
Rn = V +V ⊥ = {v + w | v ∈ V, w ∈ V ⊥ }.
This decomposition is symmetric in the sense that we can also take the subspace V ⊥ and then write
Rn = V ⊥ + (V ⊥ )⊥ = V ⊥ +V.
From Theorem 5.1.7 it follows that V = (V ⊥ )⊥ . We emphasize this important fact.
Lemma 5.1.8. Let V be a subspace of Rn . Then V = (V ⊥ )⊥ .

This lemma in combination with Theorem 5.1.6 allows us to conclude that for a matrix A we have that
C(AT ) = N(A)⊥ .
Corollary 5.1.9. Let A ∈ Rm×n . N(A) = C(AT )⊥ and C(AT ) = N(A)⊥ .

There is one final link between the nullspace of a matrix A and the nullspace of the matrix AT A that we
will need in our analysis of projections later on.
Lemma 5.1.10. Let A ∈ Rm×n . Then N(A) = N(AT A) and C(AT ) = C(AT A).

Proof. If x ∈ N(A) then Ax = 0 and so A⊤ Ax = 0, thus x ∈ N(A⊤ A). The other implication is more
interesting.
If x ∈ N(A⊤ A) then A⊤ Ax = 0. This implies
0 = x⊤ 0 = x⊤ A⊤ Ax = (Ax)⊤ (Ax) = ∥Ax∥2 .

8

This shows that Ax must be a vector of norm zero which implies that Ax = 0. Hence, x ∈ N(A).
For the second statement we utilize Corollary 5.1.9. We have
C(AT ) = N(A)⊥ = N(AT A)⊥ = C((AT A)T ) = C(AT A).
2

5.2. Projections.
The study of this topic is devoted to develop and answer the following very natural question. Given a
system of linear equations that has no solution, how do we find a “solution” that has the smallest error?
This question is central in countless applications. It is in a sense what Machine Learning is all about. Let
us make precise what we mean by projections of vectors onto a subspace.
Definition 5.2.1 (Projection of a vector onto a subspace). The projection of a vector b ∈ Rm on a subspace
S (of Rm ) is the point in S that is closest to b. In other words
(1)

projS (b) = argmin ∥b − p∥.
p∈S

Note that this is only a proper definition if the minimum exists and is unique. This will be clarified below.
Let us build us some intuition by starting with one-dimensional subspaces.
5.2.1. The one-dimensional case.
Let S be the subspace corresponding to the line that goes through the vector a ∈ Rm \ {0}, i.e. S = {λ a |
λ ∈ R} = C(a). By drawing a two dimensional example one can see that the projection p is the vector in
the subspace S such that the “error vector” e = b − p is perpendicular to a (i.e. b − p ⊥ a). This geometric
intuition turns out to be correct. We will verify it later algebraically.

F IGURE 1. Projection on a line.

9

Lemma 5.2.2. Let a ∈ Rm \ {0}. The projection of b ∈ Rm on S = {λ a | λ ∈ R} = C(a) is given by
projS (b) =

aaT
b.
aT a

Proof. Let p ∈ S, p = λ a for λ ∈ R. Then
∥b − p∥2 = (b − p)T (b − p) = bT b − 2bT p + pT p = ∥b∥2 − 2λ bT a + λ 2 ∥a∥2 = g(λ ).
g is a convex, quadratic function in one variable λ . Hence, the minimizer is obtained at the point λ ∗ where
the derivative vanishes. We obtain
g′ (λ ) = −2bT a + 2λ ∥a∥2 = 0 ⇐⇒ λ ∗ =

bT a
.
aT a

Hence, we have shown that
bT a
aT b aaT
projS (b) = λ ∗ a = a T = a T = T b.
a a
a a a a
□

In particular, the proof of Lemma 5.2.2 shows us that the minimizer projS (b) exists and is unique.
Let us next verify that our initial geometric understanding is indeed correct: the projection p should be the
vector in the subspace S such that the “error vector” e = b − p is perpendicular to a, i.e.,
(b − projS (b)) ⊥ projS (b).
Indeed by substituting what we just computed we obtain
(b −

T
T
aaT T aaT
bT aaT b
(aT b)2 bT aaT b
T aa aa
b)
b
=
−
b
b
=
−
= 0.
aT a
aT a
aT a
aT a aT a
aT a
aT a

The projection of a vector b = λ a on the subspace C(a) should be the point b. This follows from our
formula:
1
projS (b) = projS (λ a) = T λ aaT a = λ a = b.
a a

5.2.2. The general case.
For general subspaces the idea is precisely the same as with dimension one. Let S be a subspace in Rm .
Let a1 , . . . , an be vectors in S that span S, i.e.,
S = span(a1 , . . . , an ) = C(A) = {Aλ | λ ∈ Rn }, where
A is the matrix with column vectors a1 , . . . , an .

10

Lemma 5.2.3. The projection of a vector b ∈ Rm to the subspace S = C(A) is well defined. It can be
written as
projS (b) = Ax̂, where x̂ satisfies the normal equations AT Ax̂ = AT b.

Proof. Let b ∈ Rm . The vector b can be written as b = p∗ + e where p∗ ∈ S and e ∈ S⊥ , i.e., (p∗ )T e = 0.
Now consider another point p′ ∈ S. Then p∗ − p′ ∈ S and hence, eT (p∗ − p′ ) = 0. This gives
∥p′ − b∥2 = ∥p′ − p∗ + p∗ − b∥2 = ∥p′ − p∗ − e∥2 = ∥p′ − p∗ ∥2 + ∥e∥2 ≥ ∥e∥2 = ∥p∗ − b∥2 .
This shows that
p∗ = projS (b) = argmin ∥b − p∥

(2)

p∈S

is well defined because in our chain of relations before we have that ∥p′ − b∥2 > ∥p∗ − b∥2 if and only if
p∗ ̸= p′ . Hence, we have shown that p∗ = projS (b) = Ax̂ ∈ S where b = p∗ + e with e ∈ S⊥ . This shows us
that
(b − projS (b)) ⊥ ai for all i = 1, . . . , n ⇐⇒ aTi (b − projS (b)) = 0 for all i = 1, . . . , n.
This is equivalent to saying that
AT (b − projS (b)) = 0 ⇐⇒ AT (b − Ax̂) = 0 ⇐⇒ AT Ax̂ = AT b.
□
If we can show that A⊤ A is invertible then we would have p = Ax̂ = A A⊤ A
to show that it is indeed invertible.

−1

A⊤ b. Let’s make a detour

Lemma 5.2.4. A⊤ A is invertible if and only if A has linearly independent columns.

Proof. This follows essentially from Lemma 5.1.10 where we convinced ourselves that A⊤ A and A have
the same nullspace. This is enough because A⊤ A is a square matrix and it is invertible if and only if its
nullspace only has the 0 vector. On the other hand, A has linearly independent columns if and only if its
nullspace only has the 0 vector.
2
From now on we can apply the fact that if A has linearly independent columns then A⊤ A is a square matrix
that is symmetric and invertible. This will be a key ingedient in deriving a formula for projections: Since
the columns of A are a basis they are linearly independent and so A⊤ A is indeed invertible. We just proved
the following.

11

Theorem 5.2.5. Let S be a subspace in Rm and A a matrix whose columns are a basis of S. The
projection of b ∈ Rm to S is given by
projS (b) = Pb,

−1
where P = A A⊤ A
A⊤ is the projection matrix.

−1 ⊤
The matrix P = A A⊤ A
A is known as a Projection Matrix. It maps any vector b ∈ Rm to its projection
⊤
Pb on a subspace S. For the case of lines, P was given by P = aaa⊤ a = a a⊤1 a a⊤ .
The matrices A (and A⊤ ) are not necessarily square, and so they do not have inverses in general. The
−1 ⊤
−1
expression A A⊤ A
A cannot be simplified by expanding A⊤ A
(which would yield I = P, this
m
would only make sense if S was all of R and note that, unsurprisingly, this would correspond exactly to
the case when A is invertible).
Let us summarize a few facts about the projection matrix P. P can be viewed as a mapping or as an
operator: once we constructed a projection matrix P we can apply it for many vectors b to obtain their
projection given by projS (b) = Pb.
Remark 5.2.6.
• If b ∈ Rm , then projS (projs (b)) = projS (b) by definition. This requires us to have that PPb = Pb,
i.e., we should have P2 = P. Indeed
 
−1 2

−1

−1

−1
2
⊤
⊤
⊤
⊤
⊤
⊤
⊤
P = A A A
A
=A A A
A A A A
A =A A A
A⊤ = P.
• Let S⊥ be the orthogonal complement of S and P the projection matrix onto the subspace S, i.e.,
projS (b) = Pb. Then I − P is the projection matrix that maps b ∈ Rm to projS⊥ (b). This follows
since b = e + projS (b) = e + Pb where e ∈ S⊥ . Hence,
(I − P)b = b − Pb = e = projS⊥ (b).
• Note that – as it should be – we have that (I − P)2 = I − 2P + P2 = I − P.
We have so far seen that a single point can be projected to a subspace. This is only a very special case.
Indeed, one can project sets onto sets in smaller dimension in more generality. This topic is absolutely
fundamental and has numerous applications in many areas of mathematics and beyond. The reason for this
is that projections are a way to reduce the dimension of the initial question about feasibility of a set in Rn
to a question about feasibility of a set in smaller dimension. We will not consider this topic any further,
but end our discussions with a definition of a projection operation of a set onto a subspace generated by
unit vectors.
Let Q ⊆ Rn . Let S = {1, . . . , s}. The projection of Q on the subspace Rs associated with the variables in
the subset S is
projS (Q) := {x ∈ Rs | ∃y ∈ Rn−s such that (x, y) ∈ Q}.

12

6. A PPLICATIONS OF ORTHOGONALITY AND PROJECTIONS
The concept of orthogonality of vectors or subspaces and related to it the idea of projecting points to
subspaces is the basis of understanding many topics in linear algebra and beyond. We start with a direct
application of the concept of projections to least squares approximation.
6.1. Least Squares Approximation.
In this section we consider a second application of the concept of orthogonality and projections of vectors.
We go back to the guiding question of what to do when we want to “solve” a linear system that does not
have an exact solution. More precisely let A ∈ Rm×n and b ∈ Rm . Suppose we are given a linear system
Ax = b,
for which no solution x exists, for example, if there are too many equations, which could happen if m > n.
See also Example 6.2.5 for such an example.
A natural approach is to try to find x for which Ax is as close as possible to b
(3)

min ∥Ax̂ − b∥2 .

x̂∈Rn

This seemingly simple observation is key to countless technologies. Measurement systems often have
errors and so it is impossible to find the target object/signal x that satisfies them all exactly, and we look
for the one that satisfies them the best possible. In Data Science and Learning Theory we often want to find
a predictor that best describes a set of training data, but usually no predictor described the data exactly, so
we look for the best possible, etc etc. We will see a couple of applications later.
We can solve this problem using the ideas we developed above. What we are looking for is a vector x̂ for
which the error e = b − Ax̂ is as small as possible. Since the set of possible vectors y = Ax̂ is exactly C(A),
Ax̂ is precisely the projection of b on C(A). As we discussed before, this means that
A⊤ (b − Ax̂) = 0.
These are known as the normal equations and can be rewritten as
(4)

A⊤ Ax̂ = A⊤ b.

Recall that we had shown in Lemma 5.1.10 that for any matrix A, C(A⊤ ) = C(A⊤ A). Hence, the system
(4) always has a solution. We also know that if A has linearly independent columns, then A⊤ A is invertible
and so we can write x̂ = (A⊤ A)−1 A⊤ b.

Fact 6.1.1. A minimizer of (3) is also a solution of (4). When A has independent columns the unique
minimizer x̂ of (3) is given by
(5)

x̂ = (A⊤ A)−1 A⊤ b

13

One of the most common tasks in data analysis is linear regression, to fit a line through data points. Let us
consider data points
(t1 , b1 ), (t2 , b2 ), . . . , (tm , bm ),
perhaps representing some attribute b over time t. If the relation between t and b is (at least partly)
explained by a linear relationship then it makes sense to search for constants α0 ∈ R and α1 ∈ R such that
bk ≈ α0 + α1tk .

F IGURE 2. Fitting a line to points

See Figure 2. In particular, it is natural to search for α0 and α1 that minimize the sum of squares of the
errors (“least squares”),
m

min ∑ (bk − [α0 + α1tk ])2 .

α0 ,α1

k=1

In matrix-vector notation
min b − A

(6)

α0 ,α1

α0
α1

! 2
,

where


b1
b2
..
.











b=



 bm−1 
bm



and


1 t1
 1 t 

2 
 .
.. 

.
A= .
. 
.


 1 tm−1 
1 tm

14

We can assume w.l.o.g. that A has independent columns, see Lemma 6.1.2. Hence, the solution to (6) is
given by
!
"
#−1
!
m
m
m
t
α0
b
∑
∑
k
k
k=1
k=1
= (A⊤ A)−1 A⊤ b =
m
m
2
α1
t
t
∑m
∑
∑
k
k=1
k=1 k
k=1 tk bk
Lemma 6.1.2. The columns of the m × 2 matrix A defined before are linearly dependent if and only if
ti = t j for all i ̸= j.

Proof. Suppose that there are two indices i ̸= j such that ti ̸= t j . Let 1 be the all ones-vector in Rm and t
the vector with components t1 , . . . ,tm . Consider the system in variables λ , µ
λ 1 + µt = 0.
Since ti ̸= t j we can subtract row j from row i to obtain
λ 0 + µ(ti − t j ) = 0 ⇐⇒ µ = 0 since ti − t j ̸= 0.
This implies that λ = 0 and hence A has full column rank.
Conversely, if ti = t j for all i and j, then t = t1 1. Then the two columns of A are linearly dependent.

□

Remark 6.1.3. If the columns of A are pairwise orthogonal, then A⊤ A is a diagonal matrix, which is easy
to invert. In this example, the columns of A being orthogonal corresponds to ∑m
k=1 tk = 0. We could simply
1 m
new
do a change of variables to a new time tk = tk − m ∑i=1 ti to achieve this. If indeed ∑m
k=1 tk = 0 then the
equation above could be easily simplified:
!
#
! "
!
"
#−1
1
m
0
b
α0
m
0
b
∑
∑m
k
k
m
k=1
k=1
=
=
1
m
m
2
0
α1
0 ∑m
t
t
b
m
2
∑
∑
k
k
k=1 tk bk
k=1 k
k=1
∑k=1 tk
!
1 m
∑k=1 bk
m
 .
=
m
2
(∑m
k=1 tk bk ) / ∑k=1 tk
This is an instance where having orthogonal vectors is beneficial. In a later section we will see how to
build an orthonormal basis for subspaces and discuss more rigorously some of the many benefits they have.
Example 6.1.4 (Fitting a Parabola). We can use Linear Algebra to do fits of many other curves (or functions), not just lines. If we believe the relationship between tk and bk is quadratic we could attempt to fit a
Parabola:
bk ≈ α0 + α1tk + α2tk2 .
While this is not a linear function in tk , this is still a linear function on the coefficients α0 , α1 , and α2 , and
this is what is important. Similarly as with linear regression, it is natural to attempt to minimze

 2
α0


(7)
min b − A  α1  ,
α0 ,α1 ,α2
α2

15

where


b1
b2
..
.











b=



 bm−1 
bm


1 t1
t12
 1 t
t22 


2
 .

.

,
..
A =  ..



2
 1 tm−1 tm−1

2
1 tm
tm


and

and we can use the technology we developed in this section to solve this problem as well.
Try to work out the example of fitting a parabola further. What is A⊤ A? When is A⊤ A diagonal?
A few final comments are appropriate. There is a whole (beautiful) area of Mathematics related to studying
so-called Orthogonal Polynomials. The basic idea can be already hinted at from these examples: In
the example of the parabola we wrote a function of t as a linear combination of the polynomials 1, t,
and t 2 . But we could have picked other polynomials, we could have e.g. written something like b ≈
α0′ + α1′ (t − 2023) + α2 (t 2 + t), and a particularly good choice (that would depend on the distribution of
the points tk ) might have resulted in a diagonal matrix A⊤ A... search “orthogonal polynomials” online to
learn more.
A lot of Machine Learning includes Linear Regression as a key component. The idea is to create, find,
or learn features of the data points. Given n data points t1 , . . .tn (which now can be perhaps pixel images,
rather than just time points) we might want to do classification (for example, in the case of images, maybe
we want a function that is large when the picture has a dog in it and small when it has a cat in it). It is
hard to imagine that this can be done with a linear fit, but if we build good feature vectors ϕ(tk ) ∈ R p for
very large p then the function can depend on all coordinates of ϕ(tk ) (the p features) and this is incredible
powerful. There are several ways to construct features, a bit over a decade ago they were sometimes
handmade, now they are often learned (this is in a sense what Deep Learning does). Another important
way to build (or compute with) features are the so-called Kernel Methods.
We next revisit the topic of systems of linear equations and refine our understanding by using orthogonality.

6.2. The set of all solutions to a system of linear equations.
The machinery developed in the previous chapters allows us to understand the set of solutions to a system
of linear equations over the reals. This is one fundamental application of the concept of orthogonality.
To make the setting precise, let A ∈ Rm×n . There are two important subspaces associated with A:
N(A) = {x ∈ Rn | Ax = 0} and R(A) = C(AT ) = {x ∈ Rn | ∃y ∈ Rm such that x = AT y}.
We have learned that N(A) is the orthogonal complement of R(A). Vice versa, R(A) is the orthogonal
complement of N(A). Hence all of Rn can be written as the sum of two elements: one is from N(A) and
the other one from R(A). In other words:

16

∀x ∈ Rn there exist x0 ∈ N(A) and x1 ∈ R(A) such that x = x0 + x1 and x1T x0 = 0.
Let us first verify that for vectors x, y ∈ C(AT ) we have that
x ̸= y ⇐⇒ Ax ̸= Ay.

Lemma 6.2.1. Let A ∈ Rm×n . Let x, y ∈ C(AT ). We have that
Ax = Ay ⇐⇒ x = y.

Proof. Let x, y ∈ C(AT ). Then x,y ∈ N(A)⊥ .
Ax = Ay ⇐⇒ x − y ∈ N(A) ⇐⇒ xT (x − y) = 0 = yT (x − y) ⇐⇒ (x − y)T (x − y) = 0.
From the latter statement the result follows by noticing that
(x − y)T (x − y) = 0 ⇐⇒ ∥x − y∥2 = 0 ⇐⇒ x = y.
2

In view of Lemma 6.2.1 we are now in position to describe the set of all solutions to a system of equations
in detail. Of course, this requires us to assume that {x ∈ Rn | Ax = b} =
̸ 0.
/ This translated into geometry
tell us that b is a linear combination of the columns of A.

Theorem 6.2.2. Suppose that {x ∈ Rn | Ax = b} ̸= 0.
/ Then
{x ∈ Rn | Ax = b} = x1 + N(A) where x1 ∈ R(A) is unique such that Ax1 = b.

Proof. Let us first show that
{x ∈ Rn | Ax = b} ⊆ x1 + N(A).
From the assumptions of the statement there exists x̂ ∈ Rn such that Ax̂ = b. Consider x̂. Then it follows
that x̂ = x1 + y where y ∈ N(A) and x1 ∈ R(A). Since
b = Ax̂ = Ax1 ,
we have established that there exists x1 ∈ R(A) such that Ax1 = b. In view of Lemma 6.2.1 x1 is the unique
vector in R(A) such that Ax1 = b. Hence, every x̂ ∈ {x ∈ Rn | Ax = b} satisfies x̂ = x1 + y with y ∈ N(A),
so
{x ∈ Rn | Ax = b} ⊆ x1 + N(A).
The reverse inclusion follows since x1 ∈ R(A) satisfies Ax1 = b and every y ∈ N(A) satisfies Ay = 0.

2

17

Recall that for a matrix A ∈ Rm×n we have shown in Lemma 5.1.10 that N(A) = N(AT A) and R(A) =
C(AT ) = C(AT A). Hence we obtain

Corollary 6.2.3. Suppose that {x ∈ Rn | Ax = b} =
̸ 0.
/ Then there exists a unique vector x1 ∈ C(AT A)
such that Ax1 = b.

It remains to analyze the case when the given system of linear equations has no solution, i.e.,
{x ∈ Rn | Ax = b} = 0.
/
This task seems particularly difficult, because how can one convince someone that a system has no solution? The other person might always claim that it is my fault not to be able to find a solution. Hence,
we would like to give an algebraic certificate that such a system has no solution. Ideally such a certificate
is a system of equations that then must have a solution. Our knowledge about projections allows us to
accomplish this task.

Theorem 6.2.4.
{x ∈ Rn | Ax = b} = 0/ ⇐⇒ {z ∈ Rm | AT z = 0, bT z = 1} ̸= 0.
/

Proof. Set P = {x ∈ Rn | Ax = b} and D = {z ∈ Rm | AT z = 0, bT z = 1}. Let us us first verify that the
scenario P ̸= 0/ and D ̸= 0/ is impossible. Indeed, if x ∈ P and z ∈ D we obtain the following contradiction.
0 = 0T x = zT Ax = zT b = 1.
From this we conclude that P = 0/ or D = 0.
/ It remains to show that if P = 0,
/ then D ̸= 0.
/
Suppose that P = 0,
/ i.e., b is not a linear combination of the columns of A. This implies that the orthogonal
complement of b minus the projection of b on the subspace C(A) generated by the columns of A is not
zero. Let us put this sentence into a formula. Lemma 5.2.3 shows us that
projC(A) (b) = Ax̂, where x̂ satisfies the normal equations AT Ax̂ = AT b.
In particular, projC(A) (b) ∈ C(A) and hence we can write b = projC(A) (b)+ p, where p ∈ C(A)⊥ = N(AT ) and p ̸=
0. Hence, we obtain a feasible solution z ∈ D by defining
z :=

1
pT p

p ∈ Rm .

Since 0 ̸= p ∈ N(AT ), we obtain the claim:
AT z = 0 and bT z =

1
pT p

pT p = 1.
2

18

Let us first see an example of how to apply Theorem 6.2.4.
Example 6.2.5. Consider the system of two equations in three variables
P = {x ∈ R3 | x1 + 2x2 − x3 = 1, 2x1 + 4x2 − 2x3 = 0}.
The system D = {z ∈ Rm | AT z = 0, bT z = 1} referenced in Theorem 6.2.4 is
D = {z ∈ R2 | z1 + 2z2 = 0, 2z1 + 4z2 = 0, −z1 − 2z2 = 0, z1 = 1}.
P = 0/ and D ̸= 0,
/ because z = (1, − 12 )T ∈ D.
There are numerous ways to apply Theorem 6.2.4 and obtain interesting results. For instance, suppose we
know that our given matrix A ∈ Rm×n has linearly independent rows. Then for every vector b ∈ Rm there
exists a solution to the system of equations Ax = b. A very elegant way to see this is to apply Theorem
6.2.4. Indeed, since the rows of A are linearly independent, the only solution to zT A = 0 is z = 0. Hence
zT b = 0 ̸= 1 for all b ∈ Rm . This shows that
{z ∈ Rm | AT z = 0, bT z = 1} = 0.
/
Hence Ax = b always has a solution.
In a similar vein, Theorem 6.2.4 can be used to show that a vector b ∈ Rm is linearly independent from a
set of vectors {a1 , . . . , an } ⊆ Rm . Define A to be the matrix with column vectors a1 , . . . , an . b is linearly
independent from {a1 , . . . , an } ⊆ Rm if and only if Ax = b has no solution x ∈ Rn . By Theorem 6.2.4 the
latter fact requires us to find a solution z ∈ Rm such that AT z = 0, bT z = 1. Hence checking for linear
independence can be accomplished by solving a system of linear equations.
The topic of generating a certificate for the non-existence of a given set is fundamental and simply beautiful. You will certainly see several examples of this kind in your further academic live. As an outlook,
imagine we are given a matrix A ∈ Rm×n . Instead of considering a system of equations we associate with
every row a of A and a scalar α ∈ R the halfspace
{x ∈ Rn | aT x ≤ α}.
For a matrix A ∈ Rm×n and a given vector b ∈ Rm , we write Ax ≤ b to denote the intersection of all
halfspaces associated with all rows of A, i.e.,
n
T
{x ∈ Rn | Ax ≤ b} = ∩m
i=1 {x ∈ R | Ai· x ≤ bi }.

Then the famous Farkas lemma states that
P = {x ∈ Rn | Ax ≤ b} = 0/ ⇐⇒ D = {y ∈ Rm | yi ≥ 0 for all i = 1, . . . , m, yT A = 0, yT b < 0} ̸= 0.
/
Similar to the first part of the proof of Theorem 6.2.4 it is not difficult to verify that the two sets P and D
cannot simultaneously have a solution. For the purpose of deriving a contradiction, assume that P ̸= 0/ and
D ̸= 0.
/ Take any x ∈ P and y ∈ D to obtain the following contradiction:
m

m

0 = 0T x = yT Ax = ∑ yi ATi· x ≤ ∑ yTi bi = yT b < 0.
i=1

i=1

19

The difficult part is to show that if one set is empty, then the other one is non-empty. We will touch this
topic in a CS lenz. The interested reader is refereed to the text book
Alexander Schrijver, Theory of linear and integer programming, Wiley 1986.
There are also other extensions of systems of equations over the reals. For instance, let A ∈ Zm×n be a
matrix with integral entries of full row rank. For a given vector b ∈ Zm it is now natural to ask whether b is
in the ”lattice” generated by the columns of A, i.e., is there a solution to the system of equations over the
integers
Ax = b, x ∈ Zn ?
If you are interested in understanding such extensions you might want to consider to take the course
Discrete Optimization taught by the second author of these notes.
6.3. Orthonormal Bases and Gram Schmidt.
When we think of (or draw) a basis of a subspace, we tend to think of (or draw) vectors that are orthogonal
(have an angle of 90◦ ) and that have the same length (length 1). Indeed, these bases have many advantages
as we will see later. What can we say about these bases? How to construct them?
Definition 6.3.1 (Orthonormal vectors). Vectors q1 , . . . , qn ∈ Rm are orthonormal if they are orthogonal
and have norm 1. In other words, for all i, j ∈ {1, . . . , n}
qTi q j = δi j ,
where δi j is the Kronecker delta
(
(8)

δi j =

0
1

if i ̸= j
if i = j.

If Q is the matrix whose columns are the vectors qi ’s, then the condition that the vectors are orthonormal
can be rewritten as Q⊤ Q = I. Note though that Q may not be a square matrix. Hence, it is not necessarily
the case that QQ⊤ = I.
Example 6.3.2. A classical example of an orthonormal set of vectors is the canonical basis, e1 , . . . , en ∈ Rn
where ei is the vector with a 1 in the i-th entry and 0 in all other entries, i.e., (ei ) j = δi j .
When Q is a square matrix then Q⊤ Q = I implies also that QQ⊤ = I and so Q−1 = Q⊤ . We call such
matrices orthogonal matrices. This corresponds to the case when the qi ’s are an orthonormal basis for all
of Rn .
Definition 6.3.3 (Orthogonal Matrix). A square matrix Q ∈ Rn×n is an orthogonal matrix when Q⊤ Q = I.
In this case, QQ⊤ = I, Q−1 = Q⊤ , and the columns of Q form an orthonormal basis for Rn .
Example 6.3.4. The 2 × 2 matrix Q that corresponds to rotating, counterclockwise, the plane by θ ,
"
#
cos θ − sin θ
Rθ =
sin θ cos θ

20

is an orthogonal matrix. Indeed,
"
RTθ Rθ =

cos θ
− sin θ

sin θ
cos θ

#"

cos θ
sin θ

− sin θ
cos θ

#
= I.

Example 6.3.5. Permutation matrices are another example of orthogonal matrices. A permutation is a
map
π : {1, . . . , n} 7→ {1, . . . , n} such that π(i) ̸= π( j) for i ̸= j.
A permutation matrix P ∈ Rn×n associated with π has entries Pi j = 1 if π(i) = j and Pi j = 0, otherwise.
From this definition one can derive that PT is the permutation matrix associated with the permutation σ
defined as σ ( j) = i for π(i) = j. Hence, PT P = I, i.e., P is an orthogonal matrix.
It is also not so difficult to show that for every permutation matrix P there exists a positive integer k such
that Pk = I. The reason is that P2 , P3 , . . . , Pk , . . . are all permutation matrices. Since the number of different
permutations is finite there exist two indices k < l such that a permutation matrix Q say, is visited twice,
i.e.,
there exist indices k < l such that Pl = Q = Pk ⇐⇒ I = Pl−k .
This idea can be turned into an elegant clean proof. We leave it to the interested reader to work this out.

Proposition 6.3.6. Orthogonal matrices preserve norm and inner product of vectors. In other words,
if Q ∈ Rn×n is orthogonal then, for all x, y ∈ Rn
∥Qx∥ = ∥x∥ and (Qx)⊤ (Qy) = x⊤ y

Proof. To show the second equality note that, for x, y ∈ Rn we have that (Qx)⊤ (Qy) = x⊤ Q⊤ Qy = x⊤ Iy =
x⊤ y. To show the first equality note that, since for x ∈ Rn we have that ∥Qx∥ ≥ 0 and ∥x∥ ≥ 0. Hence, it
suffices to show that the squares are equal. Indeed, we obtain ∥Qx∥2 = (Qx)⊤ (Qx) = x⊤ x = ∥x∥2 .
2

One advantage of having access to an orthonormal basis is that projections become much simpler. The
reason is easy to explain. When we discussed projections and least squares, many of the expressions we
derived included A⊤ A, but in the case when A has orthonormal columns, these all simplify as A⊤ A = I. We
collect these observations in the following proposition.
Proposition 6.3.7. Let S be a subspace of Rm andhq1 , . . . , qn be an orthonormal
basis for S. Let Q be
i
the m × n matrix whose columns are the qi ’s; Q = q1 , · · · , qn . Then the Projection Matrix
that projects to S is given by QQ⊤ and the Least Squares solution to Qx = b is given by x̂ = Q⊤ b.

What can one say when Q is a square matrix? When Q is square, then the projection QQ⊤ is the identity
corresponding to projecting to the entire ambient space Rn . Even in this seemingly trivial instance, it is

21

useful to look closer at what this operation does: For a vector x ∈ Rn it gives






⊤
⊤
x = q1 q⊤
x
+
q
q
x
+
·
·
·
+
q
q
x
.
2
n
1
2
n
It is writing x as a linear combination of the orthonormal basis {qi }ni=1 . We will see later that this operation
is sometimes referred to as a change of basis. There are countless instances in which doing this operation
is beneficial, for example one of the most important algorithms, the Fast Fourier Transform, is an instance
of this operation.
By now we have given some evidence that orthonormal bases are useful. Fortunately, there is a relatively
simple process to construct orthonormal bases that will also suggest a new matrix factorization.
The idea is based on computing projections and is simple to describe: If we have 2 linearly independent
vectors a1 and a2 which span a subspace S, it is straightforward to transform them into an orthonormal
basis of S: we first normalize a1 : q1 = ∥aa11 ∥ , then subtract from a2 a multiple of q1 so that it becomes
orthogonal to q1 , followed by a normalization step:
q2 =

a2 − (a⊤
2 q1 )q1
.
⊤
a2 − (a2 q1 )q1

Let us check that indeed these vectors are orthonormal: By construction they have unit norm, and
⊤
q⊤
1 q2 = q1

⊤
⊤
a2 − (a⊤
q⊤
0
2 q1 )q1
1 a2 − (a2 q1 )q1 q1
=
=
= 0.
⊤
⊤
a2 − (a2 q1 )q1
a2 − (a2 q1 )q1
a2 − (a⊤
2 q1 )q1

Note that the denominator is not zero because a1 and a2 are linearly independent; and that, since q1 has
unit norm, (a⊤
2 q1 )q1 = projSpan(q1 ) (a2 ).
For more vectors, the idea is to apply this process recursively, by removing from a vector ak+1 the projection of it on the subspace spanned by the k vectors before it. More formally:
Algorithm 6.3.8. [Gram-Schmidt Process] Given n linearly independent vectors a1 , . . . , an that span a
subspace S, the Gram-Schmidt process constructs q1 , . . . qn in the following way:
• q1 = ∥aa11 ∥ .
• For k = 2, . . . , n set
⊤
q′k = ak − ∑k−1
i=1 (ak qi )qi
q′

qk = ∥q′k ∥ .
k

Theorem 6.3.9 (Correctness of Gram-Schmidt). Given n linearly independent vectors a1 , . . . , an , the
Gram-Schmidt process returns an orthonormal basis for the span of a1 , . . . , an .

Proof. Let Sk be the subspace spanned by a1 , . . . , ak . Then S = Sn . We will show, by induction, that
q1 , . . . , qk are an orthonormal basis for Sk . It is enough to show that they are orthonormal and are in Sk
since orthonormality implies linearly independence and Sk has dimension k.
For the base case, note that ∥q1 ∥ = 1 and q1 is a multiple of a1 and so q1 ∈ S1 .

22

Now we assume the hypothesis for i = 1, . . . k − 1 and prove it for k. By the hypothesis q1 , . . . , qk−1 are
orthonormal, so we have to show that ∥qk ∥ = 1 and that q⊤
i qk = 0 for all 1 ≤ i ≤ k − 1.
• Since ak is linearly independent from the other original vectors it is not in Sk−1 and so q′k ̸= 0.
Thus ∥qk ∥ = 1.
• By construction ak ∈ Sk and so qk ∈ Sk .
• Let 1 ≤ j ≤ k − 1. Since q1 , . . . , qk−1 are orthonormal, we have
!
k−1

q⊤j ak − ∑ (a⊤
k qi )qi
i=1

k−1

⊤
⊤
⊤
= q⊤j ak − ∑ (a⊤
k qi )q j qi = q j ak − (ak q j ) = 0,
i=1

and q⊤j qk = ∥q1′ ∥ q⊤j q′k = 0.
k

2

It might be a good exercise to perform the Gram-Schmidt process for the columns of the matrix


1 2 3 0
 0 4 5 6 



.
 0 0 7 8 
0 0 0 9
What do you observe? Is it true that the Gram-Schmidt process applied to the columns of an upper
triangular matrix with non-zero diagonal elements always produces a subset of the canonical basis?
The Gram-Schmidt process has numerous applications. One is that it provides us with a new matrix
factorization. Let A be an m × n matrix with linearly independent columns a1 , . . . , an and Q the m × n
matrix whose columns are q1 , . . . , qn as returned by Algorithm 6.3.8. Let R = Q⊤ A. R is upper triangular
because each qk is orthogonal to every ai for i < k. Note that Q is not necessarily a square matrix, and
so not necessarily invertible. But QQ⊤ is the projection on the span of the qi ’s and thus also on the ai ’s.
This allows us to conclude that QQ⊤ A = A, so we have that QR = QQ⊤ A = A. We call A = QR the QR
decomposition.
Definition 6.3.10 (QR decomposition). Let A be an m × n matrix with linearly independent columns. The
QR decomposition is given by
A = QR,
where Q is an m × n matrix with orthonormal columns (they are the output of Gram Schmidt, Algorithm 6.3.8, on the columns of A) and R is an upper triangular matrix given by R = Q⊤ A.
It requires us to show that indeed this is a proper definition. In particular, we need to convince ourselves
that R is upper triangluar.
Lemma 6.3.11. The matrix R defined in Definition 6.3.10 is upper triangular and invertible. Moreover, QQT A = A and hence, A = QR is well defined.

23

Proof. qTk qi = 0 for all i = 1, . . . k − 1. Since q1 , . . . , qk−1 and a1 , . . . , ak−1 span the same subspace Sk−1
we have that qTk ai = 0 for all i = 1, . . . , k − 1. Hence R = QT A is upper triangular. Moreover, QQT is the
projection onto the subspace C(Q) = C(A). Hence, for every index i,
projSn (ai ) = ai = QQT ai .
This is equivalent to QQT A = QR = A. Finally, N(A) = {0} and since A = QR, we must have that
N(R) = {0}. Since R is an n × n matrix, R is invertible.
2
Note that in the proof we showed that R is invertible. This also implies that RT is invertible. Hence we
obtain

Fact 6.3.12. The QR decomposition greatly simplifies calculations involving Projections and Least
Squares.
• Since C(A) = C(Q) then projections on C(A) can be done with Q which means they are given
by projC(A) (b) = QQ⊤ b.
• The least squares solution to Ax = b denoted by x̂ is defined as a solution of the normal
equations (recall (4))
A⊤ Ax̂ = A⊤ b.
Furthermore, A⊤ A = (QR)⊤ (QR) = R⊤ Q⊤ QR = R⊤ R, and so we can write
(9)

R⊤ Rx̂ = R⊤ Q⊤ b.
Since RT is invertible we can simplify (9) to

(10)

Rx̂ = Q⊤ b,
which can be efficiently solved by back-substitution since R is a triangular matrix.

6.4. The Pseudoinverse, also known as Moore–Penrose Inverse.
The goal of this Section is to construct an analogue to the inverse of a matrix A for matrices that have no
inverse. Such an analogue is called a pseudoinverse, or the Moore-Penrose Inverse, and we will denote
it by A† . It is also commonly denoted by A+ . What is the right idea to define a “pseudoinverse” for any
matrix A? It should be a matrix that is, in a sense, closest to being an inverse for A? What should “closest
to being an inverse” mean?
There are three issues we need to overcome to try to define a pseudoinverse for a non-invertible matrix A:
(i) For some vectors b there might not be a vector x such that Ax = b, (ii) For some vectors b there may be
more than one x such that Ax = b and we would have to pick one, and (iii) even if we make such choices,
it is not clear that such an operation will correspond to multiplying by a matrix A† .

24

Let A ∈ Rm×n be an m × n matrix. There are a couple of different ways we could try to define a pseudoinverse A† for a non-invertible matrix A. Let us start by building on what we discussed in Section 6.1.
If the columns of A are linearly independent then it might make sense to build A† such that A† b is the
Least Squares Solution x̂ = (A⊤ A)−1 A⊤ b (the vector x̂ such that Ax̂ is as close as possible to b), and so for
matrices A with independent columns we will define A† = (A⊤ A)−1 A⊤ . This is the following definition.
Definition 6.4.1 (Pseudoinverse for matrices of full column rank). For A ∈ Rm×n with rank(A) = n we
define the pseudo-inverse A† ∈ Rn×m of A as
A† = (A⊤ A)−1 A⊤ .

Proposition 6.4.2. For A ∈ Rm×n with rank(A) = n, the pseudoinverse A† is a left inverse of A, meaning
that A† A = I.

Proof. Since rank(A) = n, A⊤ A is invertible. Furthermore, A† A = (A⊤ A)−1 A⊤ A = I.

2

As a next step let us consider the case for which the rows are linearly independent (in other words, A ∈
Rm×n is full row rank; or equivalently rank(A) = m). One natural way to define a pseudoinverse is based
on the observation that A⊤ has full column rank and to define A† as
  ⊤
   −1   !⊤ 
−1 ⊤

−1
†
⊤
⊤
⊤
⊤
⊤
⊤
⊤
A
=
A
A
A
=
AA
A
= A⊤ AA⊤
.
Definition 6.4.3 (Pseudoinverse for matrices of full row rank). For A ∈ Rm×n with rank(A) = m we define
the pseudo-inverse A† ∈ Rn×m of A as
A† = A⊤ (AA⊤ )−1 .

Lemma 6.4.4. For A ∈ Rm×n with rank(A) = m, the pseudoinverse A† is a right inverse of A, meaning
that AA† = I.

Proof. Since rank(A) = m, AA⊤ is invertible. Furthermore, AA† = AA⊤ (AA⊤ )−1 = I.

2

Let us try to understand what A† is achieving for full row rank matrices A. Since A is full row rank, for all
b ∈ Rm , there exists x ∈ Rn such that Ax = b. The issue is that there are potentially many such vectors. A
natural strategy in this case is to pick, among all such vectors, the one with smallest norm.1 In other words

1This idea, of picking the smallest (or simplest) solution among many possibilities goes far beyond Linear Algebra and is

known as “regularization” in Statistics, Machine Learning, Signal Processing, and Image Processing, etc. It can be viewed as a
mathematical version of the famous “Occam’s razor” principle in Philosophy.

25

we solve
(11)

min

x∈Rn

∥x∥2

s.t. Ax = b,
where s.t. stands for “subject to” or “such that”.

Lemma 6.4.5. For any matrix A and a vector b ∈ C(A), the (unique) solution to (11) is given by the
vector x̂ ∈ C(A⊤ ) that satisfies the constraint Ax̂ = b.

Proof. From Theorem 6.2.2 it follows that
{x ∈ Rn | Ax = b} = x̂ + N(A) where x̂ ∈ C(AT ) is unique such that Ax̂ = b.
Moreover, for all y ∈ N(A) we notice that x̂T y = 0 since x̂ ∈ N(A)⊥ . This gives
∥x̂ + y∥2 = x̂T x̂ + 2x̂T y + yT y = ∥x̂∥2 + ∥y∥2 ≥ ∥x̂∥2 .
2
A† is precisely the matrix that maps b to a point x̂ that corresponds to a solution of (11).

Proposition 6.4.6. For a full row rank matrix A, the (unique) solution to (11) is given by the vector
x̂ = A† b.

Proof. By using Lemma 6.4.5 we just need to show that x̂ = A† b satisfies Ax̂ = b and that x̂ = A† b is in

C(A⊤ ). Both these are easy to verify: Ax̂ = AA† b = AA⊤ (AA⊤ )−1 b = b and x̂ = A† b = A⊤ (AA⊤ )−1 b
and so x̂ ∈ C(A⊤ ).
2
Our next task is to define A† for all matrices, not just full rank matrices. The idea is to write A as a product
of two matrices, one which is of full column rank and one which is of full row rank. Recall that in Part I of
the lecture we achieved this task by introducing the CR-decomposition. For A ∈ Rm×n , with rank(A) = r,
the CR decomposition writes A = CR where C ∈ Rm×r has the first r linearly independent columns of A
and R ∈ Rr×n is upper triangular. Note that C has full column rank and R full row rank.
Definition 6.4.7 (Pseudoinverse for all matrices). For A ∈ Rm×n with rank(A) = r and CR decomposition
A = CR we define the pseudoinverse A† as
A† = R†C† ,

26

which can be rewritten as

−1

−1

−1 
−1
C⊤ .
C⊤ = R⊤ C⊤ AR⊤
C⊤ = R⊤ C⊤CRR⊤
A† = R⊤ RR⊤
C⊤C
The following lemma characterizes what the matrix A† achieves for us.
Lemma 6.4.8. Given A ∈ Rm×n and a vector b ∈ Rm , the (unique) solution to
(12)

min

x∈Rn

∥x∥2

s.t. A⊤ Ax = A⊤ b,
is given by x̂ = A† b.

Proof. Let r be the rank of A and A = CR with C ∈ Rm×r and R ∈ Rr×n . Then x̂ = A† b = R⊤ C⊤ AR⊤
Thus,

−1

−1
C⊤ b = R⊤C⊤ b = A⊤ b.
C⊤ b = R⊤C⊤ AR⊤ C⊤ AR⊤
A⊤ Ax̂ = A⊤ AR⊤ C⊤ AR⊤

−1

C⊤ b.

It remains to show that x̂ is the smallest norm solution. To verify this we use Lemma 6.4.5, i.e., we need
to verify that x̂ ∈ C(A⊤ A). From Lemma 5.1.10 we conclude that C(AT A) = C(AT ). Hence it is enough
−1 ⊤
to show that x̂ ∈ C(A⊤ ) and since C(A⊤ ) = C(R⊤ ) we have that x̂ = R⊤ C⊤ AR⊤
C b ∈ C(R⊤ ) from
which the statement follows.
2
In this proof, the only property of the matrices CR we used is that A = CR and both C and R are full rank.
So we have actually shown that we can compute the pseudoinverse from any full rank factorization, not
just specifically the CR decomposition. We write it here as a proposition.
Proposition 6.4.9. For A ∈ Rm×n , with rank(A) = r, let S ∈ Rm×r and T ∈ Rr×n such that A = ST .
A† = T † S† .

Note that If A = ST and rank(A) = r then rank(S) ≥ r and rank(T ) ≥ r and so the matrices ST in Proposition 6.4.9 are indeed full rank (either full column rank or full row rank). Let us finally summarize a few
important properties about the matrix A and its pseudoinverse A† .
Theorem 6.4.10. Let A ∈ Rm×n .
⊤
†
AA† A = A and A† AA† = A† and A⊤ = A† .
AA† is symmetric. It is the projection matrix for projection on C(A),
A† A is symmetric. It is the projection matrix for projection on C(A⊤ ).

27

Proof. We calculate
AA† A = CRRT (CT CRRT )−1CT CR = CRRT (RRT )−1 (CT C)−1CT CR = CR = A.
To see that AA† is symmetric we calculate
AA† = CRRT (RRT )−1 (CT C)−1CT = C(CT C)−1CT = C(CT C)−1CT

T

= (AA† )T .

Since the column space of A, C(A) and the column space of C coincide and since C is a basis for C(A)
Theorem 5.2.5 applies and shows us that AA† = C(CT C)−1CT is the projection matrix for projecting onto
C(A).
□
The proof of the other statements in Theorem 6.4.10 is left to the reader.
7. T HE DETERMINANT
We will now introduce the notion of determinant det(A) of a square matrix A. While this has a somewhat involved definition for n × n matrices, it is useful to first discuss what the determinant geometrically
corresponds to, and to focus on small matrices.
In a nutshell, the determinant of a matrix is a number that corresponds to how much the associated linear
transformation inflates space, it corresponds precisely to the volume (or area, in R2 ) of the image of the
unit cube (the red square in the pictures above in R2 ); with a negative sign when the orientation changes
(in the pictures above in R2 , when the order of the colored dots, on the red square, changed). If we think
about the determinant this way, then many of the properties we will list below can be intuitively understood
while it is hard to do so from the formula for the n × n determinant. For this reason, this section will be
somewhat less proof-based, and rather focus on the most relevant properties of the determinant.
Remark 7.0.1. Grant Sanderson has a website https://www.3blue1brown.com/ and Youtube
channel https://www.youtube.com/3blue1brown with excellent animation-heavy explanations
of topics in Mathematics, including Linear Algebra. I particularly recommend the video on Determinants,
it has also 3 dimensional visualizations that are harder to do on a static medium. You can find it here
https://youtu.be/Ip3X9LOh2dk or here https://www.3blue1brown.com/lessons/
determinant. See also Figure 3.
A calculation of the area of the image of the unit square by left-multiplication by a 2 × 2 matrix shows (see
Figure 3) that
"
#
a b
a b
:= det
= ad − bc.
c d
c d
Before we actually formally define the determinant for general n × n matrices we will first focus on the
special case of 2 × 2- matrices to derive several important properties of the determinant.
7.1. 2 × 2 - matrices.
This section is based on the following definition for general 2 × 2 - matrices.

28

F IGURE 3. Calculation in 3Blue1Brown’s video (see Remark 7.0.1) computing the determinant of a 2 × 2 matrix as the area of the image of the unit square after a linear
transformation (that does not change orientation).

Definition 7.1.1. Let

"
A=

a c
b d

#
.

The determinant of A is det(A) = ad − bc.

Let us first understand how the determinant changes when we multiply matrices. To this end, let
#
"
#
"
a c
x z
and W =
.
A=
b d
y w
Next we multiply these two matrices and obtain an explicit representation of the coefficients
"
#
ax + cy az + cw
.
AW =
bx + dy bz + dw
Using this representation we obtain the following result.

Lemma 7.1.2. Let A,W ∈ R2×2 . Then det(AW ) = det(A) det(W ).

29

Proof.
det(AW ) =
=
=
=

(ax + cy)(bz + dw) − (az + cw)(bx + dy)
axbz + axdw + cybz + cydw − azbx − azdy − cwbx − cwdy
axdw + cybz − azdy − cwbx
ad(xw − zy) + cb(zy − xw) = det(A) det(W ).
□

This computation allows us to derive a characterization of when a 2 × 2-matrix is invertible.

Lemma 7.1.3. A matrix A ∈ R2×2 is invertible if and only if det(A) ̸= 0.

Proof. Let
"
A=

a c
b d

#
.

If A is invertible, then A−1 exists and hence, AA−1 = I implies together with the previous lemma that
det(A) det(A−1 ) = 1. Hence, det(A) ̸= 0.
Conversely, if det(A) ̸= 0, then a ̸= 0 or b ̸= 0. Without loss of generality we can assume that a ̸= 0.
Consider now the system of linear equations AW = I.
ax + cy = 1
az + cw = 0

implies that
implies that

x = 1−cy
a
z = −cw
a .

By substituting these expressions into the other two equations bx + dy = 0 and bz + dw = 1 we obtain
−b
b cyb
−
+ dy = 0 ⇐⇒ b + y(ad − bc) = 0 ⇐⇒ y =
.
a
a
det(A)
−bcw
a
+ dw = 1 ⇐⇒ −bcw + adw = a ⇐⇒ w =
.
a
det(A)
This then gives us a formula for the parameters z and x in form of
cd

1 + det(A)
−c
d
z=
and x =
=
.
det(A)
a
det(A)
These calculations show that A−1 exists whenever det(A) ̸= 0.

□

Notice that our calculations give us an explicit formula for the inverse of matrix A and its determinant:

(13)

−1

A

1
=
det(A)

"

d −c
−b a

#
.

30

7.2. The general case.
It turns out that what we have verified in dimension two carries over to general dimensions. It is, however
much more involved to verify it algebraically.
We now give the definition of a determinant for n × n matrices. This requires us to discuss permutations.
Definition 7.2.1 (Sign of a permutation). Given a permutation σ : {1, . . . , n} → {1, . . . , n} of n elements,
its sign sgn(σ ) can be 1 or −1. The sign counts the parity of the number of pairs of elements that are out
of order (sometimes called inversions) after applying the permutation. In other words,

 1
if |(i, j) ∈ {1, . . . , n} × {1, . . . , n} such that i < j and σ (i) > σ ( j)| is even,
sgn(σ ) =
 −1 if |(i, j) ∈ {1, . . . , n} × {1, . . . , n} such that i < j and σ (i) > σ ( j)| is odd.
Example 7.2.2. Let n = 4. Consider the permutation π defined as π(1) = 1, π(2) = 3, π(3) = 2, π(4) = 4.
The pairs (i, j) such that i < j are
(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4).
For all these listed pairs (i, j) we have that π(i) < π( j) except for the pair (2, 3). Hence, sgn(π) = −1.
The sign of a permutation has many nice properties that are not so easy to show.
(1) The sign of a permutation is multiplicative, i.e.: for two permutations σ , γ we have that sgn(σ ◦
γ) = sgn(σ )sgn(γ).
(2) For all n ≥ 2, exactly half of the permutations have sign 1 and exactly half have sign −1.
The identity has a sign of 1, the sign of a transposition (a permutation that only swaps two elements) is −1
and for two permutations σ , γ we have that sgn(σ ◦ γ) = sgn(σ )sgn(γ).
We are now in position to introduce the general notion of a determinant of a square matrix.
Definition 7.2.3. Given a square matrix A ∈ Rn×n the determinant det(A) is defined as
n

det(A) = ∑ sgn(σ ) ∏ Ai,σ (i) ,
σ ∈Πn

i=1

where Πn is the set of all permutations of n elements.
If A is a 1 × 1 matrix then there is only one permutation of 1 element: the permutation σ (1) = 1, which
has sign 1. It follows that det(A) = A.
For 2 × 2 matrices we observe that there exist two permutations. Let us call σ1 the identity permutation
(that doesn’t move any element, which has sign 1) and σ2 the permutation that swaps the two elements
(which has sign −1). Hence, for a 2 × 2 matrix A with entries Ai j we have
2

2

2

det(A) = ∑ sgn(σ ) ∏ Ai,σ (i) = (+1) ∏ Ai,σ1 (i) + (−1) ∏ Ai,σ2 (i) = A11 A22 − A12 A21 .
σ ∈Π2

i=1

i=1

i=1

31

This corresponds precisely to the definition that we used for the determinant of a 2 × 2-matrix with entries
A11 = a, A12 = c, A21 = b and A22 = d, namely
det(A) = ad − bc.
By applying Definition 7.2.3 we immediately obtain the following results.

Proposition 7.2.4.
(a) Given a permutation matrix P ∈ Rn×n corresponding to a permutation σ , then det(P) =
sgn(σ ). We sometimes also write sgn(P).
(b) Given a triangular (either upper- or lower-) matrix T ∈ Rn×n we have
n

det(T ) = ∏ Tkk ,
k=1

in particular, det(I) = 1.
(c) If Q ∈ Rn×n is an orthogonal matrix then
det(Q) = 1

or

det(Q) = −1.

Proof. (a) P ∈ Rn×n satisfies that ∏ni=1 Pi,σ (i) = 1 and
n

∏ Pi,π(i) = 0 for every permutation π ̸= σ .
i=1

This gives the result.
(b) Without loss of generality let T ∈ Rn×n satisty Ti j = 0 for all j < i. Let σ denote the identity permutation, i.e., σ (i) = i for all i ∈ {1, . . . , n}. Consider any permutation π different from σ , i.e., there exists an
index i such that π(i) ̸= i = σ (i). We notice that
n

∑i =

i=1

n
n(n + 1)
= ∑ π(i).
2
i=1

Hence, if for an index i, π(i) > i, then there exists another index l ̸= i such that π(l) < l. Hence we observe
that there always exists an index k, say such that π(k) < k. Since Tk,π(k) = 0, we obtain
n

∏ Ti,π(i) = 0 for every permutation π ̸= σ .
i=1

This gives the result.
(c) From Part (b) and Theorem 7.2.5 below we obtain
1 = det(I) = det(Q⊤ Q) = det(Q⊤ ) det(Q) = det(Q)2 .

32

It follows that 1 = det(Q)2 and so det(Q) is 1 or -1.
2
The most important results about determinants are described now. We begin with Theorem 7.2.5 that we
already applied.

Theorem 7.2.5. Given a matrix A ∈ Rn×n , then
det(AT ) = det(A).

Proof. For a permutation σ let σ −1 denote the inverse permutation, i.e.,
σ (i) = j ⇐⇒ σ −1 ( j) = i for all i, j.
We have that for all i σ (σ −1 )(i) = i and hence, sgn(σ )sgn(σ −1 ) = 1. It follows that sgn(σ ) = sgn(σ −1 ).
The conclusion det(A⊤ ) = det(A) follows from observing
∑σ ∈Πn sgn(σ ) ∏ni=1 Ai,σ (i) =
∑σ ∈Πn sgn(σ ) ∏ni=1 Aσ −1 (σ (i)),σ (i) =
∑σ ∈Πn sgn(σ ) ∏nj=1 Aσ −1 ( j), j =
∑σ −1 ∈Πn sgn(σ −1 ) ∏ni=1 Aσ −1 (i),i =
∑σ ∈Πn sgn(σ ) ∏ni=1 Aσ (i),i .
□

In correspondence with our analysis of the determinant for 2 × 2-matrices, we can also more generally
multiply matrices or derive a formula for the determinant of the inverse of a matrix.

Theorem 7.2.6.
• A matrix A ∈ Rn×n is invertible if and only if
det(A) ̸= 0.
• Given matrices A, B ∈ Rn×n we have
det(AB) = det(A) det(B).
• Given a matrix A ∈ Rn×n such that det(A) ̸= 0, then A is invertible and
det(A−1 ) =

1
.
det(A)

Example 7.2.7. Let A be a 3 × 3 matrix. There are 3! = 6 permutations, so in total there will be 6
terms involved in comnputing the determinant. Notice that we can write its determinant also as a sum of

33

determinants of special matrices (where an empty entry corresponds to a zero entry).
A11 A12 A13
A21 A22 A23
A31 A32 A33

det(A) =

= A11 A22 A33 − A12 A21 A33 + A12 A23 A31 − A13 A22 A31 + A13 A21 A32 − A11 A23 A32

=

A12

A12

A11

+

+ A21

A22

A11

A13

A13
A22

A31

A33

A33
+

A23

A32

A31

A23 .

+

+ A21

A32

7.3. Cofactors, Cramer’s rule and beyond.
Let us revisit Example 7.2.7 from the previous section. Let A be a 3 × 3 matrix.
There is another convenient way of writing the determinant of A.
(14)

A11 A12 A13
A21 A22 A23
A31 A32 A33

= A11

A22 A23
A21 A23
A21 A22
− A12
+ A13
.
A32 A33
A31 A33
A31 A32

In general, these terms are called the co-factors of A.
Definition 7.3.1. Given A ∈ Rn×n , for each 1 ≤ i, j ≤ n let Ai j denote the (n − 1) × (n − 1) matrix obtained
by removing row i and column j from A. Then we define the co-factors of A as
Ci j = (−1)i+ j det(Ai j ).
Just as in (14), the determinant can be written in terms of the co-factors.
Proposition 7.3.2. Let A ∈ Rn×n , for any 1 ≤ i ≤ n,
n

det(A) = ∑ Ai jCi j .
j=1

The formula we derived above for the inverse of 2 × 2 matrices (Equation 13), also has an analogue in
n dimensions. Verify that Proposition 7.3.3 indeed corresponds to the formula we derived for A−1 when
n = 2.

34

Proposition 7.3.3. Given A ∈ Rn×n with det(A) ̸= 0 we have
A−1 =

1
C⊤ ,
det(A)

where C is the n × n matrix with the co-factors of A as entries.

The formula in Proposition 7.3.3 can be rewritten as
AC⊤ = det(A)I.
Remark 7.3.4. Computationally speaking, this is not a good way to compute the inverse, as it involves
computing many determinants.
The determinant allows us to write a formula for the solution of the linear system of the type Ax = b when
A ∈ Rn×n and det(A) ̸= 0. The idea is simple, we will illustrate it here for n = 3.


 

A11 A12 A13
x1
b1


 

If  A21 A22 A23   x2  =  b2 , then we have
A31 A32 A33
x3
b3

 


A11 A12 A13
b1 A12 A13
x1 0 0

 


 A21 A22 A23   x2 1 0  =  b2 A22 A23  .
x3 0 1
A31 A32 A33
b3 A32 A33
Since the determinant is multiplicative, and the determinant of the second matrix in the expression is x1 ,
we have
det(A)x1 = det(B1 ),
where B1 is the matrix obtained from A by replacing the first column of A with the vector b.
Since we can do this for any of the columns, we have x j = det(B j )/ det(A). In general

Proposition 7.3.5 (Cramer’s Rule). Let A ∈ Rn×n such that det(A) ̸= 0 and b ∈ Rn then the solution
x ∈ Rn of Ax = b is given by
det(B j )
,
xj =
det(A)
where B j is the matrix obtained by A by replacing the j-th column of A with the vector b.

As with the formula for the inverse: computationally speaking, this is not a good way to solve linear
systems, as it involves computing many determinants.
The definition we used for the determinant of a square matrix involves a formula with n! terms. This formula is computational infeasible for even moderate levels of n (it is faster than exponential! For example,

35

100! has almost 160 digits!). In practice, the determinant of a matrix A is computed by Gaussian Elimination and the matrix decomposition PA = LU where P is a permutation matrix and so det(P) = sgn(P), U
is upper triangular and L is lower triangular with only 1s in the diagonal, so det(L) = 1). This gives us the
formula
1
det(L) det(U) = sgn(P) det(U).
(15)
det(A) =
det(P)
U is a triangular matrix. Hence, its determinant is simply the product of its diagonal entries.
Alternatively, one can also think of Gaussian Elimination as directly computing the determinant via the
following two propositions

Proposition 7.3.6. If A is an n × n matrix and P is a permutation that swaps two elements, meaning
that PA corresponds to swapping two rows of A then det(PA) = − det(A).

Proposition 7.3.7. The determinant is linear in each row (or each column). In other words, for any
a0 , a1 , a2 . . . , an ∈ Rn and α0 , α1 ∈ R we have
⊤ —
— α0 a⊤
0 + α1 a1
—
a⊤
—
2
..
.

—

a⊤
n

—

— a⊤
—
— a⊤
—
0
1
⊤
⊤
— a2 —
— a2 —
+ α1
,
..
..
.
.
— a⊤
—
— a⊤
—
n
n

|
a2 · · ·
|

|
an
|

|
|
a0 a2 · · ·
|
|

= α0

= α0

and
|
α0 a0 + α1 a1
|

|
|
|
an + α1 a1 a2 · · ·
|
|
|

|
an .
|

Let us make a final comment on the presentation here. The more mathematical way of presenting this
material is to define a determinant as a function that goes from n × n matrices to R with the following
properties:
(1) it is linear in each column,
(2) det(I) = 1 and
(3) det(A) = 0 whenever A has two identical columns.
It is then possible to prove that the only function satisfying these three properties is the determinant as we
defined it.

36

8. E IGENVALUES AND E IGENVECTORS
We are now prepared for one very important concept in Linear Algebra, eigenvalues and eigenvectors. As
we will see below this material requires us to have understood the geometry of subspaces of Rn and being
aware of the notion of the determinant of a square matrix. Given a square matrix A, as we will see below,
an eigenvalue λ and eigenvector v will be, respectively, a scalar and a non-zero vector satisfying Av = λ v.
This means that (A − λ I)v = 0 and so (A − λ I) is not invertible, or equivalently det(A − λ I) = 0. We can
search for eigenvalues as solutions of det(A − λ I) = 0 which is a polynomial in λ . Unfortunately, not all
polynomials have real zeros. A zero of a polynomial P is a point x such that P(x) = 0. Such a point is also
called a root of the polynomial. In German, it is a “Nullstelle”. In fact, a (rather deep) multidimensional
version of Theorem 8.1.2, and one of the most important facts in Algebraic Geometry, is called “Hilbert’s
Nullstellensatz”.
"
#
0 −1
For example if A =
, det(A − λ I) = 0 corresponds to λ 2 + 1 = 0 which only has solutions in
1 0
C, the complex numbers. For this reason we will start this chapter with a brief introduction to complex
numbers. It all starts with asking for a number λ such that λ 2 + 1 = 0.

8.1. Complex Numbers.
If we start with the natural numbers N and want to solve equations like x + 10 = 1, we need negative
numbers. This motivates considering the integers Z. Similarly, rational numbers Q are needed to solve
equations like 10x = 1 and real numbers R are needed to solve x2 = 2. Similarly, the Complex Numbers
are needed to solve equations such as x2 + 1 = 0. It starts with the introduction of an imaginary number
√
i ∈ C such that i2 = −1. You can think of i as −1.
The complex numbers are numbers of the form z = a + ib for a ∈ R and b ∈ R. C = {a + ib : a, b ∈ R}.
Keeping in mind that i2 = −1 we can do operations with complex numbers:
• (a + ib) + (x + iy) = (a + x) + i(b + y),
• (a + ib)(x + iy) = ax + i(ay + bx) + i2 by = ax + i(ay + bx) − by = (ax − by) + i(ay + bx),
• (a + ib)(a − ib) = a2 + b2 ,

 

(x−iy)(a+ib)
(ax+by)+i(bx−ay)
ax+by
bx−ay
• a+ib
=
=
=
+
i
.
2
2
2
2
2
2
x+iy
(x−iy)(x+iy)
x +y
x +y
x +y
Given z ∈ C with z = a + ib we have the following notation
(16)

ℜ(a + ib) := a

(17)

ℑ(a + ib) := b
called the imaginary part of z = a + ib,
p
|z| :=
a2 + b2
called the modulus of z = a + ib,

(18)
(19)

a + ib := a − ib

called the real part of z = a + ib,

called the complex conjugate of z = a + ib.

Note that for z1 , z2 ∈ C, we have |z|2 = zz, z1 z2 = z2 z1 , z1 + z2 = z1 + z2 , and 1z = |z|z 2 .

37

Remark 8.1.1. Given θ ∈ R, we have
eiθ = cos θ + i sin θ .

(20)

This means, in particular, that eiπ = −1. This is usually written as eiπ + 1 = 0 and known as Euler’s
formula.
A complex number z ∈ C can be written as
(21)

z = reiθ ,

where r ≥ 0 is the modulus of z and θ ∈ R (we can restrict to θ ∈ [0, 2π[) is an angle, also called the
argument of z. This is known under the name polar coordinates.

F IGURE 4. A complex number z = 4 + 3i in the complex plane.

The most important property of complex numbers, and what makes them a very natural mathematical object, is that any univariate polynomial equation with complex number coefficients has a (complex) solution.
In a certain sense we do not need to extend numbers further, C is algebraically closed.

Theorem 8.1.2 (Fundamental Theorem of Algebra). Any degree n non-constant (n ≥ 1) polynomial
P(z) = αn zn + αn−1 zn−1 + · · · + α1 z + α0 (with αn ̸= 0) has a zero: λ ∈ C such that P(λ ) = 0.

As the name suggests, Theorem 8.1.2 is a central result. Proving it is outside the scope of this course. See
Appendix B for a relatively elementary proof.

38

Let us return to the question why Theorem 8.1.2 is so important for us. Once we have λ a zero of P(z),
we can divide P(z) by (z − λ ) to get P(z) = (z − λ )P1 (z), then use a zero of P1 to reiterate, and so on. This
argument (carried out carefully) gives the following corollary.

Corollary 8.1.3. Any degree n non-constant (n ≥ 1) polynomial P(z) = αn zn + αn−1 zn−1 + · · · + α1 z +
α0 (with αn ̸= 0) has n zeros: λ1 , . . . , λn ∈ C, perhaps with repetitions, such that
(22)

P(z) = αn (z − λ1 )(z − λ2 ) · · · (z − λn ).

The number of times λ ∈ C appears in this expansion is called the algebraic multiplicity of the zero.

Analogously to Rn we also define Cn as the set of n-dimentional complex valued vectors. We can have
complex valued vectors v ∈ Cn and matrices A ∈ Cm×n . The natural operation of “transposing” for complex
vectors and matrices is that of “conjugate transpose” or “hermitian transpose” denoted by A∗ , or sometimes
AH ,
(23)

T

A∗ = A .

Given v ∈ Cn we have

n

n

i=1

i=1

∥v∥2 = v∗ v = vT v = ∑ vi vi = ∑ |vi |2 .
The inner-product (or dot-product) in Cn is given by ⟨v, w⟩ = w∗ v.
Similarly to the situation in Rn , we say v1 , . . . , vk ∈ Cn are linearly independent if there is no (complex
valued) non-zero linear combination giving zero, meaning that if α1 v1 + · · · + αk vk = 0 for α1 , . . . , αk ∈ C
we must have α1 = · · · = αk = 0. Also, the span of v1 , . . . , vk ∈ Cn is the set of possible linear combinations
α1 v1 + · · · + αk vk for α1 , . . . , αk ∈ C. If v1 , . . . , vk is a spanning set of a subspace and linearly independent
we say it is a basis of that subspace. As with Rn if we have v1 , . . . , vn ∈ Cn that are either a spanning set of
Cn or linearly independent then they must actually be both (and so are a basis).
With these definitions one can already understand the Discrete Fourier Transform (which is the linear
transformation corresponding to the DFT matrix, one of the most important complex valued matrices).
This is the key object behind signal processing, you can read more about it on the lecture notes of another
course that the first author usually teaches [BM23]. You can also see a discussion of Fourier Transform,
circulant matrices, and signal convolutions in [Str23] (end of Section 6.4).

8.2. Introduction to Eigenvalues and Eigenvectors.
Even though the theory can be analogously developed for complex valued matrices, we will from now on
focus on real valued matrices. We will use a guiding example to illustrate both some of the power and
some of the properties of eigenvalues and eigenvectors. The central object of study below is given next.

39

Definition 8.2.1. Given A ∈ Rn×n , we say λ ∈ C is an eigenvalue of A and v ∈ Cn \ {0} is an eigenvector
of A, associated with the eigenvalue λ , when the following holds:
Av = λ v.
We call them an eigenvalue-eigenvector pair. If λ ∈ R then we will call λ a real eigenvalue, and the
associated eigenvalue-eigenvector pair a real eigenvalue-eigenvector pair.
Example 8.2.2. In this example we will derive a formula for the n-th Ficonacci Number. The Fibonacci
numbers are defined by the recurrence:
F0 = 0, F1 = 1, and, for n ≥ 2, Fn = Fn−1 + Fn−2 .

(24)

The recurrence can be rewritten in linear algebraic notation as, for n ≥ 2,
! "
!
#
1 1
Fn+1
Fn
=
.
(25)
Fn
1 0
Fn−1
Defining
"
M=

(26)

1 1
1 0

#

1
0

!

and gn =

Fn+1
Fn

!
,

the recurrence can be rewritten as
g0 =

and gn = Mgn−1 .

This leads us to the formula
gn = M n g0 .

(27)

"

#
1 1
Let us try to find eigenvalues (and later the eigenvectors) of M =
. We are looking for v ∈ R2 \{0}
1 0
and λ ∈ R such that Mv = λ v, but this can be rewritten as (M − λ I)v = 0 and since v ̸= 0 it means that
M − λ I is non-invertible (also called singular).2 This is equivalent to det(M − λ I) = 0 and so we can find
the eigenvalues λ with this equation:
(28)

0 = det(M − λ I) =

1−λ
1

1
0−λ

= (1 − λ )(0 − λ ) − 1 = λ 2 − λ − 1.

By the quadratic formula,3 the solutions to (28) are given by
√
√
1− 5
1+ 5
and λ2 =
.
(29)
λ1 =
2
2
√

The number ϕ = 1+2 5 is the celebrated Golden Ratio; believed, since the ancient Greeks, to be the ideal
aspect ratio for a rectangle.
2Normally, we would have to look for λ ∈ C and v ∈ Cn but in this case the eigenvalues, as we will see, are real.
√

3Recall that the quadratic formula says that the zeros of ax2 + b + c are given by x = −b± b2 −4ac .
2a

40

“Some of the greatest mathematical minds of all ages, from Pythagoras and Euclid in ancient
Greece, through the medieval Italian mathematician Leonardo of Pisa and the Renaissance astronomer Johannes Kepler, to present-day scientific figures such as Oxford physicist Roger Penrose, have spent endless hours over this simple ratio and its properties. [. . . ] Biologists, artists,
musicians, historians, architects, psychologists, and even mystics have pondered and debated the
basis of its ubiquity and appeal. In fact, it is probably fair to say that the Golden Ratio has inspired
thinkers of all disciplines like no other number in the history of mathematics.”
— The Golden Ratio: The Story of Phi, the World’s Most Astonishing Number
The following is the original definition which dates back to Euclid around 2300 years ago (they called the
number “extreme and mean ratio” back then)
“A straight line is said to have been cut in extreme and mean ratio when, as the whole line is to
the greater segment, so is the greater to the lesser”
Next we can try to find the eigenvectors v1 and v2 such that Av1 = λ1 v1 and Av2 = λ2 v2 .

√ 
Let us start with v1 . We are looking for a non-zero element of N A − 1+2 5 I . In other words
#
!
! "
√
1√
(v1 )1
0
1 − 1+2 5
.
=
(v1 )2
0
1
− 1+2 5
This is an under-determined system and we are looking for a non-zero solution,
so let us start by setting
√ !
√

1+ 5
2

(v1 )2 = 1. The second equation gives us (v1 )1 = 1+2 5 . Indeed v1 =

1

√

is an eigenvector of M

associated to the eigenvalue λ1 = 1+2 5 .
√
A similar calculation for λ2 = 1−2 5 gives that v2 =

"
(30)

1 1
1 0

#

√
1+ 5
2

1

!

√
1+ 5
=
2

√
1+ 5
2

1

√
1− 5
2

1
"

!
and

!
. Indeed

1 1
1 0

#

√
1− 5
2

1

!

√
1− 5
=
2

√
1− 5
2

1

!
.

It is left to the reader to carry out the calculations and confirm that we have indeed found two eigenvectors.
This requires one to check the two equalities in (30).
The vectors v1 and v2 we constructed in the example are not the only possible choices. For example, any
non-zero scalar multiple of these would have also been a possible choice. Normally one picks a unit-norm
representative, but in this case we picked vectors that make the calculations the cleanest.
What we carried out in the example above is very general and we now develop the theory for general
matrices.
Let λ and v be an eigenvalue-eigenvector pair of a matrix A. Since v ̸= 0 and (A − λ I)v = Av − λ v = 0 we
have that det(A − λ I) = 0. Conversely, if det(A − λ I) = 0 for some λ , then there exists v ∈ N(A − λ I) \ {0}
and so λ is an eigenvalue. This gives a procedure to find eigenvalues and eigenvectors: (i) eigenvalues

41

are the solution of det(A − λ I) = 0, which is a polynomial equation, and (ii) an associated eigenvector is a
non-zero element of N(A − λ I).
Let us first formulate this for real eigenvalues and eigenvectors.

Lemma 8.2.3. Let A ∈ Rn×n . λ ∈ R is a real eigenvalue of A if and only if det(A − λ I) = 0. A vector
v ∈ Rn \ {0} is an eigenvector associated with the eigenvalue λ if (and only if) v ∈ N(A − λ I).

Proof. Since λ ∈ R, we have that a solution of Av = λ v is real valued. Suppose v = v1 + iv2 ∈ Cn . Then
Av = Av1 + iAv2 = λ v1 + iλ v2 ⇐⇒ Av1 = λ v1 and Av2 = λ v2 ,
i.e., v1 and v2 are both real eigenvectors corresponding to eigenvalue λ ∈ R.

□

A direct inspection of the formula for the determinant (Definition 7.2.3) gives the following.
Proposition 8.2.4. det(A − λ I) is a polynomial in λ of degree n. The coefficient of the λ n term is
(−1)n .

The Fundamental Theorem of Algebra (Theorem 8.1.2) immediately implies

Theorem 8.2.5. Every matrix A ∈ Rn×n has an eigenvalue (perhaps complex-valued).

Let us see an example of a matrix that has no real eigenvalues and only complex-valued ones.
"
#
0 −1
Example 8.2.6. The eigenvalues of the matrix A =
, corresponding to a 90o counterclockwise
1 0
2
rotation, are the solutions
! to 0 = det(A − λ!I) = λ + 1, which are λ1 = i and λ2 = −i. The eigenvectors
i
−i
are given by v1 =
and v2 =
.
1
1
In this example the two eigenvalues and eigenvectors are conjugates of each other. Moreover, the modulus
of the eigenvalues was one. The latter property is not a surprise. Indeed, the matrix in the example is a
particular case of an orthogonal matrix, whose eigenvalues have a special property.

Proposition 8.2.7. Let Q ∈ Rn×n be an orthogonal matrix. If λ ∈ C is an eigenvalue of Q, then
|λ | = 1.

42

Proof. Let λ ∈ C be an eigenvalue of Q and v ∈ Cn an associated eigenvector. Then Qv = λ v. Since Q is
an orthogonal matrix we have ∥v∥2 = ∥Qv∥2 = ∥λ v∥2 = |λ |2 ∥v∥2 . Since v ̸= 0 we have |λ | = 1.
2

The other property we observed in this example is generally true. Complex eigenvalue-eigenvector pairs
are closed under taking the conjugate-operation.

Lemma 8.2.8. Let A ∈ Rn×n . If (λ , v) is an eigenvalue-eigenvector pair, then (λ̄ , v̄) is an eigenvalueeigenvector pair.

Proof. Let
λ = λ1 + iλ2 and v = v1 + iv2 , v1 , v2 ∈ Rn , λ1 , λ2 ∈ R.
We calculate
Av1 + iAv2 = Av = λ v = λ1 v1 + iλ1 v2 + iλ2 v1 + i2 λ2 v2 = (λ1 v1 − λ2 v2 ) + i(λ1 v2 + λ2 v1 )
and obtain that Av1 = λ1 v1 − λ2 v2 and Av2 = λ1 v2 + λ2 v1 . This allows us to verify that
Av̄ = Av1 − iAv2 = λ1 v1 − λ2 v2 − i(λ1 v2 + λ2 v1 ) = (λ1 − iλ2 )(v1 − iv2 ) = λ̄ v̄.
□

Let us return to Example 8.2.2.
Example 8.2.9. Notice that v1 and v2 are linearly independent, and so they are a basis for R2 . We can
write g0 = α1 v1 + α2 v2 .
!
√
√ !
√ !
1
α1 1+2 5 + α2 1−2 5
(α1 + α2 ) 12 + (α1 − α2 ) 25
= g0 = α1 v1 + α2 v2 =
=
,
0
α1 + α2
α1 + α2
and so α1 = √15 and α2 = − √15 .
Recall that gn = An g0 and so


1
1
1
1
1
gn = An √ v1 − √ v2 = √ An v1 − √ An v2 = √ (An v1 − An v2 ) .
5
5
5
5
5
Since Av1 = λ1 v1 we have that A2 v1 = A(λ1 v1 ) = λ12 v1 and iterating this procedure – a formal proof would
use induction – gives An v1 = λ1n v1 . This means that
 √ n
 √ n
 √ n
 √ n
√ !
√ !
1+ 5
1− 5
1+ 5
1− 5
1+
1− 5
5
n
n
v
−
v
1
2
2
2
2
2
A v1 − A v2
2
2
√
√
√
gn =
=
=
− √
.
1
1
5
5
5
5

43

Since Fn is the second coordinate of gn , we derived a closed formula for the n-th terms of the Fibonacci
sequence:
√ !n
√ !n
1+ 5
1
1− 5
1
−√
.
(31)
Fn = √
2
2
5
5
An important property that allowed us to do the calculation above was that applying a power of a matrix
to an eigenvector was a simple operation. This is true in general as we will show in the next section.
8.3. Properties of eigenvalues and eigenvectors.
Eigenvalues and eigenvectors of a matrix tell us many secrets about a matrix. This topic is illuminated in
this section.

Proposition 8.3.1.
(a) If λ and v are an eigenvalue-eigenvector pair of a matrix A, then, for k ≥ 1, λ k and v are an
eigenvalue-eigenvector pair of the matrix Ak .
(b) Let A be an invertible matrix. If λ and v are an eigenvalue-eigenvector pair of the matrix A,
then, λ1 and v are an eigenvalue-eigenvector pair of the matrix A−1 .

Proof. (a) Proof by Induction. The base case k = 1 is obvious.
For the inductive step, assume that λ k−1 and v are an eigenvalue-eigenvector pair for Ak−1 . Then the
following calculation shows us that λ k and v are an eigenvalue-eigenvector pair for Ak :




Ak v = A Ak−1 v = A λ k−1 v = λ k v.
(b) A is invertible and hence, in the statement λ ̸= 0. Since Av = λ v we have A−1 (λ v) = v and so λ A−1 v =
v, which (since λ ̸= 0) is equivalent to A−1 v = λ1 v.
□
Another important property that we used in Example 8.2.9 was that we were able to write a vector as a
linear combination of eigenvectors. This was possible because the eigenvectors were linearly independent.

Lemma 8.3.2. Let A ∈ Rn×n and let v1 , . . . , vk ∈ Rn be eigenvectors corresponding to eigenvalues
λ1 , . . . , λk ∈ R. If λ1 , . . . , λk are all distinct, the eigenvectors v1 , . . . , vk are linearly independent.

Proof. We will prove this by contradiction. Assume that v1 , . . . , vk are linearly dependent. For i = 1, . . . , k,
let di denote the dimension of the span of v1 , . . . , vi . Since v1 ̸= 0 we have d1 = 1. By the hypothesis dk < k.
Let j be the smallest positive integer for which d j < j. Note that, by construction, d j−1 = d j = j − 1. This

44

means that v1 , . . . , v j−1 are linearly independent but that v j is in the span of v1 , . . . , v j−1 . We can then write
v j = α1 v1 + · · · α j−1 v j−1 .

(32)
If we multiply by A both sides we get

λ j v j = Av j = A (α1 v1 + · · · α j−1 v j−1 ) = α1 λ1 v1 + · · · λ j−1 α j−1 v j−1 .
Replacing the v j in the left hand side with the right hand side of (32) we get
λ j (α1 v1 + · · · α j−1 v j−1 ) = α1 λ1 v1 + · · · λ j−1 α j−1 v j−1 ,
which we can rearrange as
(33)

α1 (λ j − λ1 ) v1 + α2 (λ j − λ2 ) v2 + · · · + α j−1 (λ j − λ j−1 ) v j−1 = 0.

Since λ j − λi ̸= 0 for all i ≤ j − 1 and not all αi ’s are zero, this is a non-zero linear combination of
v1 , . . . , v j−1 adding to zero, which would be a contradiction with d j−1 = j − 1.
2

A very important consequence of this is that if a matrix has n distinct real eigenvalues then the eigenvectors
form a basis for Rn .
Theorem 8.3.3. Let A ∈ Rn×n with n distinct real eigenvalues (meaning that the n zeros of det(A−λ I),
as described in Corollary 8.1.3, are all distinct) then there is a basis of Rn , v1 , . . . , vn , made up of
eigenvectors of A.

Let us next discuss several further properties of the eigenvalues.
Definition 8.3.4. Let A ∈ Rn×n .
(34)

P(z) = (−1)n det(A − zI) = det(zI − A) = (z − λ1 )(z − λ2 ) · · · (z − λn ).

The polynomial P(z) in (34) is called the characteristic polynomial of the matrix A. The eigenvalues
λ1 , . . . , λn as they show up in (34) are not all distinct in general. The number of times an eigenvalue shows
up is called the algebraic multiplicity of the eigenvalue.
The trace of A is defined as Tr(A) = ∑ni=1 Aii .
As a side remark, there is a converse to this in the sense that any monic polynomial can be written as a
characteristic polynomial of a matrix. There is a particularly elegant way to build the matrix, if you are
interested in learning more, look-up “companion matrix”.
We continue with exploring further properties of the eigenvalues.
Lemma 8.3.5. The eigenvalues of A ∈ Rn×n are the same as the ones of A⊤ .

45

Proof. This follows from (34), and the fact that, for det(A − zI) = det((A − zI)⊤ ) = det(A⊤ − zI).

2

Even though the eigenvalues of A and A⊤ are the same, the eigenvectors are in general not! Try to verify
this in an example.
A link between eigenvalues, the determinant and the trace of a matrix is established below.

Lemma 8.3.6. Let A ∈ Rn×n and λ1 , . . . , λn its n eigenvalues as they show up in (34). Then
n

n

det(A) = ∏ λi and Tr(A) = ∑ λi .
i=1

i=1

Proof. Let us consider the characteristic polynomial.
(−1)n det(A − zI) = (z − λ1 )(z − λ2 ) · · · (z − λn )
k
n n
= zn + (− ∑ni=1 λi )zn−1 + ∑n−2
k=1 bk z + (−1) ∏i=1 λi ,
where bk ∈ C.
Set z = 0 in the expression above. It gives (−1)n det(A) = (−1)n ∏ni=1 λi as claimed.
For the second claim, note that the coefficient of zn−1 in the characteristic polynomial (34) is given in
the right hand side by (− ∑ni=1 λi ). On the left hand side the coefficient of zn−1 can only come from the
permutation that takes all diagonal elements in the matrix zI − A. Hence it is the coefficient of zn−1 of
2
∏ni=1 (z − Aii ) which is − ∑ni=1 Ai i = − Tr(A).
Lemma 8.3.6 is quite surprising. A ∈ Rn×n and hence, det(A) ∈ R and Tr(A) ∈ R. The eigenvalues
λ1 , . . . , λn ∈ C are in general not real. Still the sum of all eigenvalues is always a real number and their
product as well. Can you see why this is true? When calculating eigenvalues, Lemma 8.3.6 can also
be useful. For instance, imagine we are given a real 3 × 3-matrix A and established that λ1 = 1 is an
eigenvalue of A. Suppose that we also know that det(A) = 4 and Tr(A) = 5. Then A must have eigenvalues
λ2 = λ3 = 2. It is also elementary to verify the following calculations involving the trace-operator.

Lemma 8.3.7. For matrices A, B,C ∈ Rn×n one has
Tr(AB) = Tr(BA) and Tr(ABC) = Tr(BCA) = Tr(CAB).

Proof.
n

n

n

n

Tr(AB) = ∑ ∑ Ai j B ji = ∑ ∑ B ji Ai j = Tr(BA).
i=1 j=1

j=1 i=1

46

Tr(ABC) = Tr(A(BC)) = Tr((BC)A) = Tr(B(CA)) = Tr(CAB).
2
Let us end this chapter with a few additional comments. The eigenvalues of A + B are not easily computed
from the eigenvalues of A and the ones of B, in particular they are not their sum! Also, the eigenvalues of
AB or BA are not easily computed from the eigenvalues of A and the ones of B. They might not be their
product!
More importantly, Gaussian Elimination does not preserve eigenvalues and eigenvectors. In this respect,
eigenvalues heavily rely on the representation of the given matrix and do less reflect properties of corresponding subspaces etc.
Last, but not least let us return to Example 8.2.2. This example provided us with insight into properties of
eigenvalues and eigenvectors! Here are a couple of observations, which although outside of the core scope
of this course, have significant impact in several areas:
Notice that since |λ2 | < |λ1 |, the contribution of λ2n α2 v2 becomes negligible (when compared to λ1n α1 v1 ) as
n → ∞. This observation can be used in a clever way: we can approximate the eigenvector v1 by An g0 and
so if we have a fast way to do matrix-vector multiply, we can approximate eigenvalues and eigenvectors.
This is often referred to as the Power Method. In fact, Google’s celebrated PageRank algorithm is based
on the idea of how eigenvectors can be used for ranking. You can read more about it here [BSS]. You
might also want to take a look at “Landau on Chess Tournaments and Google’s PageRank” by Rainer Sinn
and Günter M. Ziegler (https://arxiv.org/pdf/2210.17300.pdf).
The vector gn gets larger and larger as n → ∞ because |λ1 | > 1. If both eigenvalues satisfied |λ | < 1
then gn → 0 as n → ∞. This illustrates the importance of the largest absolute values of the eigenvalues
of a matrix in understanding the long term behaviour of systems of the form An g0 for some A. If it
represents a dynamical system it is related to stability or instability/chaos. If it represents the evolution of
an economical system over time (or the finances of a company) it can be the difference between growth or
ruin.

9. D IAGONIZABLE MATRICES AND THE SINGULAR VALUE DECOMPOSITION
In this chapter we will explore a deep connection between a matrix defining a linear transformation
from the column space to the row space and the representation of these subspaces. Our guiding question
here is when can we find a basis of the subspace of the rows and columns such that viewed in this basis our
matrix has the simplest possible representation in form of a diagonal matrix. On a first glance, this topic
might have little to do with eigenvalues and eigenvectors. This is, however not true as we will show below.

47

9.1. From eigenvalues to diagonizable matrices.
An important part of the success of the strategy we took in Example 8.2.2 was the fact that we were able
to build a basis of R2 with eigenvectors of the matrix M. In Theorem 8.3.3 we showed that we can always
build a basis of Rn with eigenvectors of an n × n matrix A if A has n distinct real eigenvalues.
Let us show next that whenever we have a basis of Rn that consists of real eigenvectors of a matrix A,
then we can change the basis of the row and column space such that A can be viewed as a diagonal matrix
in this new representation. This is a remarkable fact that explains why real eigenvectors of a matrix are
important.
Theorem 9.1.1. Let A ∈ Rn×n . Suppose that A has eigenvectors v1 , . . . , vn ∈ Rn that form a basis of
Rn . For i ∈ {1, . . . , n} let λi be the eigenvalue associated to vi . Let V be the matrix whose columns
correspond to the eigenvectors v1 , . . . , vn . Then,
(35)

A = V ΛV −1 ,

where Λ is a diagonal matrix with Λii = λi (and Λi j = 0 for all i ̸= j).

Proof. Since v1 , . . . , vn is a basis of Rn , the matrix V is invertible. Hence, it suffices to verify that
(36)

V −1 AV = Λ.

This can be done by direct calculation: For any 1 ≤ j ≤ n, the j-th column of the matrix V −1 AV is given
by


V −1 AV · j := V −1 AV e j = V −1 Av j = V −1 λ j v j = λ jV −1 v j = λ j e j ,
since V −1 v j = V −1Ve j = e j . Recall that e j is the vector in Rn with a 1 in j-th entry and zero elsewhere.
Since for any 1 ≤ j ≤ n, λ j e j is also the j-th column of Λ, we have that V −1 AV = Λ.
2

Many properties of the eigenvalues that we discussed in the previous chapter are easy to prove by using
Theorem 9.1.1. Try it!
Theorem 9.1.1 justifies the following two definitions.
Definition 9.1.2 (Diagonalizable Matrix). A matrix A ∈ Rn×n is called a diagonalizable matrix if there
exists an invertible matrix V such that V −1 AV = Λ, where Λ is a diagonal matrix.
Definition 9.1.3. If, given a matrix A ∈ Rn×n , we can build a basis of Rn with eigenvectors of A we say
that A has a complete set of real eigenvectors.
If we have a matrix A ∈ Rn×n with a complete set of real eigenvectors then Theorem 9.1.1 tells us that
the corresponding linear transformation, when viewed in the basis v1 , . . . , vn is simply a diagonal matrix.
This is a remarkable fact: since many matrices have a complete set of real eigenvectors this says that all
the corresponding linear combinations, regardless of how complicated they might seem, are actually just

48

a diagonal operation when viewed in the basis v1 , . . . , vn . Let us now study classes of matrices for which
there exists a complete set of real eigenvectors.
Example 9.1.4. For D ∈ Rn×n a diagonal matrix, the eigenvalues of D are the diagonal entries of D. The
canonical basis e1 , . . . , en is a complete set of real eigenvectors of D.
Another obvious fact is
Example 9.1.5. The eigenvalues of an n × n triangular matrix are the n values in the diagonal. However,
triangular matrices may not have a complete set of real eigenvectors. Try to find an example!
The following class of matrices has been studied in the context of projections.
Proposition 9.1.6 (Eigenvalues and Eigenvectors of a Projection Matrix). Let P be the projection
matrix on the subspace U ⊆ Rn . Then P has two eigenvalues, 0 and 1, and a complete set of real
eigenvectors.

Proof. Let m be the dimension of U. Let u1 , . . . , um be an orthonormal basis of U, and w1 , . . . , wn−m an
orthonormal basis of U ⊥ . It is easy to see that Puk = 1uk for any 1 ≤ k ≤ m and Pwk = 0wk for any
1 ≤ k ≤ n − m, so all n vectors are eigenvectors of P (with eigenvalues either 1 or 0). By construction of
U ⊥ , they form an orthonormal basis.
2

We can also apply operations on matrices that preserve the property of having a complete set of real
eigenvectors.
Definition 9.1.7 (Similar matrices). We say that A ∈ Rn×n and B ∈ Rn×n are similar matrices if there exists
an invertible matrix S such that B = S−1 AS.

Proposition 9.1.8. Similar matrices A ∈ Rn×n and B = S−1 AS ∈ Rn×n have the same eigenvalues. The
matrix A has a complete set of real eigenvectors if and only if B does.

Proof. v ∈ Rn is eigenvector associated with eigenvalue λ ∈ R for matrix A iff
Av = λ v ⇐⇒ λ S−1 v = S−1 Av = S−1 AS S−1 v = B(S−1 v).
Hence, the eigenvalues of A and B coincide. Moreover, v is eigenvector associated with eigenvalue λ ∈ R
for matrix A iff S− 1v is eigenvector associated with eigenvalue λ ∈ R for matrix B. All eigenvectors
v1 , . . . , vn of A form a basis of Rn iff all eigenvectors S−1 v1 , . . . , S−1 vn of B form a basis of Rn . This
completes the proof.
2

49

We have by now seen various classes of matrices that are diagonazible. But not all matrices allow us to
work with this beautiful and powerful property.
Example 9.1.9.
"

#
0 1
(a) The matrix A =
does not have two linearly independent eigenvectors. Indeed, det(A −
0 0
λ I) = λ 2 which means that λ = 0 is the only eigenvalue and it has algebraic multiplicity 2. However, N(A − 0I) = N(A) only has dimension 1. Hence, there is only one eigenvector (and multiples
of it) corresponding to
" eigenvalue
# 0.
0 0
(b) The zero matrix A =
does have two linearly independent eigenvectors. Indeed, det(A −
0 0
λ I) = λ 2 which means that λ = 0 is the only eigenvalue and has algebraic multiplicity 2. But,
unlike in (a), we now notice that N(A − 0I) = N(A) has dimension 2. Hence there is a basis made
up of two eigenvectors (in fact any two linearly independent vectors will be such a basis).

For the matrix A considered in Example 9.1.9 (a), we obtain that N A2 has dimension 2. When there exist
a positive integer k such that Ak = 0 we call A nilpotent. There is a rather deep Theorem that essentially
says nilpotency is the only obstacle to get a complete set of eigenvectors. It roughly says that when there
are “missing” eigenvectors they can be found in the nullspace of powers of A − λ I. This gives rise to
something called “Jordan Normal Form”, a topic beyond the scope of this course.
In general when there is an eigenvalue λ ∈ R with algebraic multiplicity larger than 1, it can be that
N(A − λ I) is of large enough dimension to find enough linearly independent eigenvectors. This applies,
for instance to projection matrices as we observed above. It does, however, not apply to the nilpotent
example.
Definition 9.1.10. Given a matrix A ∈ Rn×n and an eigenvalue λ of A we call the dimension of N(A − λ I)
the geometric multiplicity of λ .
A link between the property that a matrix has a complete set of eigenvectors and the algebraic and geometric multiplicities of its eigenvalues is given below.
Lemma 9.1.11. A matrix has a complete set of real eigenvectors if all its eigenvalues are real and the
geometric multiplicities are the same as the algebraic multiplicites of all its eigenvalues.

Proof. Let A ∈ Rn×n be a matrix with eigenvalues λ1 , . . . , λk ∈ R. Let mi be the algebraic multiplicity
of eigenvalue λi , i.e., dim(N(A − λi I)) = mi . For i ∈ {1, . . . , k} take a basis vi1 , . . . , vimi of the subspace
N(A − λi I). From Lemma 8.3.2 it follows that for i ̸= j we have that the vectors vis and v jl are linearly
independent. Since m1 + m2 + . . . + mk = n, the set
k
[

{vi1 , . . . , vimi }

i=1

50

defines a basis of Rn .
2

So far we established a connection between diagonalize matrices and their eigenvectors, provided that
they form a basis of the underlying space. What can we do if this is not true? Or even more generally, the
concept of eigenvectors and eigenvalues only applies to square matrices. Is there something more general
that we can discover for m × n matrices? In order to prepare for this task let us take a detour in studying a
linear transformation written in different bases.
Let A ∈ Rm×n be a matrix representing a linear transformation
L : Rn → Rm
given by x ∈ Rn → Ax ∈ Rm , with both input and output written in the canonical bases as
n

m

x = ∑ x j e j and Ax = ∑ (Ax)i ei .
j=1

i=1

Recall (Example 6.3.2) that (ei ) j = δi j , and that (Ax)i is the i-th entry of the vector Ax.
Suppose that we are given a basis u1 , . . . , un of Rn and a basis v1 , . . . , vm of Rm . Our task is to understand
the linear transformation L written in these bases. Then L takes a vector x = ∑nj=1 α j u j and outputs L(x) =
∑nj=1 βi vi . We want to compute the matrix B that takes




α1
β1
 . 
 . 
 . 
. 
α =
 .  to Bα =  .  .
αn
βm
To achieve this, let U ∈ Rn×n be the matrix whose columns are the basis elements u1 , . . . , un and V ∈ Rm×m
the matrix whose columns are the basis elements v1 , . . . , vm . Then,
x = Uα and L(x) = V β . Hence, β = V −1 AUα.
The matrix B, corresponding to the linear transformation L represented in the new basis is B = V −1 AU.
Note that we can make such a change of bases between any pair of bases. It is not required that one basis
is the canonical basis. In this general case, the role of U and V would be played by the change of the basis
matrix (the matrix that maps the coefficients of a vector represented in the old basis to its coefficients when
written in the new basis).

51

n

(37)

L

L : Rn → Rm
!
n

=

∑ x je j

j=1

n

L

∑ (Ax)i ei

i=1

!

∑ α ju j

j=1

where B = V −1 AU ∈ Rm×n ,

n

=

∑ (Bα)i vi

i=1

U=

h

u1 · · ·

linear transformation


x1
 . 
. 
x=
 . 
xn


α1
 . 
. 
α =
 . 
αn
i
h
un ∈ Rn×n , V = v1 · · ·

vm

i

∈ Rm×m .

Let us connect this topic with with our discussions on eigenvectors of a square matrix A ∈ Rn×n . In
particular, let A be a matrix with a complete set of real eigenvectors (in the sense of Definition 9.1.3) and
let v1 , . . . , vn ∈ Rn×n be a basis formed with eigenvectors of A. If we write a vector x ∈ Rn as x = ∑ni=1 αi vi
then Ax = ∑ni=1 λi αi vi . One way to think about this is that the linear transformation corresponding to the
matrix A, when written in the basis V is simply a diagonal matrix/transformation. This is the key idea
behind matrix diagonalization and one of the most important facts in Linear Algebra.
9.2. Symmetric Matrices and the Spectral Theorem.
We continue our discussion about classes of matrices that allow us to perform a change of basis so that
the corresponding linear transformation becomes diagonal. In fact, this section deals with an important
class, namely real symmetric matrices meaning matrices A ∈ Rn×n for which A⊤ = A. The main goal of
this section is to prove the Spectral Theorem.

Theorem 9.2.1 (Spectral Theorem). Any symmetric matrix A ∈ Rn×n has n real eigenvalues and an
orthonormal basis of Rn consisting of its eigenvectors.

Together with Theorem 9.1.1 this implies the following corollary.

Corollary 9.2.2. For any symmetric matrix A ∈ Rn×n there exists an orthogonal matrix V ∈ Rn×n
(whose columns are eigenvectors of A) such that
A = V ΛV ⊤ ,
where Λ ∈ Rn×n is a diagonal matrix with the eigenvalues of A in its diagonal (and V ⊤V = I).

Remark 9.2.3 (Eigendecomposition). The decompositions in Corollary 9.2.2 and Theorem 9.1.1 are
called Eigendecompositions.

52

The following follows easily from the Spectral Theorem.

Corollary 9.2.4. The rank of a real symmetric matrix A is the number of non-zero eigenvalues (counting repetitions).

Remark 9.2.5. For general n × n (non-symmetric) matrices, the rank is n minus the dimension of the
nullspace, so it is n minus the geometric multiplicity of λ = 0. Since symmetric matrices always have
a complete set of eigenvalues and eigenvectors, the geometric multiplicities are always the same as the
algebraic multiplicities.

Proposition 9.2.6. Let A be a real n × n symmetric matrix and let v1 , . . . , vn be an orthonormal basis
of eigenvectors of A (the columns of the matrix V in Corollary 9.2.2) and λ1 , . . . , λn the associated
eigenvalues. Then
n

A = ∑ λi vi v⊤
i
k=1

Proof. Follows directly from Corollary 9.2.2.

2

We “build up” to the proof of Theorem 9.2.1 with a few observations. The first one shows us that symmetric
matrices have a very strong property. Whereas for all square matrices, eigenvectors corresponding to
distinct eigenvalues are linearly independent, the symmetry of a matrix implies a much stronger property.

Lemma 9.2.7. Let A ∈ Rn×n be a symmetric matrix and λ1 ̸= λ2 ∈ R be two distinct eigenvalues of A
with corresponding eigenvectors v1 , v2 . Then v1 and v2 are orthogonal.

Proof. Suppose that λ1 ̸= λ2 are eigenvalues of a real symmetric matrix A and v1 , v2 ∈ Rn \ {0} corresponding eigenvectors. Then
⊤
⊤ ⊤
⊤
⊤
⊤
λ1 v⊤
1 v2 = (Av1 ) v2 = v1 A v2 = v1 Av2 = v1 (Av2 ) = λ2 v1 v2 .

The fact that λ1 ̸= λ2 implies that v⊤
1 v2 = 0
A second observation is also important to know.

Lemma 9.2.8. Let A ∈ Rn×n be a symmetric matrix and λ ∈ C an eigenvalue of A, then λ ∈ R.

2

53

Proof. Let v ∈ Cn be an eigenvector associated with the eigenvalue λ . We have Av = λ v. Recall that, for
⊤
a matrix (or vector) M, its Hermitian conjugate is given by M ∗ = M . Since A is real symmetric we have
A∗ = A. Thus, we have
λ ∥v∥2 = λ v∗ v = (λ v)∗ v = (Av)∗ v = v∗ A∗ v = v∗ Av = v∗ λ v = λ ∥v∥2 .
Since v ̸= 0, then ∥v∥ ̸= 0 and so λ = λ . This implies that λ ∈ R.

2

This, together with Theorem 8.2.5, immediately implies the following.

Corollary 9.2.9. Every symmetric matrix A ∈ Rn×n has a real eigenvalue λ .

Corollary 9.2.9 is a great example of the usefulness of complex numbers. Even though it is a statement just
about real matrices and real eigenvalues we proved it by going through the complex numbers and using
results in Complex Analysis.
We are now prepared to prove the spectral theorem. The fact that two eigenvectors of a real symmetric matrix corresponding to distinct eigenvalues are orthogonal follows from Lemma 9.2.7. The main difficulty
is proving that the matrix indeed has a complete set of eigenvectors.
Proof. [of Theorem 9.2.1]
Let A ∈ Rn×n be a symmetric matrix. We will prove the following by induction, which for k = n implies
the theorem we want to show:
for any 1 ≤ k ≤ n there are k orthogonal eigenvectors of A.
The base case k = 1 follows from Corollary 9.2.9 as we can normalize an eigenvector to have norm one.
We now assume that the statement is true for k and show it for k + 1. We will show that if a real symmetric
matrix A has k (with 1 ≤ k < n) orthonormal eigenvectors then we can build an extra one, orthogonal to
the others. Note that in order to achieve norm 1 we simply need to normalize the vector.
Let v1 , . . . , vk denote k orthonormal eigenvectors of A and λ1 , . . . , λk the respective eigenvalues. Let
uk+1 , . . . , un be an orthonormal basis of the orthogonal complement of the span of v1 , . . . , vk . Let Vk be
the n × n matrix whose i-th column is vi if i ≤ k and ui if i > k. Vk is an orthogonal matrix. Moreover, let

54

us define B ∈ Rn×n as B = V ⊤ AV , then:


— v⊤
— 

1


..


.



|
|
|
| 
 — v⊤ —  




k
B = V ⊤ AV = 
  Av1 · · · Avk Auk+1 · · · Aun 

⊤
 — uk+1 —  


 |
|
|
|

..




.
— u⊤
—
n


— v⊤
— 

1


..


.



|
|
|
| 
 — v⊤ —  




k
= 
  λ v1 · · · λ vk Auk+1 · · · Aun 

⊤
 — uk+1 —  

 |

|
|
|

..




.
— u⊤
—
n
"
#
Λk
0k×(n−k)
=
,
0(n−k)×k
C
where Λk is a diagonal matrix with λ1 , . . . , λk in the diagonal, 0(n−k)×k and 0k×(n−k) are zero matrices of
size respectively (n − k) × k and k × (n − k). C is a (n − k) × (n − k) symmetric matrix. Note that we obtain
here zero matrices of size 0(n−k)×k and 0k×(n−k) , respectively because
for all i and j we have uTj vi = 0 and since A is symmetric vTi Au j = vTi AT u j = (Avi )T u j = λi vTi u j = 0.
Since C is a (n − k) × (n − k) symmetric matrix, Theorem 9.2.9 implies it has a real eigenvalue λk+1 and a
real eigenvector y ∈ Rn−k . Let w ∈ Rn be the vector with 0 in the first k coordinates and y in the remaining
n − k, in other words
(
0 if i ≤ k
wi =
yi−k if i > k.
We have
"
Bw =

Λk
0(n−k)×k

0k×(n−k)
C

#

0k×1
y

!
=

0k×1
Cy

!
=

0k×1
λk+1 y

!
= λk+1 w.

Let vk+1 := V w. Since V is orthogonal we have that A = V BV ⊤ . Thus,
Avk+1 = V BV ⊤ vk+1 = V Bw = V λk+1 w = λk+1 vk+1 ,
so vk+1 is an eigenvector of A. To see that it is orthogonal to v1 , . . . , vk note that the inner products v⊤
i vk+1
for i ≤ k appear in the first k entries of V ⊤ vk+1 = w and that w has its first k coordinates 0 by construction.
By normalizing the vector we can ensure that it attains unit norm.
2

A first application of the Spectral Theorem is presented next.

55

Proposition 9.2.10 (Rayleigh Quotient). Given a symmetric matrix A ∈ Rn×n the Rayleigh Quotient,
defined for x ∈ Rn \ {0}, as
x⊤ Ax
R(x) = ⊤
x x
attains its maximum at R(vmax ) = λmax and its minimum at R(vmin ) = λmin where λmax and λmin are,
respectively, the largest and smallest eigenvalues of A and vmax , vmin their associated eigenvectors.

Proof. It is easy to see that R(vmax ) = λmax and R(vmin ) = λmin . Thus it suffices to show that, for all
x ∈ Rn \ {0} we have λmin ≤ R(x) ≤ λmax . Using Proposition 9.2.6 we can write, for x ∈ Rn \ {0},

2
x⊤ ∑ni=1 λi vi v⊤
∑ni=1 λi x⊤ vi
i x
=
,
R(x) =
∥x∥2
∥x∥2
where v1 , . . . , vn form an orthonormal basis of eigenvectors of A and λ1 , . . . , λn are the associated eigenval2
ues. Since x⊤ vi ≥ 0 for all 1 ≤ i ≤ n we have that, for all 1 ≤ i ≤ n,

2

2

2
λmin x⊤ vi ≤ λi x⊤ vi ≤ λmax x⊤ vi .
Collecting all these inequalities we get
∑n x⊤ vi
λmin i=1 2
∥x∥

2

∑n λi x⊤ vi
≤ i=1
∥x∥2

2

∑n x⊤ vi
≤ λmax i=1 2
∥x∥

2
.

To conclude the proof note that, since the vi ’s are orthonormal, the matrix V with the vi ’s as columns is
2
2
∑n (x⊤ vi )
= 1.
2
orthogonal and ∑ni=1 x⊤ vi = ∥V x∥2 = ∥x∥2 and so i=1∥x∥2
Proposition 9.2.10 allows us to give a characterization of an important class of matrices. This class is
called Positive Definite and Positive Semidefinite matrices and defined below.
Definition 9.2.11. A symmetric matrix A ∈ Rn×n is said to be Positive Semidefinite (PSD) if all its eigenvalues are non-negative. If all the eigenvalues of A are strictly positive then we say A is Positive Definite
(PD).
The following characterization can be derived from Proposition 9.2.10.

Proposition 9.2.12. A symmetric matrix A ∈ Rn×n is Positive Semidefinite if and only if x⊤ Ax ≥ 0 for
all x ∈ Rn . Analogously, a symmetric matrix A ∈ Rn×n is Positive Definite if and only if x⊤ Ax > 0 for
all x ∈ Rn \ {0}.

Note that PSD and PD matrices are closed under taking additions: if A and B are PSD (or PD), then so is
the sum. This can be verified by applying Proposition 9.2.10. Try it!

56

Definition 9.2.13 (Gram Matrix). Given n vectors, v1 , . . . , vn in Rm we call their Gram Matrix the n × n
matrix of inner products
Gi j = v⊤
i v j.
If V ∈ Rm×n is the matrix with columns v1 , . . . , vn in Rm , then G = V ⊤V is the Gram matrix of V .
Remark 9.2.14. Given a matrix A ∈ Rm×n , as an abuse of notation, we sometimes also call AA⊤ a Gram
matrix of A. Notice that, if a1 , . . . , an ∈ Rm are the columns of A then AA⊤ is m × m and
n

AA⊤ = ∑ ai a⊤
i .

(38)

i=1

Proposition 9.2.15. Given a real matrix A ∈ Rm×n , the non-zero eigenvalues of A⊤ A ∈ Rn×n are the
same as the ones of AA⊤ ∈ Rm×m . Both matrices are symmetric and positive semidefinite.

Proof. Let r be the rank of A. We know
rank(A) = rank(A⊤ ) = rank(A⊤ A) = rank(AA⊤ ).
Both matrices A⊤ A and AA⊤ are obviously symmetric. Let us check that they are positive semidefinite. To
see this, we have x⊤ A⊤ Ax = ∥Ax∥2 ≥ 0 for all x which implies A⊤ A is PSD, and the same argument can
be used for AA⊤ .
Now, both AA⊤ and A⊤ A have a complete set of real eigenvalues and orthogonal eigenvectors. Let
λ1 , . . . , λr be the r non-zero eigenvalues of A⊤ A and v1 . . . , vr be the corresponding eigenvectors. We
have, for 1 ≤ k ≤ r, A⊤ Avk = λk vk , multiplying by A both sides we get AA⊤ Avk = λk Avk and so λk
is an eigenvalue of AA⊤ with eigenvector Avk (note that Avk ̸= 0). Furthermore, For j ̸= k we have
(Av j )⊤ (Avk ) = v⊤j A⊤ Avk = v⊤j λk vk = λk v⊤j vk = 0 and so the r eigenvectors of AA⊤ built this way are
orthogonal, and so λ1 , . . . , λr are the nonzero eigenvectors of AA⊤ .
2

Proposition 9.2.16. [Cholesky decomposition] Every symmetric positive semidefinite matrix M is a
Gram matrix of an upper triangular matrix C. M = C⊤C is known as the Cholesky Decomposition.4

Proof. Let M be a symmetric positive semidefinite matrix. Corollary 9.2.2 gives us a decomposition
M = V ΛV ⊤ with Λ a diagonal matrix with the eigenvalues of M in the diagonal. Since M is PSD, the
diagonal entries of Λ are non-negative and so we can build Λ1/2 by taking the square root of each diagonal

⊤
entry of Λ. Then M = V Λ1/2 V Λ1/2 . To make the matrices in the decomposition be upper triangular,
⊤
simply take the QR decomposition (recall Definition 6.3.10) V Λ1/2 = QR with Q such that Q⊤ Q = I

⊤
and R upper triangular. We have M = V Λ1/2 V Λ1/2 = (QR)⊤ (QR) = R⊤ Q⊤ QR = R⊤ R. Taking
C = R establishes the Proposition.
2

57

At first glance, symmetric matrices look very special (since we must have A⊤ = A), but they actually
appear very often in both applications and pure mathematics. There are (at least) two reasons for this:
(i) For any matrix B we can form a symmetric matrix B⊤ B from which we can study B. This is going to be
the key idea behind the Singular Value Decomposition that we discuss next.
(ii) In many instances, matrices represent relationship between objects — for example, Ai j can represent a
friendship connection (or a similarity measure) between person (or data point) i and j and in many cases
such relationships are symmetric.

9.3. Singular Value Decomposition.
We are now reaching “the ultimate theorem of our class”, the Singular Value Decomposition (SVD). The
SVD is a way to generalize the eigendecomposition to non-symmetric, and even non-square, matrices.
Instead of eigenvalues we will have singular values and instead of eigenvectors we will have (right and
left) singular vectors.
Definition 9.3.1 (SVD — Singular Value Decomposition). Let A ∈ Rm×n . There exist orthogonal matrices
U ∈ Rm×m and V ∈ Rn×n such that
(39)

A = UΣV ⊤ ,

where Σ ∈ Rm×n is a diagonal matrix, in the sense that Σi j = 0 when i ̸= j, and the diagonal elements are
non-negative and ordered in descending order. U ⊤U = I and V ⊤V = I.
The columns u1 , . . . um of U are called the left singular vectors of A and are orthonormal. The columns
v1 , . . . vn of V are called the right singular vectors of A and are orthonormal. The diagonal elements of Σ,
σi = Σii are called the singular values of A and are ordered as
σ1 ≥ · · · ≥ σmin{m,n} .
Remark 9.3.2. If A has rank r we can write the SVD in a more compact form:
(40)

A = Ur ΣrVr⊤ ,

where Ur ∈ Rm×r contains the first r left singular vectors, Vr ∈ Rn×r contains the first r right singular
vectors and Σr ∈ Rr×r is a diagonal matrix with the first r singular values. Notice that storing such
a decomposition in the computer requires storing r × (m + n + 1) real numbers rather than m × n real
numbers which would be required to store A naively. When a matrix has small rank these are crucial
savings. Taking this one step forward, when a matrix is well approximated by a low rank matrix, oftentimes
one stores only a small rank approximation of a matrix A. This is a crucial idea in tasks ranging from Image
Compressions, Numerical Analysis, and Machine Learning.
Oftentimes the subscript is omitted and the compact SVD is simply written as UΣV ⊤ while specifying the
dimensions of the matrices involved to specify which form of the SVD is being considered.

58

Let A ∈ Rm×n and A = UΣV ⊤ be its SVD (as in (39)) then


AA⊤ = U ΣΣ⊤ U ⊤ ,
and so the left singular vectors of A, the columns of U, are the eigenvectors of AA⊤ and the singular values
of A are the square-root of the eigenvalues of AA⊤ (note that ΣΣ⊤ is m × m diagonal). If m > n, A has n
singular values and AA⊤ has m eigenvalues (which is larger than n), but the “missing” ones are 0.
Analogously,


A⊤ A = V Σ⊤ Σ V ⊤ ,
and so the right singular vectors of A, the columns of V , are the eigenvectors of A⊤ A and the singular
values of A are the square-root of the eigenvalues of A⊤ A (note that Σ⊤ Σ is n × n diagonal). If n > m, A
has m singular values and A⊤ A has n eigenvalues (which is larger than m), but the “missing” ones are 0.
This observation makes it easier to write the singular values and singular vectors of A in terms of eigenvalues and eigenvectors of AA⊤ and A⊤ A, which are symmetric matrices (and directly implies, e.g., uniqueness of singular values; and the fact that the rank of a matrix is the number of nonzero singular values). In
fact, the proof of the existence of a SVD will heavily rely on the Spectral Theorem.

Theorem 9.3.3. Every matrix A ∈ Rm×n has an SVD decomposition of the form (39). In other words:
Every linear transformation is diagonal when viewed in the bases of the singular vectors.

Proof. Let A ∈ Rm×n . Let r be the rank of A. We will build a compact SVD as in (40). It is easy to see
that we can get an SVD in the sense of (39) from a compact one by adding singular values that are zero
and extending the singular vectors in both Ur and Vr to orthonormal bases.
By Theorem 9.2.1 and Corollary 9.2.2 the matrix AA⊤ has a complete set of orthonormal eigenvectors and
can be written as
(41)

AA⊤ = UΛU ⊤ ,

where U ∈ Rm×m is orthogonal and Λ is diagonal. Let us write (41) by ordering the diagonal entries of Λ
in decreasing order. Furthermore, let us write (41) also in a compact form, by keeping only the r non-zero
eigenvalues (and corresponding eigenvectors), i.e.,
AA⊤ = Ur ΛrUr⊤
for Ur ∈ Rm×r such that Ur⊤Ur = I and Λr is r × r diagonal with the non-zero eigenvalues of AA⊤ . By
Proposition 9.2.15 the eigenvalues of AA⊤ are non-negative and so the diagonal entries of Λr are positive.
√
Let Σr ∈ Rr×r be the diagonal matrix with diagonal entries σi := (Σr )ii = Λii . Our goal is to show that
⊤
there is a n × r matrix Vr , with orthonormal columns, such that A = Ur ΣrVr⊤ . We would have Σ−1
r Ur A =
⊤
⊤
⊤
⊤
−1
Σ−1
r Ur Ur ΣrVr = Vr , or equivalently Vr = A Ur Σr . Motivated by this, let us set
Vr := A⊤Ur Σ−1
r .

59

This corresponds to a matrix with columns v1 , . . . , vr given by vk = σ1k A⊤ uk . To conclude we need to show
that this construction indeed gives a compact SVD. This requires us to show two statements:
(1) Vr⊤Vr = I. This can be verified by direct computation, while recalling that AA⊤ = Ur ΛrUr⊤ :

⊤
⊤
⊤
−1
−1 ⊤
⊤
−1
−1 ⊤
⊤
−1
−1
−1
Vr Vr = A Ur Σr
A⊤Ur Σ−1
r = Σr Ur AA Ur Σr = Σr Ur Ur ΛrUr Ur Σr = Σr Λr Σr = I
(2) A = Ur ΣrVr⊤ . Note that

⊤
Ur ΣrVr⊤ = Ur Σr A⊤Ur Σ−1
= UrUr⊤ A.
r
Let us simply verify that A = UrUrT A.
• Let x ∈ N(A). Then Ax = 0 = UrUrT 0 = UrUrT Ax.
• Let x ∈ C(AT ). It follows that x = AT y for y ∈ Rm and hence,
Ax = AAT y = Ur ΛrUrT y = Ur IΛrUrT y = UrUrT Ur ΛrUrT y = UrUrT AAT y = UrUrT Ax.
Hence, for all x ∈ Rn we have verified that Ax = UrUrT Ax. Then A = UrUrT A follows.
2

An important direct consequence of the SVD, and in particular of (40) is that we can write any rank-r
matrix A ∈ Rm×n as a sum of r rank-1 matrices:
Proposition 9.3.4. Let A ∈ Rm×n be a matrix with rank r. Let σ1 , . . . , σr be the non-zero singular
values of A, u1 , . . . , ur the corresponding left singular vectors and v1 , . . . , vr the corresponding right
singular vectors. Then
r

(42)

A = ∑ σk uk v⊤
k.
k=1

The SVD is a powerful tool. Many of the results that we derived in this course become significantly simpler
to be shown with the SVD. Now that you have the SVD, you might want to read these notes again and try
to interpret the results we derived in terms of the SVD. For example, the Moore-Penrose Pseudoinverse
has a very simple description of the SVD, it corresponds to swapping U and V and replacing the non-zero
singular values by their inverses, while keeping the zero ones zero. Try to derive this!

10. B EYOND THE TOPICS WE DISCUSSED
Linear Algebra is absolutely fundamental as a tool to understand mathematical questions of all kinds. It
comes as no surprise that we could only touch some of the most important notions and tools in this field.
Let us mention below few topics and conjectures for interested readers of the course. This material is not
relevant for the exam.

60

√
So far, the norm of a vector x ∈ Rn was simply given by ∥x∥ = x⊤ x but there are instances where it makes
sense to measure the “lenght” of vectors in other ways. One popular way is called the “Manhattan distance”
since when traveling in Manhattan one cannot take advantage of Pythagoras Theorem because that would
involve cutting through buildings, that norm is given by ∥x∥1 = ∑ni=1 |xi |. In general, for 1 ≤ p ≤ ∞ the ℓ p
norm is given by
!1/p
n

∥x∥ p =

(43)

∑ |xi | p

,

i=1

for p < ∞, and ∥x∥∞ = maxi |xi |. Notice that ∥ · ∥2 corresponds to the Euclidean norm that we have used
in this course. The ℓ1 norm is notable for promoting sparsity when one attempts to minimize it to solve
underdetermined linear systems. This is the key idea behind “Compressed Sensing”, and plays a crucial
role in many imaging/sensing technologies. You can read more about it in Section 12 of [BM23] or Chapter
10 of [BSS] and references therein. The following relation can be shown.
√
Proposition 10.0.1. For all x ∈ Rn , we have ∥x∥2 ≤ ∥x∥1 ≤ n∥x∥2 .
In several situations one also needs to “measure” the size of matrices (for example, when talking about a
matrix being close to another one, we need a notion of distance, or norm of the difference).
Definition 10.0.2 (Two matrix norms). Given a matrix A ∈ Rm×n we define two matrix norms:
• ∥A∥F , known as the Frobenius norm, is defined as
s
m

∥A∥F =

n

∑ ∑ A2i j ,

i=1 j=1

• ∥A∥op , known as operator or specral norm, is defined as
∥A∥op = maxn ∥Ax∥.
x∈R
s.t.∥x∥=1

One can relate the two matrix norms as follows.
Proposition 10.0.3. Given A ∈ Rm×n with singular values σ1 ≥ · · · ≥ σmin{m,n} . We have

(1) ∥A∥2F = Tr AT A
min{m,n}

(2) ∥A∥2F =

∑

σi2

i=1

(3) ∥A∥op = σ1
p
(4) ∥A∥op ≤ ∥A∥F ≤ min{m, n}∥A∥op .
In the course we have covered the notion of eigenvalues and eigenvectors. There are a few fascinating open
questions we can state. These are questions (or conjectures), that we currently do now know the answer to
(or that we are not sure they are true).
The first author of this manuscript has a list of 42 open problems in some lecture notes [Ban16]: some
have been solved in the meantime, but many remain open.

61

A few open problems for which you have all the necessary background to understand are listed below
Open Problem 10.0.4 (Hadamard conjecture). For any n multiple of 4 there exists an Hadamard matrix
H that is n × n.
A Hadamard matrix H ∈ Rn×n is a matrix with only entries 1 or −1 that is a multiple of an orthogonal
matrix. In other words Hi, j = ±1 for all i, j and H ⊤ H = nI. Yet in other words: the columns of H are an
orthogonal basis for Rn formed with only vectors with entries ±1.
Open Problem 10.0.5 (Mutually Unbiased Bases). See Open Problem 6.2 in [Ban16].
Open Problem 10.0.6 (Zauner’s Conjecture). See Open Problem 6.3 in [Ban16].
Open Problem 10.0.7 (Komlos Conjecture). See Open Problem 0.1 in [Ban16].
Open Problem 10.0.8 (Matrix Spencer Conjecture). See Open Problem 4.3 in [Ban16].
Open Problem 10.0.9 (Rank of the Matrix Multiplication Tensor). What is the rank of the Matrix Multiplication Tensor corresponding to multiplication of 3 × 3 matrices.
A d1 × d2 × d3 tensor T is what we can think of as a cubic matrix. It has d1 d2 d3 entries given by Ti jl . We
say T has rank r if r is the smallest integer such that we can write
r

T = ∑ ak ⊗ bk ⊗ ck ,
k=1

for ak ∈ Rd1 , bk ∈ Rd2 , ck ∈ Rd3 , for k = 1, . . . , r. In other words
r

Ti jl = ∑ (ak )i (bk ) j (ck )l .
k=1

Recall Proposition 9.3.4 to see why for matrices this corresponds to the notion of rank we have been using.
While computing the rank of a matrix is computationally easy, doing so for tensors is notoriously difficult
(because they lack a spectral theory of eigenvalues and eigenvectors).
There is a way to think of Strassen’s algorithm (that you saw in a CS Lens in Part I of the course) in terms
of a decomposition of a certain Tensor in terms of rank-1 tensors. In this description we focus on n × n
matrices, but the same thing can be done for rectangular matrices.
The n × n matrix multiplication tensor is a n2 × n2 × n2 tensor, where each dimension is indexed by pairs
(i1 , i2 ), ( j1 , j2 ), (l1 , l2 ) and T is given by
(
1 if i1 = j1 , j2 = l1 , l2 = i2
T(i1 ,i2 ),( j1 , j2 ),(l1 ,l2 ) =
.
0 o.w.
Strassen’s algorithm can be viewed as the fact that the rank of the 2 × 2 matrix multiplication tensor (a
4 × 4 × 4 tensor) is ≤ 7. The rank of the 3 × 3 matrix multiplication tensor (a 9 × 9 × 9 tensor) remains
unknown.

62

A PPENDIX A. S OME I MPORTANT P RELIMINARIES AND R EMARKS ON N OTATION
To follow these notes the reader needs to be familiar with basics of vector and matrix operations and
manipulations; understand what is dimension of a subspace, and in particular that is well-defined (that
every basis of a subspace has the same size); and understand what is the rank of a matrix (and in particular
that the dimension of the column space and the row space are the same). Even though Gaussian Elimination
is not a core ingredient of this part of the course, we still assume that the reader is familiar with it.
Some further important preliminaries and/or remarks:
(1) The dot product x · y between two real valued vectors is sometimes also called inner product and
written as ⟨x, y⟩ (it is equal to x⊤ y). For Cn the inner product is given by ⟨x, y⟩ = y∗ x.
(2) Matrix Factorization for A an m × n matrix with rank r:
A = CR,
C is m × r with linearly independent columns (they are the first r linearly independent columns of
A). R is r × n, it is upper triangular (i.e. Ri j = 0 if i > j), and it has an r × r identity as a submatrix,
corresponding to the locations of the first r linearly independent columns of A.
(3) For V a subspace (or a vector space) with dimension n the following holds:
• Any basis of V has size n.
• Any spanning set of V has size ≥ n.
• Any spanning set of V with size n is also a basis.
• Any set of linearly independent vectors in V has size ≤ n.
• Any set of linearly independent vectors in V with size n is also a basis.

A PPENDIX B. A “S IMPLE PROOF ” OF THE F UNDAMENTAL T HEOREM OF A LGEBRA
In this appendix we present a brief sketch of a (relatively) simple proof of the Fundamental Theorem of
Algebra that we learned from Alessio Figalli.
Let P(z) be a polynomial of degree n. Without loss of generality we can assume it is monic P(z) =
zn + αn−1 zn−1 + · · · + α0 . Suppose P(z) has no zeros/roots. There is a r ∈ R large enough such that the
infimum of |P(z)| inside the close disc Dr of radius r centered at zero is smaller than that outside the disc
Dr (because far from the origin the term zn dominates and forces |P(z)| to be large outside of Dr . Since Dr
is a compact set and |P(z)| is continuous it needs to attain its minimum at a point z0 ∈ Dr . It requires some
extra analysis/topology background to see that a continuous function defined on a compact (think closed
and bounded) set needs to attain a minimum. Let us accept this fact and continue with the proof. Note that
P(z0 ) ̸= 0. Write Q(z) = P(z − z0 ), it is also a polynomial of degree n, Q(z) = β0 + β1 z + β2 z2 + · · · + zn .
Notice that β0 = P(z0 ). let k be the first coefficient of Q(z) (not including β0 ) that is nonzero (meaning
that βk ̸= 0 but βi = 0 for all0 < i < k. Then Q(z) = β0 + βk zk + βk+1 zk + · · · . Take ε > 0 arbitrarily small
 1k

. It is not difficult to see that for ε small enough the higher order terms are
and consider Q ε − ββ0k

63

 
1 
β0 k
k
negligible and the term β0 + βk z has smaller modulus and so one can pick ε such that Q ε − βk
<
|Q(0)| which is a contradiction with the fact that |P(z0 )| was minimum.
References
[Ban16] Afonso S. Bandeira. Ten lectures and forty-two open problems in the mathematics of
data
science.
Available
online
at:
https://people.math.ethz.ch/˜abandeira/
TenLecturesFortyTwoProblems.pdf.
See
also
https://ocw.mit.edu/courses/
18-s096-topics-in-mathematics-of-data-science-fall-2015/, 2016.
[BM23] Afonso S. Bandeira and Antoine Maillard. Mathematics of signals, networks, and learning. Available
online
at:
https://anmaillard.github.io/teaching/msnl_spring_2023.pdf.
Videos
from an earlier version of the course available at https://youtube.com/playlist?list=
PLiud-28tsatL0MbfJFQQS7MYkrFrujCYp, 2023.
[BSS] A. S. Bandeira, A. Singer, and T. Strohmer. Mathematics of data science. Book draft available at https://
people.math.ethz.ch/˜abandeira/BandeiraSingerStrohmer-MDS-draft.pdf. Videos available
at: https://www.youtube.com/playlist?list=PLiud-28tsatIKUitdoH3EEUZL-9i516IL.
[Str23] Gilbert Strang. Introduction to Linear Algebra. (Table of contents available at https://math.mit.edu/˜gs/
linearalgebra/ila6/indexila6.html). Wellesley - Cambridge Press., sixth edition, 2023.

